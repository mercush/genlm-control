{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>GenLM Control is a library for controlled generation from language models using programmable constraints. It leverages sequential Monte Carlo (SMC) methods to efficiently generate text that satisfies constraints or preferences encoded by arbitrary potential functions.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<p>This library can be installed using pip:</p> <pre><code>pip install genlm-control\n</code></pre> <p>See DEVELOPING.md for details on how to install the project for development.</p>"},{"location":"#examples","title":"Examples","text":""},{"location":"#controlling-an-llm-with-a-regular-expression","title":"Controlling an LLM with a regular expression","text":"<p>This example demonstrates how to constrain an LLM using a regular expression.</p> <pre><code>from genlm.control import PromptedLLM, BoolFSA, AWRS\n\n# Create a language model potential.\nllm = PromptedLLM.from_name(\"gpt2\")\nllm.set_prompt_from_str(\"Here is my honest opinion:\")\n\n# Create a finite-state automaton potential using a regular expression.\nfsa = BoolFSA.from_regex(r\" SMC is (\ud83d\udd25\ud83d\udd25|\ud83d\ude0d\ud83d\ude0d|\ud83e\udd0c\ud83e\udd0c) with LMs\")\n\n# Coerce the FSA so that it operates on the token type of the language model.\ncoerced_fsa = fsa.coerce(llm, f=b\"\".join)\n\n# Create a token sampler that combines the language model and FSA.\ntoken_sampler = AWRS(llm, coerced_fsa)\n\n# Generate text using SMC.\n# Generation is asynchronous; use `await` if calling in an async context (like in an async\n# function or in a Jupyter notebook) and `asyncio.run(token_sampler.smc(...))` otherwise.\nsequences = await token_sampler.smc(\n    n_particles=10, # Number of candidate sequences to maintain\n    ess_threshold=0.5, # Threshold for resampling\n    max_tokens=30, # Maximum sequence length\n    verbosity=1 # Print particles at each step\n)\n\nsequences.decoded_posterior\n# Example output:\n# {\n#   ' SMC is \ud83d\udd25\ud83d\udd25 with LMs': 1.0,\n# }\n</code></pre>"},{"location":"#controlling-an-llm-with-a-json-schema","title":"Controlling an LLM with a JSON schema","text":"<p>This example demonstrates how to control an LLM to generate JSON objects that match a given schema.</p> <pre><code>import json\nfrom genlm.control import PromptedLLM, JsonSchema, AWRS\n\nperson_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\n            \"type\": \"string\",\n            \"enum\": [\"Alice\", \"Bob\", \"Charlie\"],\n            \"description\": \"The name of the person\"\n        },\n        \"age\": {\n            \"type\": \"integer\",\n            \"minimum\": 20,\n            \"maximum\": 80,\n            \"description\": \"The age of the person\"\n        },\n    },\n}\n\nbook_schema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"title\": {\n            \"type\": \"string\",\n            \"minLength\": 1,\n            \"description\": \"The title of the book\"\n        },\n        \"pages\": {\n            \"type\": \"integer\",\n            \"minimum\": 1,\n            \"maximum\": 2000,\n            \"description\": \"The number of pages in the book\"\n        },\n        \"genre\": {\n            \"type\": \"string\",\n            \"enum\": [\"fiction\", \"non-fiction\", \"mystery\"],\n            \"description\": \"The genre of the book\"\n        }\n    },\n}\n\n# Create a language model potential.\n# Since this task is harder, we use a larger model.\n# (You will need to login via the Hugging Face CLI and have access to the model.)\nllm = PromptedLLM.from_name(\n    \"meta-llama/Llama-3.2-1B-Instruct\",\n    eos_tokens=[b\"&lt;|eom_id|&gt;\", b\"&lt;|eot_id|&gt;\"],\n    temperature=0.8\n)\n\n# Set the prompt for the language model.\n# Since we are using an instruction-tuned model, we use the chat template.\n# The prompt contains an example of a schema and a generated object,\n# followed by the schema we want to match.\nllm.prompt_ids = llm.model.tokenizer.apply_chat_template(\n    conversation=[\n        {\"role\": \"system\", \"content\": \"You need to generate a JSON object that matches the schema below. Only generate the JSON object on a single line with no other text.\"},\n        {\"role\": \"user\", \"content\": json.dumps(person_schema)},\n        {\"role\": \"assistant\", \"content\": '{\"name\": \"Alice\", \"age\": 30}'},\n        {\"role\": \"user\", \"content\": json.dumps(book_schema)},\n    ],\n    tokenize=True,\n    add_generation_prompt=True\n)\n\n# Create a schema potential.\nschema_potential = JsonSchema(book_schema)\n\n# Coerce the schema potential so that it operates on the token type of the language model.\ncoerced_schema = schema_potential.coerce(llm, f=b\"\".join)\n\n# Create a token sampler that combines the language model and the schema potential.\ntoken_sampler = AWRS(llm, coerced_schema)\n\n# Generate text using SMC.\n# Generation is asynchronous; use `await` if calling in an async context (like in an async\n# function or in a Jupyter notebook) and `asyncio.run(token_sampler.smc(...))` otherwise.\nsequences = await token_sampler.smc(\n    n_particles=2, # Number of candidate sequences to maintain\n    ess_threshold=0.5, # Threshold for resampling\n    max_tokens=30, # Maximum sequence length\n    verbosity=1 # Print particles at each step\n)\n\n# Show the inferred posterior distribution over complete UTF-8 decodable sequences.\nsequences.decoded_posterior\n# Example output:\n# {\n#   '{\"title\": \"The Lord of the Rings\", \"pages\": 1200, \"genre\": \"fiction\"}': 0.5008318164809697,\n#   '{\"title\": \"The Great Gatsby\", \"pages\": 178, \"genre\": \"fiction\"}': 0.49916818351903025,\n# }\n</code></pre>"},{"location":"#more-examples","title":"More examples","text":"<p>See getting_started.md to get an overview of the full range of features, including how to specify custom potential functions.</p>"},{"location":"#development","title":"Development","text":"<p>See DEVELOPING.md for details on how to install the project locally.</p>"},{"location":"getting_started/","title":"Getting Started with GenLM Control","text":"<p>This example demonstrates how to use the <code>genlm-control</code> library, starting with basic usage and building up to more complex scenarios. It's a good starting point for understanding how to build increasingly complex genlm-control programs, even though the actual example is somewhat contrived.</p>"},{"location":"getting_started/#basic-llm-sampling","title":"Basic LLM Sampling","text":"<p>First, let's look at basic language model sampling using a <code>PromptedLLM</code>:</p> <pre><code>from genlm.control import PromptedLLM, direct_token_sampler\n\n# Load gpt2 (or any other HuggingFace model)\nmtl_llm = PromptedLLM.from_name(\"gpt2\", temperature=0.5, eos_tokens=[b'.'])\n\n# Set the fixed prompt prefix for the language model\n# All language model predictions will be conditioned on this prompt\nmtl_llm.set_prompt_from_str(\"Montreal is\")\n\n# Load a sampler that proposes tokens by sampling directly from the LM's distribution\ntoken_sampler = direct_token_sampler(mtl_llm)\n\n# Run SMC with 5 particles, a maximum of 25 tokens, and an ESS threshold of 0.5\nsequences = await token_sampler.smc(n_particles=5, max_tokens=25, ess_threshold=0.5)\n\n# Show the posterior over token sequences\nsequences.posterior\n\n# Show the posterior over complete UTF-8 decodable sequences\nsequences.decoded_posterior\n</code></pre> <p>Note: Sequences are lists of <code>bytes</code> objects because each token in the language model's vocabulary is represented as a bytes object.</p>"},{"location":"getting_started/#prompt-intersection","title":"Prompt Intersection","text":"<p>Next, we'll look at combining prompts from multiple language models using a <code>Product</code> potential:</p> <pre><code># Spawn a new language model (shallow copy, sharing the same underlying model)\nbos_llm = mtl_llm.spawn()\nbos_llm.set_prompt_from_str(\"Boston is\")\n\n# Take the product of the two language models\n# This defines a `Product` potential which is the element-wise product of the two LMs\nproduct = mtl_llm * bos_llm\n\n# Create a sampler that proposes tokens by sampling directly from the product\ntoken_sampler = direct_token_sampler(product)\n\nsequences = await token_sampler.smc(n_particles=5, max_tokens=25, ess_threshold=0.5)\n\nsequences.posterior\n\nsequences.decoded_posterior\n</code></pre>"},{"location":"getting_started/#adding-regex-constraints","title":"Adding Regex Constraints","text":"<p>We can add regex constraints to our <code>product</code> using a <code>BoolFSA</code> and the <code>AWRS</code> token sampler:</p> <pre><code>from genlm.control import BoolFSA, AWRS\n\n# Create a regex constraint that matches sequences containing the word \"the\"\n# followed by either \"best\" or \"worst\" and then anything else\nbest_fsa = BoolFSA.from_regex(r\"\\sthe\\s(best|worst).*\")\n\n# BoolFSA's are defined over individual bytes by default\n# Their `prefix` and `complete` methods are called on byte sequences\nprint(\"best_fsa.prefix(b'the bes') =\", await best_fsa.prefix(b\"the bes\"))\nprint(\n    \"best_fsa.complete(b'the best city') =\",\n    await best_fsa.complete(b\"the best city\"),\n)\n\n# Coerce the FSA to work with the LLM's vocabulary\ncoerced_fsa = best_fsa.coerce(product, f=b\"\".join)\n\n# Use the AWRS token sampler; it will only call the fsa on a subset of the product vocabulary\ntoken_sampler = AWRS(product, coerced_fsa)\n\nsequences = await token_sampler.smc(n_particles=5, max_tokens=25, ess_threshold=0.5)\n\nsequences.posterior\n\nsequences.decoded_posterior\n</code></pre>"},{"location":"getting_started/#custom-sentiment-analysis-potential","title":"Custom Sentiment Analysis Potential","text":"<p>Now we'll create a custom potential by subclassing <code>Potential</code> and use it as a critic to further guide generation:</p> <pre><code>import torch\nfrom transformers import (\n    DistilBertTokenizer,\n    DistilBertForSequenceClassification,\n)\nfrom genlm.control import Potential\n\n# Create our own custom potential for sentiment analysis.\n# Custom potentials must subclass `Potential` and implement the `prefix` and `complete` methods.\n# They can also override other methods, like `batch_prefix`, and `batch_complete` for improved performance.\n# Each Potential needs to specify its vocabulary of tokens; this potential has a vocabulary of individual bytes.\nclass SentimentAnalysis(Potential):\n    def __init__(self, model, tokenizer, sentiment=\"POSITIVE\"):\n        self.model = model\n        self.tokenizer = tokenizer\n\n        self.sentiment_idx = model.config.label2id.get(sentiment, None)\n        if self.sentiment_idx is None:\n            raise ValueError(f\"Sentiment {sentiment} not found in model labels\")\n\n        super().__init__(vocabulary=list(range(256)))  # Defined over bytes\n\n    def _forward(self, contexts):\n        strings = [bytes(context).decode(\"utf-8\", errors=\"ignore\") for context in contexts]\n        inputs = self.tokenizer(strings, return_tensors=\"pt\", padding=True)\n        with torch.no_grad():\n            logits = self.model(**inputs).logits\n        return logits.log_softmax(dim=-1)[:, self.sentiment_idx].cpu().numpy()\n\n    async def prefix(self, context):\n        return self._forward([context])[0].item()\n\n    async def complete(self, context):\n        return self._forward([context])[0].item()\n\n    async def batch_complete(self, contexts):\n        return self._forward(contexts)\n\n    async def batch_prefix(self, contexts):\n        return self._forward(contexts)\n\n# Initialize sentiment analysis potential\nmodel_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\nsentiment_analysis = SentimentAnalysis(\n    model=DistilBertForSequenceClassification.from_pretrained(model_name),\n    tokenizer=DistilBertTokenizer.from_pretrained(model_name),\n    sentiment=\"POSITIVE\",\n)\n\n# Test the potential\nprint(\"\\nSentiment analysis test:\")\nprint(\n    \"sentiment_analysis.prefix(b'so good') =\",\n    await sentiment_analysis.prefix(b\"so good\"),\n)\nprint(\n    \"sentiment_analysis.prefix(b'so bad') =\",\n    await sentiment_analysis.prefix(b\"so bad\"),\n)\n\n# Verify the potential satisfies required properties\nawait sentiment_analysis.assert_logw_next_consistency(b\"the best\", top=5)\nawait sentiment_analysis.assert_autoreg_fact(b\"the best\")\n\n# Set up efficient sampling with the sentiment analysis potential\ntoken_sampler = AWRS(product, coerced_fsa)\ncritic = sentiment_analysis.coerce(token_sampler.target, f=b\"\".join)\n\n# Run SMC using the sentiment analysis potential as a critic\nsequences = await token_sampler.smc(\n    n_particles=5,\n    max_tokens=25,\n    ess_threshold=0.5,\n    critic=critic, # Pass the critic to the SMC sampler; this will reweight samples at each step based on their positivity\n)\n\n# Show the posterior over complete UTF-8 decodable sequences\nsequences.decoded_posterior\n</code></pre>"},{"location":"getting_started/#optimizing-with-autobatching","title":"Optimizing with Autobatching","text":"<p>Finally, we can optimize performance using autobatching. During generation, all requests to the sentiment analysis potential are made to the instance methods (<code>prefix</code>, <code>complete</code>). We can take advantage of the fact that we have parallelized batch versions of these methods using the <code>to_autobatched</code> method.</p> <pre><code>from arsenal.timer import timeit\n\n# Create an autobatched version of the critic\n# This creates a new potential that automatically batches concurrent\n# requests to the instance methods (`prefix`, `complete`, `logw_next`)\n# and processes them using the batch methods (`batch_complete`, `batch_prefix`, `batch_logw_next`).\nautobatched_critic = critic.to_autobatched()\n\n# Run SMC with timing for comparison\nwith timeit(\"Timing sentiment-guided sampling with autobatching\"):\n    sequences = await token_sampler.smc(\n        n_particles=10,\n        max_tokens=25,\n        ess_threshold=0.5,\n        critic=autobatched_critic, # Pass the autobatched critic to the SMC sampler\n    )\n\nsequences.decoded_posterior\n\n# The autobatched version should be significantly faster than this version\nwith timeit(\"Timing sentiment-guided sampling without autobatching\"):\n    sequences = await token_sampler.smc(\n        n_particles=10,\n        max_tokens=25,\n        ess_threshold=0.5,\n        critic=critic,\n    )\n\nsequences.decoded_posterior\n</code></pre>"},{"location":"performance/","title":"Performance Optimizations","text":"<p>The <code>genlm-control</code> library offers two key performance optimizations for instances of the <code>Potential</code> class:</p> <ul> <li>Autobatching: Automatically batches concurrent requests to the potential's instances methods</li> <li>Multiprocessing: Runs multiple instances of a <code>Potential</code> in parallel across CPU cores</li> </ul>"},{"location":"performance/#auto-batching","title":"Auto-batching","text":"<p>Auto-batching improves performance when a <code>Potential</code> class's batch methods (<code>batch_complete</code>,  <code>batch_prefix</code>, <code>batch_logw_next</code>, <code>batch_score</code>) are more efficient than sequentially running individual instance methods.</p>"},{"location":"performance/#usage","title":"Usage","text":"<p>To enable auto-batching, use the <code>to_autobatched()</code> method:</p> <pre><code>autobatched_potential = potential.to_autobatched()\n# Use it exactly like a regular potential - batching happens automatically\nresults = await asyncio.gather(\n    *(autobatched.complete(seq) for seq in sequences) # These will batched and processed by batch_complete\n)\n</code></pre> <p>This creates a new potential that is a wrapper (<code>AutoBatchedPotential</code>) around the original potential. The wrapper automatically collects concurrent requests in the background and processes them together using the potential's batch methods. This happens transparently without requiring changes to your code structure.</p>"},{"location":"performance/#multiprocessing","title":"Multiprocessing","text":"<p>CPU parallelization can significantly improve performance for compute-intensive <code>Potential</code> classes. This is particularly useful when methods like <code>complete</code>, <code>prefix</code>, or <code>logw_next</code> involve heavy computation.</p>"},{"location":"performance/#usage_1","title":"Usage","text":"<p>To enable multiprocessing, use the <code>to_multiprocess()</code> method:</p> <pre><code># Create a multiprocess wrapper with desired number of workers\nmp_potential = potential.to_multiprocess(num_workers=2)\n# Use it like a regular potential - requests are distributed across workers\nresults = await asyncio.gather(\n    *(mp_potential.complete(seq) for seq in sequences) # These will be distributed across workers\n)\n</code></pre> <p>This creates a new potential that is a wrapper (<code>MultiProcPotential</code>) around the original potential. The wrapper asynchronously distributes requests across multiple processes (in a non-blocking manner). This allows you to scale your computations across multiple cores without changing your code structure.</p>"},{"location":"performance/#requirements","title":"Requirements","text":"<p>For multiprocessing to work, the potential must implement a picklable <code>spawn()</code> method that creates a new instance of the potential. Only some built-in <code>Potential</code> classes support this by default. Custom potentials need to implement their own <code>spawn()</code> method.</p>"},{"location":"performance/#performance-benefits","title":"Performance Benefits","text":"<p>Multiprocessing improves performance for both batched methods (<code>batch_complete</code>, <code>batch_prefix</code>, <code>batch_logw_next</code>) and unbatched methods (<code>complete</code>, <code>prefix</code>, <code>logw_next</code>).</p> <p>In the batched case, requests within a batch are processed in parallel across workers. For individual method calls, requests are distributed to available worker processes and are executed asynchronously.</p>"},{"location":"performance/#when-to-use-each-optimization","title":"When to use each optimization","text":"<p>Note: Built-in <code>Potential</code> classes that can benefit from auto-batching support (e.g., <code>PromptedLLM</code>) will have auto-batching enabled by default.</p> <ul> <li>Use auto-batching when the potential's batch operations are more efficient than sequential operations</li> <li>Use multiprocessing when the potential's operations are compute-intensive and can benefit from parallel processing</li> <li>Consider the overhead of each optimization when deciding which to use. Multiprocessing in particular incurs a significant overhead when the potential's operations are not compute-intensive.</li> </ul>"},{"location":"potentials/","title":"Potentials","text":"<p>Potentials are the core object in <code>genlm-control</code>. A potential encodes constraints or preferences by assigning non-negative weights to sequences of tokens.</p> <p>Potentials guide text generation by:</p> <ul> <li>Acting as components of samplers, which serve to propose new tokens at each step of the generation process.</li> <li>Serving as critics, which serve to reweight sequences based on whether they satisfy the constraint encoded by the potential at each step of the generation process.</li> </ul>"},{"location":"potentials/#key-concepts","title":"Key concepts","text":""},{"location":"potentials/#vocabulary","title":"Vocabulary","text":"<p>Each potential has a vocabulary which defines the set of tokens it operates on. Most built-in potentials operate on vocabularies whose tokens are <code>bytes</code> or <code>int</code> objects (the latter often representing individual bytes).</p>"},{"location":"potentials/#weight-assignment","title":"Weight assignment","text":"<p>Potentials assign weights to sequences of tokens from their vocabulary. These weights are always non-negative real numbers, though they are computed in log space for numerical stability.</p> <p>A potential defines two core weighting functions:</p> <ol> <li> <p><code>complete</code> - Assigns weights to sequences that are considered \"finished\" or \"complete\". For example, a potential enforcing grammatical correctness would assign positive weights to grammatically valid sentences and zero weights (negative infinity in log space) to invalid ones.</p> </li> <li> <p><code>prefix</code> - Assigns weights to partial sequences that could potentially be extended into valid complete sequences. For example, a potential enforcing grammatical correctness would assign positive weights to prefixes of grammatically valid sequences.</p> <p>Given a complete method, there are many possible prefix methods that could be used, providing as much or as little information as desired. The key requirement is that if a prefix has zero weight, then all of its extensions and completions must also have zero weight - in other words, prefix cannot rule out sequences that could later become valid.</p> </li> </ol> <p>The relationship between complete and prefix weights is formalized in the Formalization section.</p>"},{"location":"potentials/#next-token-weights","title":"Next-token weights","text":"<p>Potentials also implement a <code>logw_next</code> method, which computes weights for each possible next token in the potential's vocabulary (and a reserved end-of-sequence token) given a context sequence. These weights are crucial for controlled text generation as they can be used to guide the selection of the next token at each step.</p> <p>The <code>logw_next</code> method is implemented by default in terms of the <code>complete</code> and <code>prefix</code> methods. Potentials will often override this method to provide a more efficient implementation. However, <code>logw_next</code> must satisfy a contract with <code>complete</code>/<code>prefix</code>, specified in Formalization.</p>"},{"location":"potentials/#batch-methods","title":"Batch methods","text":"<p>For improved performance with large batches of inputs, potentials support batch operations:</p> <ul> <li><code>batch_complete(contexts)</code></li> <li><code>batch_prefix(contexts)</code></li> <li><code>batch_logw_next(contexts)</code></li> <li><code>batch_score(contexts)</code></li> </ul> <p>By default, these methods simply call the corresponding non-batch method for all inputs, but potentials can override them to provide more efficient implementations. They can be used in conjunction with auto batching for improved performance during generation.</p>"},{"location":"potentials/#built-in-potentials","title":"Built-in potentials","text":"<p><code>genlm-control</code> comes with a number of built-in potentials that can be used in controlled text generation.</p>"},{"location":"potentials/#language-models","title":"Language models","text":"<p><code>PromptedLLM</code> represents a language model conditioned on a fixed prompt prefix.</p> <pre><code># Load GPT-2 with temperature 0.5\nllm = PromptedLLM.from_name(\"gpt2\", temperature=0.5)\n\n# Set a prompt prefix that all generations will be conditioned on\nllm.set_prompt_from_str(\"Montreal is\")\n</code></pre> <p><code>PromptedLLM</code>s have a vocabulary of <code>bytes</code> tokens, obtained from the language model's tokenizer.</p>"},{"location":"potentials/#finite-state-automata","title":"Finite-state automata","text":"<p><code>genlm-control</code> provides two FSA implementations:</p> <ol> <li> <p><code>WFSA</code> (Weighted Finite-State Automata) - For weighted constraints: <pre><code># Create a WFSA from a regex pattern\n# Transitions are automatically normalized to form probability distributions\nwfsa = WFSA.from_regex(r\"\\sthe\\s(best|worst).*\ud83d\ude0e\")\n</code></pre></p> </li> <li> <p><code>BoolFSA</code> (Boolean Finite-State Automata) - For hard constraints: <pre><code># Create a boolean FSA from a regex pattern\n# Transitions are binary (0 or -inf in log space)\nfsa = BoolFSA.from_regex(r\"\\sthe\\s(best|worst).*\ud83d\ude0e\")\n</code></pre></p> </li> </ol> <p>Both FSAs:</p> <ul> <li>Support regex patterns with standard syntax</li> <li>Operate on byte-level sequences by default</li> <li>Can be combined with other potentials via products</li> </ul>"},{"location":"potentials/#context-free-grammars","title":"Context-free grammars","text":"<p>Similar to FSAs, <code>genlm-control</code> provides two CFG implementations:</p> <ol> <li> <p><code>WCFG</code> (Weighted Context-Free Grammar). <pre><code>cfg = WCFG.from_string(\"\"\"\n    1.0: S -&gt; NP VP\n    0.5: NP -&gt; the N\n    0.5: NP -&gt; a N\n    1.0: VP -&gt; V NP\n    0.5: N -&gt; cat\n    0.5: N -&gt; dog\n    0.5: V -&gt; saw\n    0.5: V -&gt; chased\n\"\"\")\n</code></pre></p> </li> <li> <p><code>BoolCFG</code> (Boolean Context-Free Grammar). <pre><code># Create a boolean CFG from a Lark grammar string\ncfg = BoolCFG.from_lark(\"\"\"\n    start: np vp\n    np: (\"the\" | \"a\") WS n\n    vp: WS v WS np\n    n: \"cat\" | \"dog\"\n    v: \"saw\" | \"chased\"\n    %import common.WS\n\"\"\")\n</code></pre></p> </li> </ol> <p><code>BoolCFG</code>s support grammar specification via Lark syntax.</p> <p>Both CFGs:</p> <ul> <li>Use Earley parsing for efficient recognition</li> <li>Can be combined with other potentials</li> <li>Operate on byte-level sequences by default</li> </ul> <p>Note: It is recommended to specify grammars via lark syntax. The <code>from_string</code> method is provided for convenience, but it is not as flexible and robust.</p>"},{"location":"potentials/#custom-potentials","title":"Custom potentials","text":"<p>You can create custom potentials to implement specialized constraints or preferences that aren't covered by the built-in options.</p>"},{"location":"potentials/#creating-a-custom-potential","title":"Creating a custom potential","text":"<p>To define a custom potential:</p> <ol> <li>Create a subclass of <code>Potential</code></li> <li>Implement the <code>complete</code> and <code>prefix</code> methods</li> <li>Optionally override <code>logw_next</code> and the batch methods for performance optimization</li> </ol> <p>When implementing custom potentials, the key is understanding the relationship between <code>complete</code> and <code>prefix</code>. Consider the following example of a potential that only allows sequences of a given length:</p> <pre><code>class LengthPotential(Potential):\n    \"\"\" A potential that only allows sequences of a given length. \"\"\"\n    def __init__(self, vocabulary, length):\n        # Initialize the superclass with the potential's vocabulary.\n        super().__init__(vocabulary)\n        self.length = length\n\n    async def complete(self, context):\n        # Note: 0.0 = log(1.0) and float('-inf') = log(0.0)\n        return 0.0 if len(context) == self.length else float('-inf')\n\n    async def prefix(self, context):\n        # Note: 0.0 = log(1.0) and float('-inf') = log(0.0)\n        return 0.0 if len(context) &lt;= self.length else float('-inf')\n\nlength_potential = LengthPotential(vocabulary=[b'the', b'a', b'cat', b'dog', b'saw', b'chased'], length=5)\n</code></pre> <p>This example illustrates the key difference between <code>complete</code> and <code>prefix</code>: the <code>complete</code> method only allows sequences of exactly the target length, while the <code>prefix</code> method allows any sequence that could potentially reach the target length (i.e., any sequence not exceeding the target length).</p>"},{"location":"potentials/#common-pitfalls","title":"Common pitfalls","text":"<p>When implementing custom potentials, be aware of these common issues:</p> <ol> <li> <p>Inconsistent complete/prefix relationship - If your <code>prefix</code> method assigns zero weight to a sequence, all extensions must also have zero weight.</p> </li> <li> <p>Inefficient implementations - For complex potentials, consider overriding <code>logw_next</code> with a more efficient implementation than the default.</p> </li> <li> <p>Not handling async properly - All potential methods are asynchronous. Make sure to use <code>await</code> when calling them and define your methods with <code>async def</code>.</p> </li> </ol>"},{"location":"potentials/#testing-your-custom-potential","title":"Testing your custom potential","text":"<p>Potentials automatically inherit from the <code>PotentialTests</code> mixin, which provides a number of tests for validating the correctness of the potential's implementation.</p> <pre><code># These will raise an exception if the potential implementation does not satisfy the properties\nawait potential.assert_logw_next_consistency(context)\nawait potential.assert_autoreg_fact(context)\nawait potential.assert_batch_consistency(contexts)\n</code></pre>"},{"location":"potentials/#complex-usage","title":"Complex usage","text":""},{"location":"potentials/#products-of-potentials","title":"Products of potentials","text":"<p>The <code>Product</code> class allows you to combine two potentials. A <code>Product</code> is itself is a potential, meaning that it implements all potential methods and that it is possible to chain products to combine more than two potentials.</p> <pre><code># Example: Prompt intersection\nmtl_llm = PromptedLLM.from_name(\"gpt2\")\nmtl_llm.set_prompt_from_str(\"Montreal is\")\n\nbos_llm = mtl_llm.spawn()\nbos_llm.set_prompt_from_str(\"Boston is\")\n\n# Create product using multiplication operator\nproduct = mtl_llm * bos_llm\n</code></pre> <p>The product potential operates on the intersection of the two potentials' vocabularies. For a product potential:</p> <ul> <li>The vocabulary \\(\\A\\) is the intersection of the two potentials' vocabularies: \\(\\A = \\A_1 \\cap \\A_2\\).</li> <li>The prefix potential \\(\\prefix\\) is the product (sum in log space) of the individual prefix potentials: \\(\\log \\prefix(\\xx) = \\log \\prefix_1(\\xx) + \\log \\prefix_2(\\xx)\\).</li> <li>The complete potential \\(\\complete\\) is the product (sum in log space) of the individual complete potentials: \\(\\log \\complete(\\xx) = \\log \\complete_1(\\xx) + \\log \\complete_2(\\xx)\\).</li> <li>The next-token potential \\(\\pot(\\cdot \\mid \\xx)\\) is the product (sum in log space) of the individual next-token potentials: \\(\\log \\pot(x \\mid \\xx) = \\log \\pot_1(x \\mid \\xx) + \\log \\pot_2(x \\mid \\xx)\\) for \\(x \\in (\\A_1 \\cap \\A_2) \\cup \\{\\eos\\}\\)</li> </ul> <p>Warning: Be careful when taking products of potentials with minimal vocabulary overlap, as the resulting potential will only operate on tokens present in both vocabularies. A warning will be raised if the vocabulary overlap is less than 10% of either potential's vocabulary.</p>"},{"location":"potentials/#coerced-potentials","title":"Coerced potentials","text":"<p>The <code>Coerced</code> class allows you to adapt a potential to work with a different vocabulary using a coercion function. The coercion function must map between sequences in the new vocabulary and sequences in the potential's original vocabulary. This is particularly useful when combining potentials that operate on different types of tokens.</p> <pre><code># Example: Coercing a byte-level FSA to work with a language model's tokens\nfsa = BoolFSA.from_regex(r\"\\sthe\\s(best|worst).*\")  # Works on bytes\nllm = PromptedLLM.from_name(\"gpt2\")  # Works on byte sequences\n\n# Coerce the FSA to work with the LLM's tokens by joining tokens into bytes\ncoerced_fsa = fsa.coerce(llm, f=b''.join)\n\n# Now we can combine them using the product operator!\nproduct = llm * coerced_fsa\n</code></pre> <p>Common use cases for coercion include:</p> <ul> <li>Adapting byte-level constraints (like FSAs) to work with token-level language models (which have vocabularies of byte sequences)</li> <li>Implementing constraints that operate on processed versions of the tokens (e.g., lowercase text)</li> <li>Converting between different tokenization schemes</li> </ul> <p>Performance Note: The coercion operation can impact performance, especially when mapping from a coarser token type to a finer token type (e.g., byte sequences to individual bytes). To sample tokens from a coerced product, consider using specialized samplers (e.g., <code>eager_token_sampler</code>, <code>topk_token_sampler</code>).</p>"},{"location":"potentials/#performance-optimizations","title":"Performance optimizations","text":"<p><code>genlm-control</code> provides a number of performance optimizations for potentials, described in the performance section.</p>"},{"location":"potentials/#formalization","title":"Formalization","text":"<p>This section provides a formal definition of potentials and the relationships between their complete, prefix, and next-token potentials.</p> <p>Notation Let \\(\\A\\) be a vocabulary of tokens and \\(\\eos\\) a specialized end-of-sequence token. Let \\(\\A^*\\) denote the set of all sequences of tokens which can be built from \\(\\A\\) (including the empty sequence \\(\\epsilon\\)) and \\(\\A^*{\\eos} = \\{\\xx\\eos : \\xx \\in \\A^*\\}\\) the set of \\(\\eos\\)-terminated sequences. We refer to \\(\\A^*\\) as the set of prefix sequences and \\(\\A^*{\\eos}\\) the set of complete sequences.</p> <p>A potential \\(\\pot\\) is a function \\(\\pot: \\A^* \\cup\\A^*{\\eos} \\rightarrow \\mathbb{R}_{\\geq 0}\\) which assigns a non-negative real number to prefix and complete sequences from its vocabulary \\(\\A\\):</p> \\[ \\pot(\\xx) = \\begin{cases}     \\prefix(\\xx) &amp; \\text{if } \\xx \\in \\A^* \\\\     \\complete(\\yy) &amp; \\text{if } \\xx = \\yy\\eos, \\yy \\in \\A^* \\end{cases} \\] <p>where</p> <ul> <li>\\(\\prefix : \\A^* \\rightarrow \\mathbb{R}_{\\geq 0}\\) is the prefix potential</li> <li>\\(\\complete : \\A^* \\rightarrow \\mathbb{R}_{\\geq 0}\\) is the complete potential</li> </ul> <p>The complete and prefix potentials are related by the following equality:</p> \\[ \\prefix(\\xx) = 0 \\implies \\complete(\\xx\\yy) = 0 \\, \\forall \\xx,\\yy \\text{ such that } \\xx\\yy \\in \\A^* \\] <p>Intuitively, this means that the prefix potential cannot rule out a sequence which can later on turn out to be valid according to the complete potential.</p> <p>Finally, we define the next-token weights function \\(\\pot(x \\mid \\xx) : \\A \\cup \\{\\eos\\} \\rightarrow \\mathbb{R}_{\\geq 0}\\), which assigns a non-negative real number to each token \\(x \\in \\A \\cup \\{\\eos\\}\\) given a sequence \\(\\xx \\in \\A^*\\):</p> \\[ \\pot(x \\mid \\xx) = \\frac{\\pot(\\xx x)}{\\prefix(\\xx)} = \\begin{cases}     \\frac{\\prefix(\\xx x)}{\\prefix(\\xx)} &amp; \\text{if } x \\in \\A \\\\     \\frac{\\complete(\\xx)}{\\prefix(\\xx)} &amp; \\text{if } x = \\eos \\end{cases} \\] <p>\\(\\pot(\\cdot \\mid \\xx)\\) is related to the complete and prefix potentials according to the following autoregressive factorization:</p> \\[ \\frac{\\complete(\\xx)}{\\prefix(\\epsilon)} = \\pot(\\eos \\mid \\xx) \\prod_{x \\in \\xx} \\pot(x \\mid \\xx) \\]"},{"location":"potentials/#correspondance-with-the-potential-class","title":"Correspondance with the <code>Potential</code> class","text":"<p>Each of the quantities above directly corresponds to a method or attribute of the <code>Potential</code> class:</p> Method/Attribute Mathematical Quantity Description <code>vocab</code> \\(\\A\\) The vocabulary of the potential. <code>eos</code> \\(\\eos\\) The end-of-sequence token. <code>vocab_eos</code> \\(\\A \\cup \\{\\eos\\}\\) The vocabulary of the potential including the end-of-sequence token. <code>complete(self, context)</code> \\(\\log \\complete(\\xx)\\) The complete potential for a given sequence. <code>prefix(self, context)</code> \\(\\log \\prefix(\\xx)\\) The prefix potential for a given sequence. <code>logw_next(self, context)</code> \\(\\log \\pot(\\cdot \\mid \\xx)\\) The next-token potential for a given prefix sequence. <code>score(self, context)</code> \\(\\log \\pot(\\xx)\\) The potential, dispatching to <code>complete</code> for eos-terminated sequences and <code>prefix</code> otherwise."},{"location":"samplers/","title":"Token Samplers","text":"<p>TokenSamplers  are the objects that propose new tokens during generation. They generate individual tokens \\(x\\) given a <code>context</code> sequence. Each sample \\(x\\) is attached with a log importance weight \\(w\\).<sup>1</sup></p>"},{"location":"samplers/#direct-token-sampling","title":"Direct Token Sampling","text":"<p>The simplest token sampler is the <code>DirectTokenSampler</code>, which samples directly from the normalized version of a potential's <code>logw_next</code> method:</p> <pre><code># Create a direct token sampler for a potential\nsampler = DirectTokenSampler(potential)\n\n# Sample a token\ntoken, logw, logp = await sampler.sample(context)\n</code></pre> <p><code>DirectTokenSampler</code> is efficient when the potential's <code>logw_next</code> method is efficient (e.g., for language models). However, for potentials with large vocabularies or expensive <code>logw_next</code> computations, other sampling strategies may be more appropriate.</p>"},{"location":"samplers/#adaptive-weighted-rejection-sampling","title":"Adaptive Weighted Rejection Sampling","text":"<p>When attempting to sample from the product of a potential (e.g., a language model) and a boolean constraint potential (e.g., a CFG or JSON schema potential), the most efficient and lowest variance sampler is <code>AWRS</code>.<sup>2</sup> This framework is described in detail in Lipkin et al. (2025).</p> <pre><code># Create a AWRS token sampler from an llm and a cfg\ntoken_sampler = AWRS(llm, cfg)\n# Sample a token and weight\ntoken, logw, _ = await token_sampler.sample(context)\n</code></pre>"},{"location":"samplers/#set-based-token-sampling","title":"Set-based Token Sampling","text":"<p>A <code>SetTokenSampler</code> samples tokens by first sampling a weighted subset of tokens using a <code>SetSampler</code>, and then selects one token from the set proportional to its weight. These samplers are commonly used to sample tokens from a language model while enforcing non-boolean byte-level constraints. This algorithm is described in Appendix C of Loula et al. (2025).</p>"},{"location":"samplers/#set-samplers","title":"Set Samplers","text":"<p>SetTokenSamplers wrap a SetSampler, which is responsible for sampling a weighted subset of tokens. Currently, <code>genlm-control</code> provides two set samplers:</p> <ol> <li><code>EagerSetSampler</code> - Eagerly samples a set of tokens by sampling one \"subtoken\" (e.g., byte) at a time.</li> <li><code>TopKSetSampler</code> - Lazily enumerates the top \\(K\\) tokens by weight and samples an additional \"wildcard\" token to ensure absolute continuity. This sampler is typically slower than <code>EagerSetSampler</code>.</li> </ol> <p>Both of these set samplers are designed to work with two types of potentials:</p> <ol> <li>An iterable potential which has a vocabulary of iterable tokens (e.g., over byte sequences)</li> <li>An item potential which has a vocabulary of items which form the elements of iterable tokens (e.g., over individual bytes)</li> </ol> <p>In common scenarios, the iterable potential is a language model and the item potential is a byte-level potential.</p> <pre><code># Create a set-based token sampler using a set sampler\nset_sampler = EagerSetSampler(llm, fsa)\ntoken_sampler = SetTokenSampler(set_sampler)\n\n# Sample a token and weight\ntoken, logw, _ = await token_sampler.sample(context)\n</code></pre>"},{"location":"samplers/#factory-methods","title":"Factory methods","text":"<p>For convenience, we provide factory methods for creating set token samplers from potentials.</p> <pre><code>from genlm.control.sampler import topk_token_sampler, eager_token_sampler\n\ntopk_sampler = topk_token_sampler(llm, fsa, K=10)\n\neager_sampler = eager_token_sampler(llm, fsa)\n</code></pre>"},{"location":"samplers/#sampler-selection-guide-for-controlled-generation","title":"Sampler Selection Guide for Controlled Generation","text":"<p>The following table provides general guidelines for selecting a sampler in the context of controlled generation from an LLM. Note that the best sampler may vary depending on the specific controlled generation task.</p> Scenario Recommended Sampler Notes No token-level constraints <code>DirectTokenSampler</code> Basic LM sampling; used when all constraints are enforced using <code>critics</code> Boolean constraints (e.g., FSA, CFG, JSON schema) <code>AWRS</code> Efficient, low-variance, and exact sampling from product of a LLM and constraint Byte-level non-boolean constraints <code>eager_token_sampler</code> Generally less efficient than <code>AWRS</code>, but more flexible"},{"location":"samplers/#custom-token-samplers","title":"Custom Token Samplers","text":"<p>It is also possible to implement custom token samplers by subclassing the <code>TokenSampler</code> class and implementing the <code>sample</code> method. These implementations must satisfy the following contract.</p>"},{"location":"samplers/#token-sampler-contract","title":"Token Sampler Contract","text":"<p>All token samplers in <code>genlm-control</code> must generate properly weighted samples with respect to a target potential's next-token weights \\(\\pot(\\cdot \\mid \\bm{x})\\) given a context \\(\\xx\\):</p> <p>A weighted sample \\((x, w) \\sim q(\\cdot \\mid \\xx)\\) is properly weighted with respect to \\(\\pot(\\cdot \\mid \\xx)\\) if, for any function \\(f\\),</p> \\[ \\mathbb{E}_{(x,w) \\sim q(\\cdot \\mid \\xx)}[w f(x)] = \\sum_{x \\in \\A \\cup \\{\\eos\\}} f(x)\\cdot\\pot(x \\mid \\xx) \\] <p>where \\(\\mathcal{A}\\) is the vocabulary of the target potenital \\(\\pot\\).</p> <ol> <li> <p>Tokens samplers also return a log-probability which corresponds to the log-probability of all the random choices made by the sampler. It is returned for testing purposes and is not used during generation.\u00a0\u21a9</p> </li> <li> <p>\"Higher variance\" refers to the variance of the estimator, which is influenced by the variance of the importance weights. When a sampler has high variance, the importance weights can vary dramatically across different samples, leading to unstable estimates in downstream tasks. While high-variance samplers may generate samples efficiently, they often require more samples to achieve the same level of accuracy as lower-variance alternatives.\u00a0\u21a9</p> </li> </ol>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>genlm<ul> <li>control<ul> <li>constant</li> <li>potential<ul> <li>autobatch</li> <li>base</li> <li>built_in<ul> <li>canonical</li> <li>json</li> <li>llm</li> <li>wcfg</li> <li>wfsa</li> </ul> </li> <li>coerce</li> <li>multi_proc</li> <li>operators</li> <li>product</li> <li>testing</li> </ul> </li> <li>sampler<ul> <li>sequence</li> <li>set</li> <li>token</li> </ul> </li> <li>typing</li> <li>util</li> <li>viz</li> </ul> </li> </ul> </li> </ul>"},{"location":"reference/genlm/control/__init__/","title":"control","text":""},{"location":"reference/genlm/control/__init__/#genlm.control.Potential","title":"<code>Potential</code>","text":"<p>               Bases: <code>ABC</code>, <code>PotentialOps</code>, <code>PotentialTests</code></p> <p>Abstract base class for potentials.</p> <p>A Potential is a function that maps sequences of tokens in a vocabulary to non-negative real numbers (weights).</p> <p>Potentials assign weights to sequences of tokens based on whether they are complete sequences or prefixes of complete sequences.</p> <ul> <li><code>complete</code>: Assess the log weight of a sequence of tokens in the vocabulary as a complete sequence.</li> <li><code>prefix</code>: Assess the log weight of a sequence of tokens in the vocabulary as a prefix.</li> </ul> <p>Potentials additionally implement a <code>logw_next</code> method:</p> <ul> <li><code>logw_next</code>: Compute the next-token log weights of each token in the vocabulary and a special EOS (end-of-sequence) token given a context.</li> </ul> <p>Subclasses must minimally implement <code>complete</code> and <code>prefix</code>. <code>logw_next</code> and batched versions of the above methods come with default implementations, but may be overridden by subclasses for improved performance.</p> <p>All Potentials must satisfy a set of properties which can be tested using PotentialTests.</p> <p>Attributes:</p> Name Type Description <code>token_type</code> <code>TokenType</code> <p>The type of tokens in the vocabulary.</p> <code>vocab</code> <code>list</code> <p>List of tokens making up the vocabulary.</p> <code>eos</code> <code>EndOfSequence</code> <p>Special token to use as end-of-sequence.</p> <code>vocab_eos</code> <code>list</code> <p>List of tokens in <code>vocab</code> and <code>eos</code>. <code>eos</code> is assumed to be the last token in <code>vocab_eos</code>.</p> <code>lookup</code> <code>dict</code> <p>Mapping from tokens and <code>eos</code> to their indices in <code>vocab_eos</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>class Potential(ABC, PotentialOps, PotentialTests):\n    \"\"\"Abstract base class for potentials.\n\n    A Potential is a function that maps sequences of tokens in a vocabulary to non-negative real numbers (weights).\n\n    Potentials assign weights to sequences of tokens based on whether they are complete sequences or prefixes of complete sequences.\n\n    - `complete`: Assess the log weight of a sequence of tokens in the vocabulary as a complete sequence.\n    - `prefix`: Assess the log weight of a sequence of tokens in the vocabulary as a prefix.\n\n    Potentials additionally implement a `logw_next` method:\n\n    - `logw_next`: Compute the next-token log weights of each token in the vocabulary and a special EOS (end-of-sequence) token given a context.\n\n    Subclasses must minimally implement `complete` and `prefix`. `logw_next` and batched versions of the above methods\n    come with default implementations, but may be overridden by subclasses for improved performance.\n\n    All Potentials must satisfy a set of properties which can be tested using [PotentialTests][genlm.control.potential.testing.PotentialTests].\n\n    Attributes:\n        token_type (TokenType): The type of tokens in the vocabulary.\n        vocab (list): List of tokens making up the vocabulary.\n        eos (EndOfSequence): Special token to use as end-of-sequence.\n        vocab_eos (list): List of tokens in `vocab` and `eos`. `eos` is assumed to be the last token in `vocab_eos`.\n        lookup (dict): Mapping from tokens and `eos` to their indices in `vocab_eos`.\n    \"\"\"\n\n    def __init__(self, vocabulary, token_type=None, eos=None):\n        \"\"\"\n        Initialize the potential.\n\n        Args:\n            vocabulary (list): List of tokens that make up the vocabulary.\n            token_type (TokenType, optional): Optional TokenType of all elements of the vocabulary.\n                If None, will be inferred from vocabulary.\n            eos (EndOfSequence, optional): Special token to use as end-of-sequence. Defaults to `EOS`.\n                In general, this should not be set by users.\n\n        Raises:\n            ValueError: If vocabulary is empty.\n            TypeError: If vocabulary contains tokens which are not of `token_type`.\n        \"\"\"\n        if not vocabulary:\n            raise ValueError(\"vocabulary cannot be empty\")\n\n        if token_type is None:\n            token_type = infer_vocabulary_type(vocabulary)\n        elif not isinstance(token_type, TokenType):\n            raise ValueError(f\"token_type must be a TokenType, got {token_type!r}.\")\n\n        if not all(token_type.check(x) for x in vocabulary):\n            raise TypeError(f\"Tokens in vocabulary must be of type {token_type}.\")\n\n        if eos and not isinstance(eos, EndOfSequence):\n            raise ValueError(f\"EOS must be an instance of EndOfSequence, got {eos!r}.\")\n\n        self.eos = eos or EOS\n\n        self.token_type = token_type\n        self.vocab = vocabulary\n        self.vocab_eos = self.vocab + [self.eos]\n        self.lookup = {}\n        for i, x in enumerate(vocabulary):\n            if x in self.lookup:\n                raise ValueError(f\"Duplicate token {x!r} found in vocabulary\")\n            self.lookup[x] = i\n        self.lookup[self.eos] = len(self.vocab)\n\n    ####################\n    # Instance methods #\n    ####################\n\n    @abstractmethod\n    async def complete(self, context):\n        \"\"\"Assess the weight of `context` as a complete sequence.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context under the language.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @abstractmethod\n    async def prefix(self, context):\n        \"\"\"Assess the weight of `context` as a prefix.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context as a prefix.\n        \"\"\"\n        pass  # pragma: no cover\n\n    async def score(self, context):\n        \"\"\"Assess the weight of `context` based on EOS-termination.\n\n        This is a convenience method which dispatches to `complete` if `context` ends with `self.eos`, otherwise to `prefix`.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context, either as a prefix or complete sequence.\n        \"\"\"\n        if context and context[-1] == self.eos:\n            return await self.complete(context[:-1])\n        else:\n            return await self.prefix(context)\n\n    async def logw_next(self, context):\n        \"\"\"Compute the next-token weights of each token in `self.vocab_eos` given `context`.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (LazyWeights): Weights of each token in the vocabulary and EOS.\n        \"\"\"\n        ctx_log_w = await self.prefix(context)\n\n        if ctx_log_w == float(\"-inf\"):\n            raise ValueError(f\"Context {context!r} has weight zero under `prefix`.\")\n\n        scores = await self.batch_score([[*context, x] for x in self.vocab_eos])\n        logws = scores - ctx_log_w\n\n        return self.make_lazy_weights(logws)\n\n    ###################\n    # Batched methods #\n    ###################\n\n    async def batch_complete(self, contexts):\n        \"\"\"Batched equivalent to `complete`.\n\n        Assess the weight of each context as a complete sequence.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return np.array(\n            await asyncio.gather(*[self.complete(context) for context in contexts])\n        )\n\n    async def batch_prefix(self, contexts):\n        \"\"\"Batched equivalent to `prefix`.\n\n        Assess the weight of each context as a prefix.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return np.array(\n            await asyncio.gather(*[self.prefix(context) for context in contexts])\n        )\n\n    async def batch_score(self, contexts):\n        \"\"\"Batched equivalent to `score`.\n\n        Assess the weight of each context based on EOS-termination.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        complete, prefix = [], []\n        complete_indices, prefix_indices = [], []\n\n        for i, context in enumerate(contexts):\n            # We want == here instead of `is`.\n            if context and context[-1] == self.eos:\n                complete.append(context[:-1])\n                complete_indices.append(i)\n            else:\n                prefix.append(context)\n                prefix_indices.append(i)\n\n        complete_scores = (\n            await self.batch_complete(complete) if complete else np.array([])\n        )\n        prefix_scores = await self.batch_prefix(prefix) if prefix else np.array([])\n\n        results = np.empty(len(contexts))\n        if len(complete_scores) &gt; 0:\n            results[complete_indices] = complete_scores\n        if len(prefix_scores) &gt; 0:\n            results[prefix_indices] = prefix_scores\n\n        return results\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"Batched equivalent to `logw_next`.\n\n        Computes the next-token weights of each token in `self.vocab_eos` given each context in the batch.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (list): List of LazyWeights objects, one for each context.\n\n        Raises:\n            ValueError: If any context has zero weight (log weight of -inf) under `prefix`.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return await asyncio.gather(*[self.logw_next(context) for context in contexts])\n\n    #############\n    # Utilities #\n    #############\n\n    def make_lazy_weights(self, weights, log=True):\n        \"\"\"Helper method to create a LazyWeights object over the potential's vocabulary and EOS.\n\n        Args:\n            weights (np.array): Array of weights.\n            log (bool, optional): Whether the weights are in log space. Defaults to True.\n\n        Returns:\n            (LazyWeights): LazyWeights object defined over `self.vocab_eos`.\n        \"\"\"\n        return LazyWeights(\n            weights=weights, encode=self.lookup, decode=self.vocab_eos, log=log\n        )\n\n    def alloc_logws(self, default=float(\"-inf\")):\n        \"\"\"Allocate a new array of log weights for the potential's vocabulary and EOS.\n\n        Args:\n            default (float, optional): Default log weight. Defaults to -inf.\n\n        Returns:\n            (np.array): Array of length `len(self.vocab_eos)` filled with `default`.\n        \"\"\"\n        return np.full((len(self.vocab_eos),), default)\n\n    def spawn(self):\n        \"\"\"\n        Spawn a fresh instance of the potential.\n\n        This method is not required by default, but may be implemented by subclasses\n        to support CPU-parallelism using (`MultiProcPotential`)[genlm.control.potential.multi_proc.MultiProcPotential].\n        \"\"\"\n        raise NotImplementedError(\n            \"Potential.spawn() must be implemented by subclasses.\"\n        )\n\n    async def cleanup(self):\n        \"\"\"\n        Cleanup the potential.\n\n        This method may be implemented by subclasses to release resources.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.__init__","title":"<code>__init__(vocabulary, token_type=None, eos=None)</code>","text":"<p>Initialize the potential.</p> <p>Parameters:</p> Name Type Description Default <code>vocabulary</code> <code>list</code> <p>List of tokens that make up the vocabulary.</p> required <code>token_type</code> <code>TokenType</code> <p>Optional TokenType of all elements of the vocabulary. If None, will be inferred from vocabulary.</p> <code>None</code> <code>eos</code> <code>EndOfSequence</code> <p>Special token to use as end-of-sequence. Defaults to <code>EOS</code>. In general, this should not be set by users.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If vocabulary is empty.</p> <code>TypeError</code> <p>If vocabulary contains tokens which are not of <code>token_type</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def __init__(self, vocabulary, token_type=None, eos=None):\n    \"\"\"\n    Initialize the potential.\n\n    Args:\n        vocabulary (list): List of tokens that make up the vocabulary.\n        token_type (TokenType, optional): Optional TokenType of all elements of the vocabulary.\n            If None, will be inferred from vocabulary.\n        eos (EndOfSequence, optional): Special token to use as end-of-sequence. Defaults to `EOS`.\n            In general, this should not be set by users.\n\n    Raises:\n        ValueError: If vocabulary is empty.\n        TypeError: If vocabulary contains tokens which are not of `token_type`.\n    \"\"\"\n    if not vocabulary:\n        raise ValueError(\"vocabulary cannot be empty\")\n\n    if token_type is None:\n        token_type = infer_vocabulary_type(vocabulary)\n    elif not isinstance(token_type, TokenType):\n        raise ValueError(f\"token_type must be a TokenType, got {token_type!r}.\")\n\n    if not all(token_type.check(x) for x in vocabulary):\n        raise TypeError(f\"Tokens in vocabulary must be of type {token_type}.\")\n\n    if eos and not isinstance(eos, EndOfSequence):\n        raise ValueError(f\"EOS must be an instance of EndOfSequence, got {eos!r}.\")\n\n    self.eos = eos or EOS\n\n    self.token_type = token_type\n    self.vocab = vocabulary\n    self.vocab_eos = self.vocab + [self.eos]\n    self.lookup = {}\n    for i, x in enumerate(vocabulary):\n        if x in self.lookup:\n            raise ValueError(f\"Duplicate token {x!r} found in vocabulary\")\n        self.lookup[x] = i\n    self.lookup[self.eos] = len(self.vocab)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.complete","title":"<code>complete(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Assess the weight of <code>context</code> as a complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context under the language.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>@abstractmethod\nasync def complete(self, context):\n    \"\"\"Assess the weight of `context` as a complete sequence.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context under the language.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.prefix","title":"<code>prefix(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Assess the weight of <code>context</code> as a prefix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context as a prefix.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>@abstractmethod\nasync def prefix(self, context):\n    \"\"\"Assess the weight of `context` as a prefix.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context as a prefix.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.score","title":"<code>score(context)</code>  <code>async</code>","text":"<p>Assess the weight of <code>context</code> based on EOS-termination.</p> <p>This is a convenience method which dispatches to <code>complete</code> if <code>context</code> ends with <code>self.eos</code>, otherwise to <code>prefix</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context, either as a prefix or complete sequence.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def score(self, context):\n    \"\"\"Assess the weight of `context` based on EOS-termination.\n\n    This is a convenience method which dispatches to `complete` if `context` ends with `self.eos`, otherwise to `prefix`.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context, either as a prefix or complete sequence.\n    \"\"\"\n    if context and context[-1] == self.eos:\n        return await self.complete(context[:-1])\n    else:\n        return await self.prefix(context)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next-token weights of each token in <code>self.vocab_eos</code> given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Weights of each token in the vocabulary and EOS.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Compute the next-token weights of each token in `self.vocab_eos` given `context`.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (LazyWeights): Weights of each token in the vocabulary and EOS.\n    \"\"\"\n    ctx_log_w = await self.prefix(context)\n\n    if ctx_log_w == float(\"-inf\"):\n        raise ValueError(f\"Context {context!r} has weight zero under `prefix`.\")\n\n    scores = await self.batch_score([[*context, x] for x in self.vocab_eos])\n    logws = scores - ctx_log_w\n\n    return self.make_lazy_weights(logws)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.batch_complete","title":"<code>batch_complete(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>complete</code>.</p> <p>Assess the weight of each context as a complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_complete(self, contexts):\n    \"\"\"Batched equivalent to `complete`.\n\n    Assess the weight of each context as a complete sequence.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return np.array(\n        await asyncio.gather(*[self.complete(context) for context in contexts])\n    )\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.batch_prefix","title":"<code>batch_prefix(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>prefix</code>.</p> <p>Assess the weight of each context as a prefix.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_prefix(self, contexts):\n    \"\"\"Batched equivalent to `prefix`.\n\n    Assess the weight of each context as a prefix.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return np.array(\n        await asyncio.gather(*[self.prefix(context) for context in contexts])\n    )\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.batch_score","title":"<code>batch_score(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>score</code>.</p> <p>Assess the weight of each context based on EOS-termination.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_score(self, contexts):\n    \"\"\"Batched equivalent to `score`.\n\n    Assess the weight of each context based on EOS-termination.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    complete, prefix = [], []\n    complete_indices, prefix_indices = [], []\n\n    for i, context in enumerate(contexts):\n        # We want == here instead of `is`.\n        if context and context[-1] == self.eos:\n            complete.append(context[:-1])\n            complete_indices.append(i)\n        else:\n            prefix.append(context)\n            prefix_indices.append(i)\n\n    complete_scores = (\n        await self.batch_complete(complete) if complete else np.array([])\n    )\n    prefix_scores = await self.batch_prefix(prefix) if prefix else np.array([])\n\n    results = np.empty(len(contexts))\n    if len(complete_scores) &gt; 0:\n        results[complete_indices] = complete_scores\n    if len(prefix_scores) &gt; 0:\n        results[prefix_indices] = prefix_scores\n\n    return results\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>logw_next</code>.</p> <p>Computes the next-token weights of each token in <code>self.vocab_eos</code> given each context in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of LazyWeights objects, one for each context.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any context has zero weight (log weight of -inf) under <code>prefix</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"Batched equivalent to `logw_next`.\n\n    Computes the next-token weights of each token in `self.vocab_eos` given each context in the batch.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (list): List of LazyWeights objects, one for each context.\n\n    Raises:\n        ValueError: If any context has zero weight (log weight of -inf) under `prefix`.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return await asyncio.gather(*[self.logw_next(context) for context in contexts])\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.make_lazy_weights","title":"<code>make_lazy_weights(weights, log=True)</code>","text":"<p>Helper method to create a LazyWeights object over the potential's vocabulary and EOS.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>array</code> <p>Array of weights.</p> required <code>log</code> <code>bool</code> <p>Whether the weights are in log space. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LazyWeights</code> <p>LazyWeights object defined over <code>self.vocab_eos</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def make_lazy_weights(self, weights, log=True):\n    \"\"\"Helper method to create a LazyWeights object over the potential's vocabulary and EOS.\n\n    Args:\n        weights (np.array): Array of weights.\n        log (bool, optional): Whether the weights are in log space. Defaults to True.\n\n    Returns:\n        (LazyWeights): LazyWeights object defined over `self.vocab_eos`.\n    \"\"\"\n    return LazyWeights(\n        weights=weights, encode=self.lookup, decode=self.vocab_eos, log=log\n    )\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.alloc_logws","title":"<code>alloc_logws(default=float('-inf'))</code>","text":"<p>Allocate a new array of log weights for the potential's vocabulary and EOS.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>float</code> <p>Default log weight. Defaults to -inf.</p> <code>float('-inf')</code> <p>Returns:</p> Type Description <code>array</code> <p>Array of length <code>len(self.vocab_eos)</code> filled with <code>default</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def alloc_logws(self, default=float(\"-inf\")):\n    \"\"\"Allocate a new array of log weights for the potential's vocabulary and EOS.\n\n    Args:\n        default (float, optional): Default log weight. Defaults to -inf.\n\n    Returns:\n        (np.array): Array of length `len(self.vocab_eos)` filled with `default`.\n    \"\"\"\n    return np.full((len(self.vocab_eos),), default)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a fresh instance of the potential.</p> <p>This method is not required by default, but may be implemented by subclasses to support CPU-parallelism using (<code>MultiProcPotential</code>)[genlm.control.potential.multi_proc.MultiProcPotential].</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def spawn(self):\n    \"\"\"\n    Spawn a fresh instance of the potential.\n\n    This method is not required by default, but may be implemented by subclasses\n    to support CPU-parallelism using (`MultiProcPotential`)[genlm.control.potential.multi_proc.MultiProcPotential].\n    \"\"\"\n    raise NotImplementedError(\n        \"Potential.spawn() must be implemented by subclasses.\"\n    )\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.Potential.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleanup the potential.</p> <p>This method may be implemented by subclasses to release resources.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def cleanup(self):\n    \"\"\"\n    Cleanup the potential.\n\n    This method may be implemented by subclasses to release resources.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM","title":"<code>PromptedLLM</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A potential representing a language model conditioned on a fixed prompt prefix.</p> <p><code>PromptedLLM</code>s operate on byte sequences.</p> <p>Notes on EOS Token Handling:</p> <ul> <li> <p>Tokens to treat as end-of-sequence tokens are specified via the <code>eos_tokens</code> argument.</p> </li> <li> <p>These tokens are excluded from the potential's vocabulary and as such do not appear in the <code>vocab</code> attribute.</p> <p>This means they cannot appear in any input contexts to the potential nor in the output of <code>logw_next</code>. They can be used in the prompt however.</p> </li> <li> <p>The log probability assigned to the <code>genlm.control</code>'s reserved <code>EOS</code> token is the sum of the log probabilities of all the specified EOS tokens.</p> </li> </ul> <p>This class wraps an <code>AsyncLM</code> instance.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>class PromptedLLM(Potential):\n    \"\"\"A potential representing a language model conditioned on a fixed prompt prefix.\n\n    `PromptedLLM`s operate on byte sequences.\n\n    Notes on EOS Token Handling:\\n\n    - Tokens to treat as end-of-sequence tokens are specified via the `eos_tokens` argument.\\n\n    - These tokens are excluded from the potential's vocabulary and as such do not appear in the `vocab` attribute.\\n\n        This means they cannot appear in any input contexts to the potential nor in the output of `logw_next`. They can be used in the prompt however.\\n\n    - The log probability assigned to the `genlm.control`'s reserved `EOS` token is the sum of the log probabilities of all the specified EOS tokens.\\n\n\n    This class wraps an `AsyncLM` instance.\n    \"\"\"\n\n    def __init__(self, llm, prompt_ids=None, eos_tokens=None, temperature=1):\n        \"\"\"`\n        Initializes the PromptedLLM potential.\n\n        Args:\n            llm (AsyncLM): The language model to use.\n            prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n                Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `prompt` or `prompt_ids`.\n            eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n                Defaults to the EOS token of the language model's tokenizer.\n            temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n\n        Raises:\n            ValueError: If any EOS token is not in the language model vocabulary.\n        \"\"\"\n        self.model = llm\n        self.prompt_ids = prompt_ids or []\n\n        if not eos_tokens:\n            self._eos_tokens = [llm.byte_vocab[self.model.tokenizer.eos_token_id]]\n        else:\n            self._eos_tokens = eos_tokens\n\n        assert len(set(self._eos_tokens)) == len(self._eos_tokens), (\n            \"duplicate eos tokens\"\n        )\n\n        self.token_maps = TokenMappings.create(\n            decode=llm.byte_vocab, eos_tokens=self._eos_tokens\n        )\n\n        self.temperature = temperature\n\n        V = [x for x in self.token_maps.decode if x not in self._eos_tokens]\n\n        super().__init__(vocabulary=V)\n\n    @classmethod\n    def from_name(\n        cls,\n        name,\n        backend=None,\n        eos_tokens=None,\n        prompt_ids=None,\n        temperature=1.0,\n        **kwargs,\n    ):\n        \"\"\"Create a `PromptedLLM` from a HugginFace model name.\n\n        Args:\n            name (str): Name of the model to load\n            backend (str, optional): `AsyncLM` backend to use:\\n\n                * 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\\n\n                * 'hf' for an `AsyncTransformer`; ideal for CPU usage\\n\n                * 'mock' for a `MockAsyncLM`; ideal for testing.\\n\n                Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n            eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n                Defaults to the EOS token of the language model's tokenizer.\n            prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n                Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `set_prompt_from_str` or `prompt_ids`.\n            temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n            **kwargs (dict): Additional arguments passed to AsyncLM constructor\n\n        Returns:\n            (PromptedLLM): An instance of PromptedLLM\n        \"\"\"\n        backend = backend or (\"vllm\" if torch.cuda.is_available() else \"hf\")\n        model = load_model_by_name(name, backend=backend, **kwargs)\n        return cls(\n            model, prompt_ids=prompt_ids, eos_tokens=eos_tokens, temperature=temperature\n        )\n\n    @property\n    def eos_tokens(self):\n        return self._eos_tokens\n\n    @eos_tokens.setter\n    def eos_tokens(self, value):\n        raise ValueError(\n            \"Cannot reset eos_tokens after initialization. \"\n            \"Use spawn_new_eos(new_eos_tokens) instead.\"\n        )\n\n    @property\n    def prompt(self):\n        \"\"\"\n        Get the current prompt as a list of byte sequences corresponding to the prompt token IDs.\n\n        Returns:\n            (list[bytes]|None): The current prompt as a list of bytes sequences or None if no prompt_ids are set.\n        \"\"\"\n        if not self.prompt_ids:\n            return  # pragma: no cover\n        return [self.token_maps.decode[x] for x in self.prompt_ids]\n\n    def set_prompt_from_str(self, prompt_str):\n        \"\"\"Set the fixed prompt from a string.\n\n        Modifies `prompt_ids` to be the token IDs of the input prompt according to the language model's tokenizer.\n\n        Args:\n            prompt_str (str): The prompt to set.\n        \"\"\"\n        # TODO: Handle race condition where prompt_ids reset concurrently.\n        if not isinstance(prompt_str, str):\n            raise ValueError(\n                f\"Prompt must a string got {type(prompt_str)}. \"\n                f\"To set the prompt from a list of token IDs, use prompt_ids.\"\n            )\n\n        if prompt_str.endswith(\" \"):\n            warnings.warn(\n                \"Prompt ends with whitespace, which may affect tokenization. \"\n                \"Consider removing trailing whitespace.\",\n                stacklevel=2,\n            )\n\n        self.prompt_ids = self.model.tokenizer.encode(prompt_str)\n\n    def encode_tokens(self, tokens):\n        \"\"\"Encode a list of byte tokens to a list of token IDs in\n        the underlying language model's vocabulary.\n\n        Args:\n            tokens (list[bytes]): List of byte tokens to encode\n\n        Returns:\n            (list[int]): A list of token IDs corresponding to the input tokens.\n\n        Raises:\n            ValueError: If any token is not in the vocabulary\n        \"\"\"\n        try:\n            return [self.token_maps.encode[x] for x in tokens]\n        except KeyError as e:\n            raise ValueError(f\"Token {e.args[0]} not in vocabulary\") from e\n\n    def decode_tokens(self, ids):\n        \"\"\"\n        Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.\n\n        Args:\n            ids (list[int]): A list of token IDs in the language model's vocabulary.\n\n        Returns:\n            (list[bytes]): A list of byte tokens corresponding to the input token IDs.\n        \"\"\"\n        return [self.token_maps.decode[x] for x in ids]\n\n    def tokenize(self, context_str):\n        \"\"\"Tokenize a string to a list of `bytes` objects, each corresponding to a token in the vocabulary.\n\n        Uses the language model's tokenizer to map `context_str` to a list of token IDs, and then decodes the token IDs to bytes.\n\n        Args:\n            context_str (str): A string to encode\n\n        Returns:\n            (List[bytes]): A list of byte tokens corresponding to the input string.\n        \"\"\"\n        return self.decode_tokens(self.model.tokenizer.encode(context_str))\n\n    async def log_probability(self, context):\n        \"\"\"\n        Compute the log probability of `context` given the prompt.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of `context`.\n        \"\"\"\n        if not context:\n            return 0\n\n        context_ids = self.encode_tokens(context)\n        return await self._log_probability(context_ids)\n\n    async def _log_probability(self, context_ids):\n        prefixes = [self.prompt_ids + context_ids[:i] for i in range(len(context_ids))]\n        log_ps = self._maybe_temper(\n            await self.model.batch_next_token_logprobs(prefixes)\n        )\n        target_ids = torch.tensor(context_ids, device=log_ps.device)\n        with torch.no_grad():\n            token_logprobs = torch.gather(log_ps, 1, target_ids.unsqueeze(1))\n            total_logprob = token_logprobs.sum().item()\n\n        return total_logprob\n\n    def _maybe_temper(self, logps):\n        if self.temperature == 1:\n            return logps\n        return torch.log_softmax(logps / self.temperature, dim=-1)\n\n    async def prefix(self, context):\n        \"\"\"\n        Compute the log probability of `context` given the prompt.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of `context`.\n        \"\"\"\n        return await self.log_probability(context)\n\n    async def complete(self, context):\n        \"\"\"\n        Compute the log probability of `context` and the eos tokens given the prompt.\n\n        If the model has multiple eos tokens, their probabilities will be summed.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of the context.\n        \"\"\"\n        context_ids = self.encode_tokens(context)\n        logp_context = await self._log_probability(context_ids)\n        logp_next = self._maybe_temper(\n            await self.model.next_token_logprobs(self.prompt_ids + context_ids)\n        )\n        logp_eos = torch.logsumexp(logp_next[self.token_maps.eos_idxs], dim=0).item()\n        return logp_context + logp_eos\n\n    def _process_logw_next(self, logw_next):\n        \"\"\"Process the log probabilities for the next tokens.\n\n        This function rearranges the log probabilities such that the end-of-sequence (EOS) token's log probability\n        is the sum of the log probabilities of `self.eos_tokens`.\n\n        Args:\n            logw_next (torch.tensor): The log probabilities for the next tokens.\n\n        Returns:\n            (LazyWeights): Processed log probabilities for the next tokens.\n        \"\"\"\n        # This is ugly, but it's useful for all potentials to adhere to the convention\n        # of keeping the EOS token at the end of the weights array.\n        logw_next = logw_next[: len(self.token_maps.decode)]\n        logw_next = logw_next.log_softmax(dim=0)\n        _logw_next = torch.full((len(self.vocab) + 1,), float('-inf'), dtype=logw_next.dtype, device=logw_next.device)\n        _logw_next[: len(self.vocab)] = logw_next[\n            ~torch.isin(torch.arange(len(logw_next)), torch.tensor(self.token_maps.eos_idxs))\n        ]\n        _logw_next[-1] = torch.logsumexp(logw_next[self.token_maps.eos_idxs], dim=0).item()\n        return self.make_lazy_weights(_logw_next.float().cpu().numpy())\n\n    async def logw_next(self, context):\n        \"\"\"Get log probabilities for next tokens given the prompt and `context`.\n\n        Args:\n            context (List[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (LazyWeights): Log probabilities for next tokens and EOS.\n        \"\"\"\n        logw_next = self._maybe_temper(\n            await self.model.next_token_logprobs(\n                self.prompt_ids + self.encode_tokens(context)\n            )\n        )\n        return self._process_logw_next(logw_next)\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"Get log probabilities for next tokens given the prompt and `context`, for a batch of contexts.\n\n        Args:\n            contexts (list[list[bytes]]): A list of sequences of bytes tokens.\n\n        Returns:\n            (List[LazyWeights]): Log probabilities for next tokens and EOS for each context.\n        \"\"\"\n        logw_nexts = self._maybe_temper(\n            await self.model.batch_next_token_logprobs(\n                [self.prompt_ids + self.encode_tokens(context) for context in contexts]\n            )\n        )\n        return [\n            self._process_logw_next(logw_next)\n            for logw_next in logw_nexts\n        ]\n\n    def __repr__(self):\n        return f\"PromptedLLM(prompt={self.prompt!r})\"\n\n    def spawn(self):\n        \"\"\"\n        Spawn a new PromptedLLM with the same prompt and eos tokens.\n\n        Returns:\n            (PromptedLLM): A new PromptedLLM with the same prompt and eos tokens.\n\n        Note:\n            This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.\n        \"\"\"\n        return PromptedLLM(\n            self.model,\n            prompt_ids=self.prompt_ids.copy(),\n            eos_tokens=self._eos_tokens.copy(),\n            temperature=self.temperature,\n        )\n\n    def spawn_new_eos(self, eos_tokens):\n        \"\"\"\n        Create a new PromptedLLM with a different set of end-of-sequence tokens.\n\n        Args:\n            eos_tokens (list[bytes]): A list of tokens to treat as end-of-sequence tokens.\n\n        Returns:\n            (PromptedLLM): A new PromptedLLM with the specified end-of-sequence tokens.\n                The new model will have the same prompt_ids as `self`.\n        \"\"\"\n        return PromptedLLM(\n            self.model,\n            prompt_ids=self.prompt_ids.copy(),\n            eos_tokens=eos_tokens.copy(),\n            temperature=self.temperature,\n        )\n\n    def to_autobatched(self):\n        raise ValueError(\"PromptedLLMs are autobatched by default.\")\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.__init__","title":"<code>__init__(llm, prompt_ids=None, eos_tokens=None, temperature=1)</code>","text":"<p>` Initializes the PromptedLLM potential.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>AsyncLM</code> <p>The language model to use.</p> required <code>prompt_ids</code> <code>list[int]</code> <p>Optional prompt to use as a prompt prefix for all input contexts. Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via <code>prompt</code> or <code>prompt_ids</code>.</p> <code>None</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens to treat as end-of-sequence tokens. Defaults to the EOS token of the language model's tokenizer.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to apply to the language model's logits. Defaults to 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any EOS token is not in the language model vocabulary.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def __init__(self, llm, prompt_ids=None, eos_tokens=None, temperature=1):\n    \"\"\"`\n    Initializes the PromptedLLM potential.\n\n    Args:\n        llm (AsyncLM): The language model to use.\n        prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n            Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `prompt` or `prompt_ids`.\n        eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n            Defaults to the EOS token of the language model's tokenizer.\n        temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n\n    Raises:\n        ValueError: If any EOS token is not in the language model vocabulary.\n    \"\"\"\n    self.model = llm\n    self.prompt_ids = prompt_ids or []\n\n    if not eos_tokens:\n        self._eos_tokens = [llm.byte_vocab[self.model.tokenizer.eos_token_id]]\n    else:\n        self._eos_tokens = eos_tokens\n\n    assert len(set(self._eos_tokens)) == len(self._eos_tokens), (\n        \"duplicate eos tokens\"\n    )\n\n    self.token_maps = TokenMappings.create(\n        decode=llm.byte_vocab, eos_tokens=self._eos_tokens\n    )\n\n    self.temperature = temperature\n\n    V = [x for x in self.token_maps.decode if x not in self._eos_tokens]\n\n    super().__init__(vocabulary=V)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.from_name","title":"<code>from_name(name, backend=None, eos_tokens=None, prompt_ids=None, temperature=1.0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>PromptedLLM</code> from a HugginFace model name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model to load</p> required <code>backend</code> <code>str</code> <p><code>AsyncLM</code> backend to use:</p> <ul> <li> <p>'vllm' to instantiate an <code>AsyncVirtualLM</code>; ideal for GPU usage</p> </li> <li> <p>'hf' for an <code>AsyncTransformer</code>; ideal for CPU usage</p> </li> <li> <p>'mock' for a <code>MockAsyncLM</code>; ideal for testing.</p> </li> </ul> <p>Defaults to 'vllm' if CUDA is available, otherwise 'hf'.</p> <code>None</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens to treat as end-of-sequence tokens. Defaults to the EOS token of the language model's tokenizer.</p> <code>None</code> <code>prompt_ids</code> <code>list[int]</code> <p>Optional prompt to use as a prompt prefix for all input contexts. Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via <code>set_prompt_from_str</code> or <code>prompt_ids</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to apply to the language model's logits. Defaults to 1.</p> <code>1.0</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to AsyncLM constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>An instance of PromptedLLM</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>@classmethod\ndef from_name(\n    cls,\n    name,\n    backend=None,\n    eos_tokens=None,\n    prompt_ids=None,\n    temperature=1.0,\n    **kwargs,\n):\n    \"\"\"Create a `PromptedLLM` from a HugginFace model name.\n\n    Args:\n        name (str): Name of the model to load\n        backend (str, optional): `AsyncLM` backend to use:\\n\n            * 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\\n\n            * 'hf' for an `AsyncTransformer`; ideal for CPU usage\\n\n            * 'mock' for a `MockAsyncLM`; ideal for testing.\\n\n            Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n        eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n            Defaults to the EOS token of the language model's tokenizer.\n        prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n            Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `set_prompt_from_str` or `prompt_ids`.\n        temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n        **kwargs (dict): Additional arguments passed to AsyncLM constructor\n\n    Returns:\n        (PromptedLLM): An instance of PromptedLLM\n    \"\"\"\n    backend = backend or (\"vllm\" if torch.cuda.is_available() else \"hf\")\n    model = load_model_by_name(name, backend=backend, **kwargs)\n    return cls(\n        model, prompt_ids=prompt_ids, eos_tokens=eos_tokens, temperature=temperature\n    )\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.prompt","title":"<code>prompt</code>  <code>property</code>","text":"<p>Get the current prompt as a list of byte sequences corresponding to the prompt token IDs.</p> <p>Returns:</p> Type Description <code>list[bytes] | None</code> <p>The current prompt as a list of bytes sequences or None if no prompt_ids are set.</p>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.set_prompt_from_str","title":"<code>set_prompt_from_str(prompt_str)</code>","text":"<p>Set the fixed prompt from a string.</p> <p>Modifies <code>prompt_ids</code> to be the token IDs of the input prompt according to the language model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_str</code> <code>str</code> <p>The prompt to set.</p> required Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def set_prompt_from_str(self, prompt_str):\n    \"\"\"Set the fixed prompt from a string.\n\n    Modifies `prompt_ids` to be the token IDs of the input prompt according to the language model's tokenizer.\n\n    Args:\n        prompt_str (str): The prompt to set.\n    \"\"\"\n    # TODO: Handle race condition where prompt_ids reset concurrently.\n    if not isinstance(prompt_str, str):\n        raise ValueError(\n            f\"Prompt must a string got {type(prompt_str)}. \"\n            f\"To set the prompt from a list of token IDs, use prompt_ids.\"\n        )\n\n    if prompt_str.endswith(\" \"):\n        warnings.warn(\n            \"Prompt ends with whitespace, which may affect tokenization. \"\n            \"Consider removing trailing whitespace.\",\n            stacklevel=2,\n        )\n\n    self.prompt_ids = self.model.tokenizer.encode(prompt_str)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.encode_tokens","title":"<code>encode_tokens(tokens)</code>","text":"<p>Encode a list of byte tokens to a list of token IDs in the underlying language model's vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[bytes]</code> <p>List of byte tokens to encode</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of token IDs corresponding to the input tokens.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any token is not in the vocabulary</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def encode_tokens(self, tokens):\n    \"\"\"Encode a list of byte tokens to a list of token IDs in\n    the underlying language model's vocabulary.\n\n    Args:\n        tokens (list[bytes]): List of byte tokens to encode\n\n    Returns:\n        (list[int]): A list of token IDs corresponding to the input tokens.\n\n    Raises:\n        ValueError: If any token is not in the vocabulary\n    \"\"\"\n    try:\n        return [self.token_maps.encode[x] for x in tokens]\n    except KeyError as e:\n        raise ValueError(f\"Token {e.args[0]} not in vocabulary\") from e\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.decode_tokens","title":"<code>decode_tokens(ids)</code>","text":"<p>Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[int]</code> <p>A list of token IDs in the language model's vocabulary.</p> required <p>Returns:</p> Type Description <code>list[bytes]</code> <p>A list of byte tokens corresponding to the input token IDs.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def decode_tokens(self, ids):\n    \"\"\"\n    Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.\n\n    Args:\n        ids (list[int]): A list of token IDs in the language model's vocabulary.\n\n    Returns:\n        (list[bytes]): A list of byte tokens corresponding to the input token IDs.\n    \"\"\"\n    return [self.token_maps.decode[x] for x in ids]\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.tokenize","title":"<code>tokenize(context_str)</code>","text":"<p>Tokenize a string to a list of <code>bytes</code> objects, each corresponding to a token in the vocabulary.</p> <p>Uses the language model's tokenizer to map <code>context_str</code> to a list of token IDs, and then decodes the token IDs to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>context_str</code> <code>str</code> <p>A string to encode</p> required <p>Returns:</p> Type Description <code>List[bytes]</code> <p>A list of byte tokens corresponding to the input string.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def tokenize(self, context_str):\n    \"\"\"Tokenize a string to a list of `bytes` objects, each corresponding to a token in the vocabulary.\n\n    Uses the language model's tokenizer to map `context_str` to a list of token IDs, and then decodes the token IDs to bytes.\n\n    Args:\n        context_str (str): A string to encode\n\n    Returns:\n        (List[bytes]): A list of byte tokens corresponding to the input string.\n    \"\"\"\n    return self.decode_tokens(self.model.tokenizer.encode(context_str))\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.log_probability","title":"<code>log_probability(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> given the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def log_probability(self, context):\n    \"\"\"\n    Compute the log probability of `context` given the prompt.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of `context`.\n    \"\"\"\n    if not context:\n        return 0\n\n    context_ids = self.encode_tokens(context)\n    return await self._log_probability(context_ids)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> given the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Compute the log probability of `context` given the prompt.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of `context`.\n    \"\"\"\n    return await self.log_probability(context)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> and the eos tokens given the prompt.</p> <p>If the model has multiple eos tokens, their probabilities will be summed.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of the context.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Compute the log probability of `context` and the eos tokens given the prompt.\n\n    If the model has multiple eos tokens, their probabilities will be summed.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of the context.\n    \"\"\"\n    context_ids = self.encode_tokens(context)\n    logp_context = await self._log_probability(context_ids)\n    logp_next = self._maybe_temper(\n        await self.model.next_token_logprobs(self.prompt_ids + context_ids)\n    )\n    logp_eos = torch.logsumexp(logp_next[self.token_maps.eos_idxs], dim=0).item()\n    return logp_context + logp_eos\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Get log probabilities for next tokens given the prompt and <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>List[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Log probabilities for next tokens and EOS.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Get log probabilities for next tokens given the prompt and `context`.\n\n    Args:\n        context (List[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (LazyWeights): Log probabilities for next tokens and EOS.\n    \"\"\"\n    logw_next = self._maybe_temper(\n        await self.model.next_token_logprobs(\n            self.prompt_ids + self.encode_tokens(context)\n        )\n    )\n    return self._process_logw_next(logw_next)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Get log probabilities for next tokens given the prompt and <code>context</code>, for a batch of contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list[list[bytes]]</code> <p>A list of sequences of bytes tokens.</p> required <p>Returns:</p> Type Description <code>List[LazyWeights]</code> <p>Log probabilities for next tokens and EOS for each context.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"Get log probabilities for next tokens given the prompt and `context`, for a batch of contexts.\n\n    Args:\n        contexts (list[list[bytes]]): A list of sequences of bytes tokens.\n\n    Returns:\n        (List[LazyWeights]): Log probabilities for next tokens and EOS for each context.\n    \"\"\"\n    logw_nexts = self._maybe_temper(\n        await self.model.batch_next_token_logprobs(\n            [self.prompt_ids + self.encode_tokens(context) for context in contexts]\n        )\n    )\n    return [\n        self._process_logw_next(logw_next)\n        for logw_next in logw_nexts\n    ]\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new PromptedLLM with the same prompt and eos tokens.</p> <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>A new PromptedLLM with the same prompt and eos tokens.</p> Note <p>This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def spawn(self):\n    \"\"\"\n    Spawn a new PromptedLLM with the same prompt and eos tokens.\n\n    Returns:\n        (PromptedLLM): A new PromptedLLM with the same prompt and eos tokens.\n\n    Note:\n        This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.\n    \"\"\"\n    return PromptedLLM(\n        self.model,\n        prompt_ids=self.prompt_ids.copy(),\n        eos_tokens=self._eos_tokens.copy(),\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.PromptedLLM.spawn_new_eos","title":"<code>spawn_new_eos(eos_tokens)</code>","text":"<p>Create a new PromptedLLM with a different set of end-of-sequence tokens.</p> <p>Parameters:</p> Name Type Description Default <code>eos_tokens</code> <code>list[bytes]</code> <p>A list of tokens to treat as end-of-sequence tokens.</p> required <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>A new PromptedLLM with the specified end-of-sequence tokens. The new model will have the same prompt_ids as <code>self</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def spawn_new_eos(self, eos_tokens):\n    \"\"\"\n    Create a new PromptedLLM with a different set of end-of-sequence tokens.\n\n    Args:\n        eos_tokens (list[bytes]): A list of tokens to treat as end-of-sequence tokens.\n\n    Returns:\n        (PromptedLLM): A new PromptedLLM with the specified end-of-sequence tokens.\n            The new model will have the same prompt_ids as `self`.\n    \"\"\"\n    return PromptedLLM(\n        self.model,\n        prompt_ids=self.prompt_ids.copy(),\n        eos_tokens=eos_tokens.copy(),\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolCFG","title":"<code>BoolCFG</code>","text":"<p>               Bases: <code>Potential</code></p> <p>BoolCFG represents a boolean context-free grammar.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>class BoolCFG(Potential):\n    \"\"\"BoolCFG represents a boolean context-free grammar.\"\"\"\n\n    def __init__(self, cfg):\n        if cfg.R != Boolean:\n            cfg = cfg.map_values(lambda x: Boolean(x &gt; 0), Boolean)\n        self.cfg = cfg  # cfg before prefix transform\n        self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n        self.model = Earley(self.cfg_eos.prefix_grammar)\n        super().__init__(vocabulary=list(cfg.V))\n\n    @classmethod\n    def from_lark(cls, lark_string, charset=\"core\"):\n        \"\"\"\n        Create a BoolCFG instance from a Lark grammar string.\n\n        The output grammar will be defined at the byte-level.\n\n        Args:\n            lark_string (str): The Lark grammar string to parse. See Lark documentation for correct syntax.\n            charset (str): The character set to use. Defaults to \"core\".\n                See `genlm-grammar` documentation for more details.\n\n        Returns:\n            (BoolCFG): An instance of BoolCFG created from the provided Lark grammar.\n        \"\"\"\n        byte_cfg = LarkStuff(lark_string).byte_cfg(charset=charset)\n        return cls(byte_cfg)\n\n    async def complete(self, context):\n        \"\"\"\n        Checks whether the context is accepted by the CFG.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (float): Log weight for whether `context` is accepted by the CFG.\n        \"\"\"\n        w = self.model([*context, EOS])\n        return 0 if w.score else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Checks whether `context` is accepted as a prefix by the CFG, i.e.,\n        whether there exists a completion to `context` that is accepted by the CFG.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (float): Log weight for whether `context` is accepted as a prefix by the CFG.\n        \"\"\"\n        if not context:  # FIX: this is a hack to handle the empty string because genlm-grammar doesn't support it\n            return 0\n        w = self.model(context)\n        return 0 if w.score else float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute the next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (LazyWeights): The log weights for the next tokens and EOS given `context`.\n        \"\"\"\n        ws = self.model.next_token_weights(self.model.chart(context))\n        log_ws = np.array([0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos])\n        return self.make_lazy_weights(log_ws)\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"\n        Batch version of `logw_next`.\n\n        Args:\n            contexts (list): A list of sequences of tokens in the CFG's alphabet.\n\n        Returns:\n            (list): A list of log-weights for next token, one per context.\n        \"\"\"\n        Ws = []\n        for context in contexts:\n            ws = self.model.next_token_weights(self.model.chart(context))\n            log_ws = np.array(\n                [0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos]\n            )\n            Ws.append(self.make_lazy_weights(log_ws))\n        return Ws\n\n    def spawn(self):\n        \"\"\"Spawn a new BoolCFG.\"\"\"\n        return BoolCFG(self.cfg)\n\n    def clear_cache(self):\n        \"\"\"Clear the internal cache of the parser.\"\"\"\n        self.model.clear_cache()\n\n    def __repr__(self):\n        return f\"BoolCFG(cfg={self.cfg!r})\"\n\n    def _repr_html_(self):\n        return self.cfg._repr_html_()\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolCFG.from_lark","title":"<code>from_lark(lark_string, charset='core')</code>  <code>classmethod</code>","text":"<p>Create a BoolCFG instance from a Lark grammar string.</p> <p>The output grammar will be defined at the byte-level.</p> <p>Parameters:</p> Name Type Description Default <code>lark_string</code> <code>str</code> <p>The Lark grammar string to parse. See Lark documentation for correct syntax.</p> required <code>charset</code> <code>str</code> <p>The character set to use. Defaults to \"core\". See <code>genlm-grammar</code> documentation for more details.</p> <code>'core'</code> <p>Returns:</p> Type Description <code>BoolCFG</code> <p>An instance of BoolCFG created from the provided Lark grammar.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>@classmethod\ndef from_lark(cls, lark_string, charset=\"core\"):\n    \"\"\"\n    Create a BoolCFG instance from a Lark grammar string.\n\n    The output grammar will be defined at the byte-level.\n\n    Args:\n        lark_string (str): The Lark grammar string to parse. See Lark documentation for correct syntax.\n        charset (str): The character set to use. Defaults to \"core\".\n            See `genlm-grammar` documentation for more details.\n\n    Returns:\n        (BoolCFG): An instance of BoolCFG created from the provided Lark grammar.\n    \"\"\"\n    byte_cfg = LarkStuff(lark_string).byte_cfg(charset=charset)\n    return cls(byte_cfg)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolCFG.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Checks whether the context is accepted by the CFG.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight for whether <code>context</code> is accepted by the CFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Checks whether the context is accepted by the CFG.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (float): Log weight for whether `context` is accepted by the CFG.\n    \"\"\"\n    w = self.model([*context, EOS])\n    return 0 if w.score else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolCFG.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Checks whether <code>context</code> is accepted as a prefix by the CFG, i.e., whether there exists a completion to <code>context</code> that is accepted by the CFG.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight for whether <code>context</code> is accepted as a prefix by the CFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Checks whether `context` is accepted as a prefix by the CFG, i.e.,\n    whether there exists a completion to `context` that is accepted by the CFG.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (float): Log weight for whether `context` is accepted as a prefix by the CFG.\n    \"\"\"\n    if not context:  # FIX: this is a hack to handle the empty string because genlm-grammar doesn't support it\n        return 0\n    w = self.model(context)\n    return 0 if w.score else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolCFG.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>The log weights for the next tokens and EOS given <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute the next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (LazyWeights): The log weights for the next tokens and EOS given `context`.\n    \"\"\"\n    ws = self.model.next_token_weights(self.model.chart(context))\n    log_ws = np.array([0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos])\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolCFG.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Batch version of <code>logw_next</code>.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>A list of sequences of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of log-weights for next token, one per context.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"\n    Batch version of `logw_next`.\n\n    Args:\n        contexts (list): A list of sequences of tokens in the CFG's alphabet.\n\n    Returns:\n        (list): A list of log-weights for next token, one per context.\n    \"\"\"\n    Ws = []\n    for context in contexts:\n        ws = self.model.next_token_weights(self.model.chart(context))\n        log_ws = np.array(\n            [0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos]\n        )\n        Ws.append(self.make_lazy_weights(log_ws))\n    return Ws\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolCFG.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new BoolCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def spawn(self):\n    \"\"\"Spawn a new BoolCFG.\"\"\"\n    return BoolCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolCFG.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the internal cache of the parser.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the internal cache of the parser.\"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolFSA","title":"<code>BoolFSA</code>","text":"<p>               Bases: <code>WFSA</code></p> <p>Boolean FSA potential.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>class BoolFSA(WFSA):\n    \"\"\"Boolean FSA potential.\"\"\"\n\n    async def prefix(self, context):\n        \"\"\"\n        Computes whether the context is accepted as a prefix by the FSA.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): `0` if the context is accepted as a prefix, `-inf` otherwise.\n        \"\"\"\n        prefix_w = await super().prefix(context)\n        if prefix_w &gt; float(\"-inf\"):\n            return 0\n        return float(\"-inf\")\n\n    async def complete(self, context):\n        \"\"\"\n        Computes whether the context is accepted by the FSA.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): `0` if the context is accepted, `-inf` otherwise.\n        \"\"\"\n        complete_w = await super().complete(context)\n        if complete_w &gt; float(\"-inf\"):\n            return 0\n        return float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Returns next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (LazyWeights): Boolean log-weights for next token.\n        \"\"\"\n        logw_next = await super().logw_next(context)\n        return logw_next.spawn(\n            new_weights=np.where(\n                logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n            )\n        )\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"\n        Returns next token log weights for a batch of contexts.\n\n        Args:\n            contexts (list): The list of contexts.\n\n        Returns:\n            (list): List of log-weights for next token, one per context.\n        \"\"\"\n        logw_nexts = await super().batch_logw_next(contexts)\n        return [\n            logw_next.spawn(\n                new_weights=np.where(\n                    logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n                )\n            )\n            for logw_next in logw_nexts\n        ]\n\n    def __repr__(self):\n        return f\"BoolFSA(wfsa={self.wfsa!r})\"\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolFSA.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Computes whether the context is accepted as a prefix by the FSA.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p><code>0</code> if the context is accepted as a prefix, <code>-inf</code> otherwise.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Computes whether the context is accepted as a prefix by the FSA.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): `0` if the context is accepted as a prefix, `-inf` otherwise.\n    \"\"\"\n    prefix_w = await super().prefix(context)\n    if prefix_w &gt; float(\"-inf\"):\n        return 0\n    return float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolFSA.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Computes whether the context is accepted by the FSA.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p><code>0</code> if the context is accepted, <code>-inf</code> otherwise.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Computes whether the context is accepted by the FSA.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): `0` if the context is accepted, `-inf` otherwise.\n    \"\"\"\n    complete_w = await super().complete(context)\n    if complete_w &gt; float(\"-inf\"):\n        return 0\n    return float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolFSA.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Returns next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Boolean log-weights for next token.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Returns next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (LazyWeights): Boolean log-weights for next token.\n    \"\"\"\n    logw_next = await super().logw_next(context)\n    return logw_next.spawn(\n        new_weights=np.where(\n            logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n        )\n    )\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.BoolFSA.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Returns next token log weights for a batch of contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>The list of contexts.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of log-weights for next token, one per context.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"\n    Returns next token log weights for a batch of contexts.\n\n    Args:\n        contexts (list): The list of contexts.\n\n    Returns:\n        (list): List of log-weights for next token, one per context.\n    \"\"\"\n    logw_nexts = await super().batch_logw_next(contexts)\n    return [\n        logw_next.spawn(\n            new_weights=np.where(\n                logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n            )\n        )\n        for logw_next in logw_nexts\n    ]\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WFSA","title":"<code>WFSA</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A weighted finite state automaton (WFSA) potential.</p> <p>This class wraps a <code>genlm_grammar.WFSA</code> and provides methods for computing the log-weight of a context, the prefix log-weight of a context, and the log-weights of the next token given a context.</p> <p>Attributes:</p> Name Type Description <code>wfsa</code> <code>WFSA</code> <p>The weighted finite state automaton used for potential calculations.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>class WFSA(Potential):\n    \"\"\"\n    A weighted finite state automaton (WFSA) potential.\n\n    This class wraps a `genlm_grammar.WFSA` and provides methods for computing the log-weight of a context,\n    the prefix log-weight of a context, and the log-weights of the next token given a context.\n\n    Attributes:\n        wfsa (genlm_grammar.WFSA): The weighted finite state automaton used for potential calculations.\n    \"\"\"\n\n    def __init__(self, wfsa):\n        \"\"\"\n        Initializes the WFSA potential.\n\n        Args:\n            wfsa (genlm_grammar.WFSA): The weighted finite state automaton.\n\n        Raises:\n            ValueError: If the semiring of the provided WFSA is not Float or Log.\n\n        Note:\n            The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.\n        \"\"\"\n        if wfsa.R not in (Float, Log):\n            raise ValueError(f\"Unsupported semiring: {wfsa.R}\")\n\n        if wfsa.R is Float:\n            self.wfsa = self._convert_to_log(wfsa)\n        else:\n            self.wfsa = wfsa\n\n        self.cache = {(): self.wfsa.epsremove.start}\n        super().__init__(vocabulary=list(self.wfsa.alphabet))\n\n    @classmethod\n    def from_regex(cls, pattern, charset=None, to_bytes=True):\n        \"\"\"\n        Create a WFSA from a regex pattern.\n\n        Args:\n            pattern (str): The regex pattern to convert into a WFSA.\n            charset (set): The character set to use for negative character classes.\n                Defaults to characters in string.printable.\n            to_bytes (bool): Whether to convert the WFSA transitions to bytes.\n                Defaults to True. When set to False, the WFSA transitions will be strings.\n\n        Returns:\n            (WFSA): An instance of the WFSA class.\n\n        Note:\n            The transition weights are automatically normalized to form a probability distribution.\n            For each state, the weights of all outgoing transitions (including final state transitions)\n            sum to 1.0. This means if a state has n possible transitions, each transition will have\n            weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use `BoolFSA`.\n        \"\"\"\n        charset = charset or set(string.printable)\n        wfsa = interegular_to_wfsa(pattern, charset=charset)\n        if to_bytes:\n            wfsa = wfsa.to_bytes()\n        return cls(wfsa=wfsa)\n\n    @staticmethod\n    def _convert_to_log(wfsa):\n        \"\"\"Convert a WFSA from the Float semiring to the Log semiring.\"\"\"\n        assert wfsa.R is Float\n        assert isinstance(wfsa, BaseWFSA)\n        new = BaseWFSA(Log)\n\n        for i, w in wfsa.I:\n            new.add_I(i, Log(np.log(w)))\n\n        for i, w in wfsa.F:\n            new.add_F(i, Log(np.log(w)))\n\n        for i, a, j, w in wfsa.arcs():\n            new.add_arc(i, a, j, Log(np.log(w)))\n\n        return new\n\n    def _consume(self, bs):\n        # XXX implement cache eviction\n        bs = tuple(bs)\n\n        try:\n            return self.cache[bs]\n        except KeyError:\n            pass\n\n        wfsa = self.wfsa.epsremove\n        curr = wfsa.R.chart()\n        prev = self._consume(bs[:-1])\n        for i in prev:\n            for j, w in wfsa.arcs(i, bs[-1]):\n                curr[j] += prev[i] * w\n\n        self.cache[bs] = curr\n\n        return curr\n\n    async def complete(self, context):\n        \"\"\"\n        Computes the log weight of the context under the weighted language represented by the WFSA.\n\n        For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\\n\n        - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): Log weight of context under the WFSA.\n        \"\"\"\n        # TODO: optimize to use _consume cache\n        return self.wfsa(context).score\n\n    def _prefix(self, context):\n        curr = self._consume(context)\n\n        if not curr:\n            return float(\"-inf\"), curr\n\n        bkwd = self.wfsa.epsremove.backward\n        log_ctx_w = logsumexp([(curr[i] * bkwd[i]).score for i in curr])\n\n        if np.isnan(log_ctx_w):\n            return float(\"-inf\"), curr\n\n        return log_ctx_w, curr\n\n    async def prefix(self, context):\n        \"\"\"\n        Computes the prefix log weight of `context` under the WFSA.\n\n        This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n        For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n        - `prefix(\"ca\")` returns $\\\\log(w_{cat})$\\n\n        - `prefix(\"d\")` returns $-\\\\infty$ since the WFSA does not accept any sequences with prefix \"d\"\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): Log weight of `context` as a prefix under the WFSA.\n        \"\"\"\n        return self._prefix(context)[0]\n\n    async def logw_next(self, context):\n        \"\"\"Returns next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (LazyWeights): Log-weights for next token and EOS.\n        \"\"\"\n        log_ctx_w, curr = self._prefix(context)\n\n        if log_ctx_w == float(\"-inf\"):\n            raise ValueError(f\"Context {context!r} has zero weight.\")\n\n        bkwd = self.wfsa.epsremove.backward\n\n        ws = self.wfsa.R.chart()\n        for i in curr:\n            for b, j, w in self.wfsa.epsremove.arcs(i=i):\n                ws[b] += curr[i] * w * bkwd[j]\n\n        ws[self.eos] = self.wfsa.R.zero\n        for j, w in self.wfsa.epsremove.F:\n            ws[self.eos] += curr[j] * w\n\n        log_ws = np.array([ws[b].score for b in self.vocab_eos]) - log_ctx_w\n\n        return self.make_lazy_weights(log_ws)\n\n    def _repr_svg_(self):\n        return self.wfsa._repr_svg_()\n\n    def __repr__(self):\n        return f\"WFSA(wfsa={self.wfsa!r})\"\n\n    def spawn(self):\n        cls = type(self)\n        return cls(wfsa=self.wfsa)\n\n    def clear_cache(self):\n        self.cache = {(): self.wfsa.epsremove.start}\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WFSA.__init__","title":"<code>__init__(wfsa)</code>","text":"<p>Initializes the WFSA potential.</p> <p>Parameters:</p> Name Type Description Default <code>wfsa</code> <code>WFSA</code> <p>The weighted finite state automaton.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the semiring of the provided WFSA is not Float or Log.</p> Note <p>The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>def __init__(self, wfsa):\n    \"\"\"\n    Initializes the WFSA potential.\n\n    Args:\n        wfsa (genlm_grammar.WFSA): The weighted finite state automaton.\n\n    Raises:\n        ValueError: If the semiring of the provided WFSA is not Float or Log.\n\n    Note:\n        The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.\n    \"\"\"\n    if wfsa.R not in (Float, Log):\n        raise ValueError(f\"Unsupported semiring: {wfsa.R}\")\n\n    if wfsa.R is Float:\n        self.wfsa = self._convert_to_log(wfsa)\n    else:\n        self.wfsa = wfsa\n\n    self.cache = {(): self.wfsa.epsremove.start}\n    super().__init__(vocabulary=list(self.wfsa.alphabet))\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WFSA.from_regex","title":"<code>from_regex(pattern, charset=None, to_bytes=True)</code>  <code>classmethod</code>","text":"<p>Create a WFSA from a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regex pattern to convert into a WFSA.</p> required <code>charset</code> <code>set</code> <p>The character set to use for negative character classes. Defaults to characters in string.printable.</p> <code>None</code> <code>to_bytes</code> <code>bool</code> <p>Whether to convert the WFSA transitions to bytes. Defaults to True. When set to False, the WFSA transitions will be strings.</p> <code>True</code> <p>Returns:</p> Type Description <code>WFSA</code> <p>An instance of the WFSA class.</p> Note <p>The transition weights are automatically normalized to form a probability distribution. For each state, the weights of all outgoing transitions (including final state transitions) sum to 1.0. This means if a state has n possible transitions, each transition will have weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use <code>BoolFSA</code>.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>@classmethod\ndef from_regex(cls, pattern, charset=None, to_bytes=True):\n    \"\"\"\n    Create a WFSA from a regex pattern.\n\n    Args:\n        pattern (str): The regex pattern to convert into a WFSA.\n        charset (set): The character set to use for negative character classes.\n            Defaults to characters in string.printable.\n        to_bytes (bool): Whether to convert the WFSA transitions to bytes.\n            Defaults to True. When set to False, the WFSA transitions will be strings.\n\n    Returns:\n        (WFSA): An instance of the WFSA class.\n\n    Note:\n        The transition weights are automatically normalized to form a probability distribution.\n        For each state, the weights of all outgoing transitions (including final state transitions)\n        sum to 1.0. This means if a state has n possible transitions, each transition will have\n        weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use `BoolFSA`.\n    \"\"\"\n    charset = charset or set(string.printable)\n    wfsa = interegular_to_wfsa(pattern, charset=charset)\n    if to_bytes:\n        wfsa = wfsa.to_bytes()\n    return cls(wfsa=wfsa)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WFSA.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Computes the log weight of the context under the weighted language represented by the WFSA.</p> <p>For example, if the WFSA accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>complete(\"c\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WFSA</p> </li> <li> <p><code>complete(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>complete(\"d\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WFSA</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of context under the WFSA.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Computes the log weight of the context under the weighted language represented by the WFSA.\n\n    For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\\n\n    - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): Log weight of context under the WFSA.\n    \"\"\"\n    # TODO: optimize to use _consume cache\n    return self.wfsa(context).score\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WFSA.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Computes the prefix log weight of <code>context</code> under the WFSA.</p> <p>This corresponds to the log of the sum of the weights of all sequences with prefix <code>context</code>.</p> <p>For example, if the WFSA accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>prefix(\"c\")</code> returns \\(\\log(w_{cat} + w_{car})\\)</p> </li> <li> <p><code>prefix(\"ca\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>prefix(\"d\")</code> returns \\(-\\infty\\) since the WFSA does not accept any sequences with prefix \"d\"</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of <code>context</code> as a prefix under the WFSA.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Computes the prefix log weight of `context` under the WFSA.\n\n    This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n    For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n    - `prefix(\"ca\")` returns $\\\\log(w_{cat})$\\n\n    - `prefix(\"d\")` returns $-\\\\infty$ since the WFSA does not accept any sequences with prefix \"d\"\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): Log weight of `context` as a prefix under the WFSA.\n    \"\"\"\n    return self._prefix(context)[0]\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WFSA.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Returns next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Log-weights for next token and EOS.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Returns next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (LazyWeights): Log-weights for next token and EOS.\n    \"\"\"\n    log_ctx_w, curr = self._prefix(context)\n\n    if log_ctx_w == float(\"-inf\"):\n        raise ValueError(f\"Context {context!r} has zero weight.\")\n\n    bkwd = self.wfsa.epsremove.backward\n\n    ws = self.wfsa.R.chart()\n    for i in curr:\n        for b, j, w in self.wfsa.epsremove.arcs(i=i):\n            ws[b] += curr[i] * w * bkwd[j]\n\n    ws[self.eos] = self.wfsa.R.zero\n    for j, w in self.wfsa.epsremove.F:\n        ws[self.eos] += curr[j] * w\n\n    log_ws = np.array([ws[b].score for b in self.vocab_eos]) - log_ctx_w\n\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WCFG","title":"<code>WCFG</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A weighted context-free grammar potential.</p> <p>This class wraps a <code>genlm_grammar.CFG</code> and provides methods for computing the log-weight of a sequence, the prefix log-weight of a sequence, and the log-weights of the next token given a sequence.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>class WCFG(Potential):\n    \"\"\"\n    A weighted context-free grammar potential.\n\n    This class wraps a `genlm_grammar.CFG` and provides methods for computing the log-weight of a sequence,\n    the prefix log-weight of a sequence, and the log-weights of the next token given a sequence.\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Initialize the WCFG potential.\n\n        Args:\n            cfg (genlm_grammar.CFG): The context-free grammar configuration to use.\n                The CFG must in the Float semiring.\n        \"\"\"\n        # TODO: convert to LogSemiring to handle underflow\n        if cfg.R is not Float:\n            raise ValueError(\"cfg semiring must be Float\")\n        self.cfg = cfg  # cfg before prefix transform\n        self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n        self.model = Earley(self.cfg_eos.prefix_grammar)\n        super().__init__(vocabulary=list(cfg.V))\n\n    @classmethod\n    def from_string(cls, grammar, to_bytes=True, **kwargs):\n        \"\"\"Create a WCFG from a string.\n\n        Args:\n            grammar (str): The string grammar specification to create the WCFG from.\n            to_bytes (bool, optional): Whether to convert the WCFG terminals to indivudual bytes.\n                Defaults to True.\n            **kwargs (dict): Additional arguments passed to the WCFG constructor.\n\n        Returns:\n            (WCFG): The created WCFG.\n        \"\"\"\n        cfg = CFG.from_string(grammar, Float)\n        if to_bytes:\n            cfg = cfg.to_bytes()\n        return cls(cfg, **kwargs)\n\n    async def complete(self, context):\n        \"\"\"\n        Compute the log weight of `context` under the WCFG.\n\n        For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\\n\n        - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (float): The log weight of `context` under the WCFG.\n        \"\"\"\n        w = self.model([*context, EOS])\n        return np.log(w) if w &gt; 0 else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Compute the log prefix weight of `context` under the WCFG.\n\n        This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n        For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n        - `prefix(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `prefix(\"d\")` returns $-\\\\infty$ since the WCFG does not accept any sequences with prefix \"d\"\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (float): The log prefix weight of `context` under the WCFG.\n        \"\"\"\n        w = self.model(context)\n        return np.log(w) if w &gt; 0 else float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute the next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (LazyWeights): The log weights for the next tokens and EOS given `context`.\n        \"\"\"\n        ws = self.model.next_token_weights(self.model.chart(context))\n        ws = ws.trim().normalize()\n\n        ws_array = np.array([ws[x] for x in self.vocab_eos])\n        mask = ws_array &gt; 0\n        log_ws = np.full_like(ws_array, float(\"-inf\"), dtype=np.float64)\n        log_ws[mask] = np.log(ws_array[mask])\n\n        return self.make_lazy_weights(log_ws)\n\n    def clear_cache(self):\n        \"\"\"Clear the internal cache of the parser.\"\"\"\n        self.model.clear_cache()\n\n    def __repr__(self):\n        return f\"WCFG(cfg={self.cfg!r})\"\n\n    def _repr_html_(self):\n        return self.cfg._repr_html_()\n\n    def spawn(self):\n        \"\"\"Spawn a new WCFG.\"\"\"\n        return WCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WCFG.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Initialize the WCFG potential.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CFG</code> <p>The context-free grammar configuration to use. The CFG must in the Float semiring.</p> required Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def __init__(self, cfg):\n    \"\"\"\n    Initialize the WCFG potential.\n\n    Args:\n        cfg (genlm_grammar.CFG): The context-free grammar configuration to use.\n            The CFG must in the Float semiring.\n    \"\"\"\n    # TODO: convert to LogSemiring to handle underflow\n    if cfg.R is not Float:\n        raise ValueError(\"cfg semiring must be Float\")\n    self.cfg = cfg  # cfg before prefix transform\n    self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n    self.model = Earley(self.cfg_eos.prefix_grammar)\n    super().__init__(vocabulary=list(cfg.V))\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WCFG.from_string","title":"<code>from_string(grammar, to_bytes=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a WCFG from a string.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The string grammar specification to create the WCFG from.</p> required <code>to_bytes</code> <code>bool</code> <p>Whether to convert the WCFG terminals to indivudual bytes. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the WCFG constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>WCFG</code> <p>The created WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>@classmethod\ndef from_string(cls, grammar, to_bytes=True, **kwargs):\n    \"\"\"Create a WCFG from a string.\n\n    Args:\n        grammar (str): The string grammar specification to create the WCFG from.\n        to_bytes (bool, optional): Whether to convert the WCFG terminals to indivudual bytes.\n            Defaults to True.\n        **kwargs (dict): Additional arguments passed to the WCFG constructor.\n\n    Returns:\n        (WCFG): The created WCFG.\n    \"\"\"\n    cfg = CFG.from_string(grammar, Float)\n    if to_bytes:\n        cfg = cfg.to_bytes()\n    return cls(cfg, **kwargs)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WCFG.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Compute the log weight of <code>context</code> under the WCFG.</p> <p>For example, if the WCFG accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>complete(\"c\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WCFG</p> </li> <li> <p><code>complete(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>complete(\"d\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WCFG</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log weight of <code>context</code> under the WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Compute the log weight of `context` under the WCFG.\n\n    For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\\n\n    - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (float): The log weight of `context` under the WCFG.\n    \"\"\"\n    w = self.model([*context, EOS])\n    return np.log(w) if w &gt; 0 else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WCFG.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Compute the log prefix weight of <code>context</code> under the WCFG.</p> <p>This corresponds to the log of the sum of the weights of all sequences with prefix <code>context</code>.</p> <p>For example, if the WCFG accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>prefix(\"c\")</code> returns \\(\\log(w_{cat} + w_{car})\\)</p> </li> <li> <p><code>prefix(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>prefix(\"d\")</code> returns \\(-\\infty\\) since the WCFG does not accept any sequences with prefix \"d\"</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log prefix weight of <code>context</code> under the WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Compute the log prefix weight of `context` under the WCFG.\n\n    This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n    For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n    - `prefix(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `prefix(\"d\")` returns $-\\\\infty$ since the WCFG does not accept any sequences with prefix \"d\"\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (float): The log prefix weight of `context` under the WCFG.\n    \"\"\"\n    w = self.model(context)\n    return np.log(w) if w &gt; 0 else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WCFG.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>The log weights for the next tokens and EOS given <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute the next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (LazyWeights): The log weights for the next tokens and EOS given `context`.\n    \"\"\"\n    ws = self.model.next_token_weights(self.model.chart(context))\n    ws = ws.trim().normalize()\n\n    ws_array = np.array([ws[x] for x in self.vocab_eos])\n    mask = ws_array &gt; 0\n    log_ws = np.full_like(ws_array, float(\"-inf\"), dtype=np.float64)\n    log_ws[mask] = np.log(ws_array[mask])\n\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WCFG.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the internal cache of the parser.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the internal cache of the parser.\"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.WCFG.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def spawn(self):\n    \"\"\"Spawn a new WCFG.\"\"\"\n    return WCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.CanonicalTokenization","title":"<code>CanonicalTokenization</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A custom potential that enforces canonical BPE tokenization.</p> <p>This potential ensures that tokens follow the canonical tokenization rules by using the FastCanonicalityFilterBPE under the hood.</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>class CanonicalTokenization(Potential):\n    \"\"\"\n    A custom potential that enforces canonical BPE tokenization.\n\n    This potential ensures that tokens follow the canonical tokenization rules\n    by using the FastCanonicalityFilterBPE under the hood.\n    \"\"\"\n\n    def __init__(self, canonicality_filter):\n        \"\"\"\n        Initialize the Canonical Potential\n\n        Args:\n            canonicality_filter (FastCanonicalityFilterBPE): An initialized FastCanonicalityFilterBPE instance.\n        \"\"\"\n        # Store the pre-initialized filter and tokenizer\n        self.canonicality_filter = canonicality_filter\n\n        # IMPORTANT: In the base Potential class, EOS will be added to vocab automatically\n        # So we should NOT add it ourselves to the vocabulary we pass to super().__init__\n        vocabulary = self.canonicality_filter._decode\n        super().__init__(vocabulary)\n\n    @classmethod\n    def from_llm(cls, llm):\n        \"\"\"\n        Factory method to create CanonicalTokenization from a PromptedLLM instance.\n\n        Args:\n            llm (PromptedLLM): An instance of PromptedLLM containing the model and tokenizer.\n\n        Returns:\n            (CanonicalTokenization): An initialized CanonicalTokenization instance.\n        \"\"\"\n        if not isinstance(llm, PromptedLLM):\n            raise TypeError(\n                f\"Expected llm to be an instance of PromptedLLM, got {type(llm)}\"\n            )\n\n        # Extract necessary components from llm\n        tokenizer = llm.model.tokenizer\n        eos_token_ids = llm.token_maps.eos_idxs\n        model_name = tokenizer.name_or_path\n\n        # Create the filter using its factory method\n        canonicality_filter = FastCanonicalityFilterBPE.from_tokenizer(\n            tokenizer, eos_token_ids\n        )\n\n        # Set overrides on the filter\n        canonicality_filter.set_overrides(model_name)\n\n        # Call __init__ with the created filter and tokenizer\n        return cls(canonicality_filter)\n\n    async def complete(self, context):\n        \"\"\"\n        Assess if a complete sequence follows canonical tokenization.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (float): 0.0 if canonical, float('-inf') otherwise\n        \"\"\"\n        # Empty sequences are considered canonical\n        if not context:\n            return 0.0\n\n        # Check if the sequence is canonical\n        is_canonical = self._check_canonicality(context)\n        return 0.0 if is_canonical else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Assess if a prefix sequence could potentially extend to a canonical sequence.\n        For canonicality, this is the same as complete.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (float): 0.0 if potentially canonical, float('-inf') otherwise\n        \"\"\"\n        return await self.complete(context)\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute weights for each possible next token given the context.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (LazyWeights): Weights for each token in the vocabulary and EOS\n        \"\"\"\n        # Get the prefix weight (to check if context itself is canonical)\n        ctx_log_w = await self.prefix(context)\n\n        if ctx_log_w == float(\"-inf\"):\n            raise ValueError(\"Context is non-canonical\")\n        else:\n            if context:\n                t = (None, context[-1])\n                filter_mask = self.canonicality_filter(t)\n            else:\n                filter_mask = np.ones(len(self.canonicality_filter._decode), dtype=bool)\n\n            # Create log weights directly instead of using np.log(filter_mask)\n            # This is more efficient, avoids torch (with torch can't combine with other potentials!)\n            logws_no_eos = np.where(filter_mask, 0.0, float(\"-inf\")).astype(np.float32)\n\n            # append eos to the logws, always allow eos.\n            # NOTE: concat is because ._decode does not include eos while .vocab_eos does\n            logws = np.concatenate([logws_no_eos, np.array([0.0], dtype=np.float32)])\n\n        return self.make_lazy_weights(logws)\n\n    def _check_canonicality(self, context):\n        \"\"\"\n        Check if a sequence follows canonical tokenization.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (bool): True if the sequence is canonical, False otherwise\n        \"\"\"\n        # If we're checking a single token, it's always canonical\n        if len(context) &lt;= 1:\n            return True\n\n        # Check all adjacent token pairs for canonicality\n        for i in range(1, len(context)):\n            prev_token = context[i - 1]\n            current_token = context[i]\n\n            # Format expected by the filter: (None, previous_token)\n            t = (None, prev_token)\n            mask = self.canonicality_filter(t)\n            # print(\"percent of mask: \", np.sum(mask)*100 / len(mask))\n\n            # Find token_id in the canonicality filter's vocabulary\n            token_id = self.canonicality_filter._encode[current_token]\n            if not mask[token_id]:\n                return False\n\n        return True\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.CanonicalTokenization.__init__","title":"<code>__init__(canonicality_filter)</code>","text":"<p>Initialize the Canonical Potential</p> <p>Parameters:</p> Name Type Description Default <code>canonicality_filter</code> <code>FastCanonicalityFilterBPE</code> <p>An initialized FastCanonicalityFilterBPE instance.</p> required Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>def __init__(self, canonicality_filter):\n    \"\"\"\n    Initialize the Canonical Potential\n\n    Args:\n        canonicality_filter (FastCanonicalityFilterBPE): An initialized FastCanonicalityFilterBPE instance.\n    \"\"\"\n    # Store the pre-initialized filter and tokenizer\n    self.canonicality_filter = canonicality_filter\n\n    # IMPORTANT: In the base Potential class, EOS will be added to vocab automatically\n    # So we should NOT add it ourselves to the vocabulary we pass to super().__init__\n    vocabulary = self.canonicality_filter._decode\n    super().__init__(vocabulary)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.CanonicalTokenization.from_llm","title":"<code>from_llm(llm)</code>  <code>classmethod</code>","text":"<p>Factory method to create CanonicalTokenization from a PromptedLLM instance.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>PromptedLLM</code> <p>An instance of PromptedLLM containing the model and tokenizer.</p> required <p>Returns:</p> Type Description <code>CanonicalTokenization</code> <p>An initialized CanonicalTokenization instance.</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>@classmethod\ndef from_llm(cls, llm):\n    \"\"\"\n    Factory method to create CanonicalTokenization from a PromptedLLM instance.\n\n    Args:\n        llm (PromptedLLM): An instance of PromptedLLM containing the model and tokenizer.\n\n    Returns:\n        (CanonicalTokenization): An initialized CanonicalTokenization instance.\n    \"\"\"\n    if not isinstance(llm, PromptedLLM):\n        raise TypeError(\n            f\"Expected llm to be an instance of PromptedLLM, got {type(llm)}\"\n        )\n\n    # Extract necessary components from llm\n    tokenizer = llm.model.tokenizer\n    eos_token_ids = llm.token_maps.eos_idxs\n    model_name = tokenizer.name_or_path\n\n    # Create the filter using its factory method\n    canonicality_filter = FastCanonicalityFilterBPE.from_tokenizer(\n        tokenizer, eos_token_ids\n    )\n\n    # Set overrides on the filter\n    canonicality_filter.set_overrides(model_name)\n\n    # Call __init__ with the created filter and tokenizer\n    return cls(canonicality_filter)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.CanonicalTokenization.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Assess if a complete sequence follows canonical tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>float</code> <p>0.0 if canonical, float('-inf') otherwise</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Assess if a complete sequence follows canonical tokenization.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (float): 0.0 if canonical, float('-inf') otherwise\n    \"\"\"\n    # Empty sequences are considered canonical\n    if not context:\n        return 0.0\n\n    # Check if the sequence is canonical\n    is_canonical = self._check_canonicality(context)\n    return 0.0 if is_canonical else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.CanonicalTokenization.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Assess if a prefix sequence could potentially extend to a canonical sequence. For canonicality, this is the same as complete.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>float</code> <p>0.0 if potentially canonical, float('-inf') otherwise</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Assess if a prefix sequence could potentially extend to a canonical sequence.\n    For canonicality, this is the same as complete.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (float): 0.0 if potentially canonical, float('-inf') otherwise\n    \"\"\"\n    return await self.complete(context)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.CanonicalTokenization.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute weights for each possible next token given the context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Weights for each token in the vocabulary and EOS</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute weights for each possible next token given the context.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (LazyWeights): Weights for each token in the vocabulary and EOS\n    \"\"\"\n    # Get the prefix weight (to check if context itself is canonical)\n    ctx_log_w = await self.prefix(context)\n\n    if ctx_log_w == float(\"-inf\"):\n        raise ValueError(\"Context is non-canonical\")\n    else:\n        if context:\n            t = (None, context[-1])\n            filter_mask = self.canonicality_filter(t)\n        else:\n            filter_mask = np.ones(len(self.canonicality_filter._decode), dtype=bool)\n\n        # Create log weights directly instead of using np.log(filter_mask)\n        # This is more efficient, avoids torch (with torch can't combine with other potentials!)\n        logws_no_eos = np.where(filter_mask, 0.0, float(\"-inf\")).astype(np.float32)\n\n        # append eos to the logws, always allow eos.\n        # NOTE: concat is because ._decode does not include eos while .vocab_eos does\n        logws = np.concatenate([logws_no_eos, np.array([0.0], dtype=np.float32)])\n\n    return self.make_lazy_weights(logws)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.SMC","title":"<code>SMC</code>","text":"<p>This class implements sequential Monte Carlo (SMC) inference for controlled text generation. The generation process works as follows:</p> <ol> <li> <p>Token Sampling: At each step, the <code>unit_sampler</code> is used to extend each particle (candidate sequence)    by sampling a new token. This grows all sequences by one token at a time. The sampler also outputs    an importance weight with each extension to correct for the myopic nature of token-by-token sampling.</p> </li> <li> <p>Critic Evaluation: If a <code>critic</code> is provided, it scores the updated sequences (via it's <code>score</code> method),    reweighting the particles based on how well they satisfy the constraints encoded by the critic.</p> </li> <li> <p>Resampling: When the effective sample size (ESS) falls below the threshold,    particles are resampled according to their weights. This helps focus computation    on more promising sequences.</p> </li> <li> <p>Termination: The process continues until either:</p> <ul> <li> <p>All sequences reach an end-of-sequence (EOS) token</p> </li> <li> <p>The maximum token length is reached</p> </li> </ul> </li> </ol> <p>If a critic is provided, the resulting sequences are properly weighted with respect to the product of the unit sampler's target potential and the critic potential (<code>unit_sampler.target * critic</code>). If a critic is not provided, the resulting sequences are weighted with respect to the unit sampler's target potential.</p> <p>Parameters:</p> Name Type Description Default <code>unit_sampler</code> <code>TokenSampler</code> <p>The sampler that generates tokens.</p> required <code>critic</code> <code>Potential</code> <p>A potential function that guides the generation process by scoring candidate sequences. Must have the same token type as the unit_sampler.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unit_sampler is not a TokenSampler, if critic is not a Potential, or if the token types of unit_sampler and critic don't match.</p> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>class SMC:\n    \"\"\"This class implements sequential Monte Carlo (SMC) inference for controlled text generation.\n    The generation process works as follows:\n\n    1. Token Sampling: At each step, the `unit_sampler` is used to extend each particle (candidate sequence)\n       by sampling a new token. This grows all sequences by one token at a time. The sampler also outputs\n       an importance weight with each extension to correct for the myopic nature of token-by-token sampling.\n\n    2. Critic Evaluation: If a `critic` is provided, it scores the updated sequences (via it's `score` method),\n       reweighting the particles based on how well they satisfy the constraints encoded by the critic.\n\n    3. Resampling: When the effective sample size (ESS) falls below the threshold,\n       particles are resampled according to their weights. This helps focus computation\n       on more promising sequences.\n\n    4. Termination: The process continues until either:\\n\n        - All sequences reach an end-of-sequence (EOS) token\\n\n        - The maximum token length is reached\n\n    If a critic is provided, the resulting sequences are properly weighted with respect to the product of the unit sampler's\n    target potential and the critic potential (`unit_sampler.target * critic`). If a critic is not provided,\n    the resulting sequences are weighted with respect to the unit sampler's target potential.\n\n    Args:\n        unit_sampler (TokenSampler): The sampler that generates tokens.\n        critic (Potential, optional): A potential function that guides the generation process\n            by scoring candidate sequences. Must have the same token type as the unit_sampler.\n\n    Raises:\n        ValueError: If unit_sampler is not a TokenSampler, if critic is not a Potential,\n            or if the token types of unit_sampler and critic don't match.\n    \"\"\"\n\n    def __init__(self, unit_sampler, critic=None):\n        if not isinstance(unit_sampler, TokenSampler):\n            raise ValueError(\"`unit_sampler` must be a TokenSampler\")\n\n        if critic:\n            if not isinstance(critic, Potential):\n                raise ValueError(\"`critic` must be a Potential\")\n            if not unit_sampler.token_type == critic.token_type:\n                raise ValueError(\n                    \"`critic` must have the same token type as the `unit_sampler`. \"\n                    f\"Got {unit_sampler.token_type} and {critic.token_type}.\"\n                    + (\n                        \"\\nMaybe you forgot to coerce the critic to the token type of the unit sampler? See `Coerce`.\"\n                        if unit_sampler.token_type.is_iterable_of(critic.token_type)\n                        else \"\"\n                    )\n                )\n\n        self.unit_sampler = unit_sampler\n        self.critic = critic\n\n    async def __call__(\n        self,\n        n_particles,\n        ess_threshold,\n        max_tokens,\n        verbosity=0,\n        json_path=None,\n        **kwargs,\n    ):\n        \"\"\"Generate sequences using sequential Monte Carlo inference.\n\n        Args:\n            n_particles (int): Number of particles (candidate sequences) to maintain during\n                generation. Higher values provide better exploration but require more\n                computation.\n            ess_threshold (float): Effective sample size threshold for resampling,\n                expressed as a fraction of the number of particles. When ESS falls below\n                this value, particles are resampled according to their weights. Should be between 0 and 1.\n                Higher values lead to more frequent resampling. Note that when ess_threshold = 0,\n                the critic is only applied at the end of the generation (if it is provided).\n            max_tokens (int): Maximum number of tokens to generate per sequence. Generation\n                may terminate earlier if all sequences reach an EOS token.\n            verbosity (int, optional): Verbosity level for the SMC algorithm. 0 is silent, 1 prints the\n                particles at each step. Default is 0.\n            json_path (str, optional): JSON file path for saving a record of the inference run.\n                This can be used in conjunction with the `InferenceVisualizer` to visualize the inference run.\n            **kwargs (dict): Additional keyword arguments to pass to the SMC algorithm.\n                See the `llamppl.inference.smc_standard` documentation for more details.\n\n        Returns:\n            (Sequences): A container holding the generated sequences, their importance weights, and\n                other metadata from the generation process.\n        \"\"\"\n        model = SequenceModel(\n            unit_sampler=self.unit_sampler,\n            critic=self.critic,\n            max_tokens=max_tokens,\n            verbosity=verbosity,\n            twist_with_critic=ess_threshold &gt; 0,\n        )\n\n        particles = await smc_standard(\n            model=model,\n            n_particles=n_particles,\n            ess_threshold=ess_threshold,\n            json_file=json_path,\n            **kwargs,\n        )\n\n        return Sequences(*_unpack_particles(particles))\n\n    async def cleanup(self):\n        \"\"\"Clean up resources used by the inference engine.\n\n        This method should be called when the InferenceEngine is no longer needed.\n\n        Example:\n            ```python\n            sampler = SequenceSampler(unit_sampler, critic)\n            try:\n                sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\n            finally:\n                await sampler.cleanup()\n            ```\n        \"\"\"\n        await self.unit_sampler.cleanup()\n        if self.critic:\n            await self.critic.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.SMC.__call__","title":"<code>__call__(n_particles, ess_threshold, max_tokens, verbosity=0, json_path=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate sequences using sequential Monte Carlo inference.</p> <p>Parameters:</p> Name Type Description Default <code>n_particles</code> <code>int</code> <p>Number of particles (candidate sequences) to maintain during generation. Higher values provide better exploration but require more computation.</p> required <code>ess_threshold</code> <code>float</code> <p>Effective sample size threshold for resampling, expressed as a fraction of the number of particles. When ESS falls below this value, particles are resampled according to their weights. Should be between 0 and 1. Higher values lead to more frequent resampling. Note that when ess_threshold = 0, the critic is only applied at the end of the generation (if it is provided).</p> required <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to generate per sequence. Generation may terminate earlier if all sequences reach an EOS token.</p> required <code>verbosity</code> <code>int</code> <p>Verbosity level for the SMC algorithm. 0 is silent, 1 prints the particles at each step. Default is 0.</p> <code>0</code> <code>json_path</code> <code>str</code> <p>JSON file path for saving a record of the inference run. This can be used in conjunction with the <code>InferenceVisualizer</code> to visualize the inference run.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the SMC algorithm. See the <code>llamppl.inference.smc_standard</code> documentation for more details.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequences</code> <p>A container holding the generated sequences, their importance weights, and other metadata from the generation process.</p> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>async def __call__(\n    self,\n    n_particles,\n    ess_threshold,\n    max_tokens,\n    verbosity=0,\n    json_path=None,\n    **kwargs,\n):\n    \"\"\"Generate sequences using sequential Monte Carlo inference.\n\n    Args:\n        n_particles (int): Number of particles (candidate sequences) to maintain during\n            generation. Higher values provide better exploration but require more\n            computation.\n        ess_threshold (float): Effective sample size threshold for resampling,\n            expressed as a fraction of the number of particles. When ESS falls below\n            this value, particles are resampled according to their weights. Should be between 0 and 1.\n            Higher values lead to more frequent resampling. Note that when ess_threshold = 0,\n            the critic is only applied at the end of the generation (if it is provided).\n        max_tokens (int): Maximum number of tokens to generate per sequence. Generation\n            may terminate earlier if all sequences reach an EOS token.\n        verbosity (int, optional): Verbosity level for the SMC algorithm. 0 is silent, 1 prints the\n            particles at each step. Default is 0.\n        json_path (str, optional): JSON file path for saving a record of the inference run.\n            This can be used in conjunction with the `InferenceVisualizer` to visualize the inference run.\n        **kwargs (dict): Additional keyword arguments to pass to the SMC algorithm.\n            See the `llamppl.inference.smc_standard` documentation for more details.\n\n    Returns:\n        (Sequences): A container holding the generated sequences, their importance weights, and\n            other metadata from the generation process.\n    \"\"\"\n    model = SequenceModel(\n        unit_sampler=self.unit_sampler,\n        critic=self.critic,\n        max_tokens=max_tokens,\n        verbosity=verbosity,\n        twist_with_critic=ess_threshold &gt; 0,\n    )\n\n    particles = await smc_standard(\n        model=model,\n        n_particles=n_particles,\n        ess_threshold=ess_threshold,\n        json_file=json_path,\n        **kwargs,\n    )\n\n    return Sequences(*_unpack_particles(particles))\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.SMC.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Clean up resources used by the inference engine.</p> <p>This method should be called when the InferenceEngine is no longer needed.</p> Example <pre><code>sampler = SequenceSampler(unit_sampler, critic)\ntry:\n    sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\nfinally:\n    await sampler.cleanup()\n</code></pre> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Clean up resources used by the inference engine.\n\n    This method should be called when the InferenceEngine is no longer needed.\n\n    Example:\n        ```python\n        sampler = SequenceSampler(unit_sampler, critic)\n        try:\n            sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\n        finally:\n            await sampler.cleanup()\n        ```\n    \"\"\"\n    await self.unit_sampler.cleanup()\n    if self.critic:\n        await self.critic.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.direct_token_sampler","title":"<code>direct_token_sampler(potential)</code>","text":"<p>Create a <code>DirectTokenSampler</code> that samples directly from a potential's vocabulary.</p> <p>See <code>DirectTokenSampler</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The potential function to sample from. Should have an efficient logw_next method.</p> required <p>Returns:</p> Type Description <code>DirectTokenSampler</code> <p>A sampler that directly samples tokens from the potential's vocabulary.</p> Source code in <code>genlm/control/sampler/__init__.py</code> <pre><code>def direct_token_sampler(potential):\n    \"\"\"Create a `DirectTokenSampler` that samples directly from a potential's vocabulary.\n\n    See `DirectTokenSampler` for more details.\n\n    Args:\n        potential (Potential): The potential function to sample from. Should have an efficient logw_next method.\n\n    Returns:\n        (DirectTokenSampler): A sampler that directly samples tokens from the potential's vocabulary.\n    \"\"\"\n    assert isinstance(potential, Potential)\n    return DirectTokenSampler(potential)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.eager_token_sampler","title":"<code>eager_token_sampler(iter_potential, item_potential)</code>","text":"<p>Create a <code>SetTokenSampler</code> that uses the <code>EagerSetSampler</code> to sample a set of tokens.</p> <p>See <code>EagerSetSampler</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>iter_potential</code> <code>Potential</code> <p>A potential function defined over a vocabulary of iterables.</p> required <code>item_potential</code> <code>Potential</code> <p>A potential function defined over a vocabulary of items which are elements of the iterables.</p> required <p>Returns:</p> Type Description <code>SetTokenSampler</code> <p>A sampler that wraps an <code>EagerSetSampler</code>.</p> Note <p>This is the fastest sampler in most cases.</p> Source code in <code>genlm/control/sampler/__init__.py</code> <pre><code>def eager_token_sampler(iter_potential, item_potential):\n    \"\"\"Create a `SetTokenSampler` that uses the `EagerSetSampler` to sample a set of tokens.\n\n    See `EagerSetSampler` for more details.\n\n    Args:\n        iter_potential (Potential): A potential function defined over a vocabulary of iterables.\n        item_potential (Potential): A potential function defined over a vocabulary of items which are elements of the iterables.\n\n    Returns:\n        (SetTokenSampler): A sampler that wraps an `EagerSetSampler`.\n\n    Note:\n        This is the fastest sampler in most cases.\n    \"\"\"\n    return SetTokenSampler(EagerSetSampler(iter_potential, item_potential))\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.topk_token_sampler","title":"<code>topk_token_sampler(iter_potential, item_potential, K)</code>","text":"<p>Create a <code>SetTokenSampler</code> that uses the <code>TopKSetSampler</code> to sample a set of tokens.</p> <p>See <code>TopKSetSampler</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>iter_potential</code> <code>Potential</code> <p>A potential function defined over a vocabulary of iterables.</p> required <code>item_potential</code> <code>Potential</code> <p>A potential function defined over a vocabulary of items which are elements of the iterables.</p> required <code>K</code> <code>int | None</code> <p>The <code>K</code> parameter for the <code>TopKSetSampler</code>.</p> required <p>Returns:</p> Type Description <code>SetTokenSampler</code> <p>A sampler that wraps an <code>TopKSetSampler</code>.</p> Source code in <code>genlm/control/sampler/__init__.py</code> <pre><code>def topk_token_sampler(iter_potential, item_potential, K):\n    \"\"\"Create a `SetTokenSampler` that uses the `TopKSetSampler` to sample a set of tokens.\n\n    See `TopKSetSampler` for more details.\n\n    Args:\n        iter_potential (Potential): A potential function defined over a vocabulary of iterables.\n        item_potential (Potential): A potential function defined over a vocabulary of items which are elements of the iterables.\n        K (int|None): The `K` parameter for the `TopKSetSampler`.\n\n    Returns:\n        (SetTokenSampler): A sampler that wraps an `TopKSetSampler`.\n    \"\"\"\n    return SetTokenSampler(TopKSetSampler(iter_potential, item_potential, K))\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.AWRS","title":"<code>AWRS</code>","text":"<p>               Bases: <code>TokenSampler</code></p> <p>Samples individual tokens through an adaptive weighted rejection sampling algorithm.</p> <p>This sampler is based on the algorithm described in Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</p> <p>It draws properly weighted samples from the product of a non-boolean potential and a boolean condition.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The non-boolean potential.</p> required <code>condition</code> <code>Potential</code> <p>The boolean condition. This potential must only output boolean values (0 or -inf in log-space).</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>42</code> <code>prune_logws</code> <code>bool</code> <p>Whether to prune the logws to only include the tokens in the intersection of the potential and condition vocabularies</p> <code>True</code> <code>proper_weights</code> <code>bool</code> <p>Whether to return properly weighted samples. If False, the sampler will only run one round of adaptive rejection sampling.</p> <code>True</code> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>class AWRS(TokenSampler):\n    \"\"\"Samples individual tokens through an adaptive weighted rejection sampling algorithm.\n\n    This sampler is based on the algorithm described in [Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling](https://arxiv.org/abs/2504.05410)\n\n    It draws properly weighted samples from the product of a non-boolean potential and a boolean condition.\n\n    Args:\n        potential (Potential): The non-boolean potential.\n        condition (Potential): The boolean condition. This potential must only output boolean values (0 or -inf in log-space).\n        seed (int): The seed for the random number generator.\n        prune_logws (bool): Whether to prune the logws to only include the tokens in the intersection of the potential and condition vocabularies\n        proper_weights (bool): Whether to return properly weighted samples.\n            If False, the sampler will only run one round of adaptive rejection sampling.\n    \"\"\"\n\n    def __init__(\n        self, potential, condition, seed=42, prune_logws=True, proper_weights=True\n    ):\n        super().__init__(target=potential * condition)\n        self.potential = potential\n        self.condition = condition\n\n        self.prune_logws = prune_logws\n        self.proper_weights = proper_weights\n        self.valid_idxs = np.array(\n            [self.potential.lookup[t] for t in self.target.vocab_eos]\n        )\n\n        self.vocab_eos_set = set(self.target.vocab_eos)\n        self.V = len(self.potential.vocab_eos)\n        self.rng = np.random.default_rng(seed=seed)\n\n    def _prune_logws(self, logws):\n        # Prune the logws to only include the tokens in the\n        # target vocabulary. (This zeros-out tokens which we know a priori\n        # will be rejected.) Note: We need an additional correction term\n        # to account for the fact that we're throwing away some probability mass.\n        # This should be handled in `sample`.\n        pruned = self.potential.alloc_logws()\n        pruned[self.valid_idxs] = logws.weights[self.valid_idxs]\n        logws.weights = pruned\n        return logws\n\n    async def _accept(self, context, token, verbosity=0):\n        if self.prune_logws or token in self.vocab_eos_set:\n            if token is self.target.eos:\n                logscore = await self.condition.complete(context)\n            else:\n                logscore = await self.condition.prefix(context + [token])\n            assert logscore in {-np.inf, 0}, \"`condition` must be Boolean\"\n        else:\n            logscore = -np.inf\n\n        do_accept = logscore == 0\n\n        if verbosity &gt; 0:\n            if do_accept:\n                print(colors.green % f\". {repr(token)}\")\n            else:\n                print(colors.red % \".\", end=\"\")\n\n        return do_accept\n\n    async def sample(self, context, verbosity=0):\n        \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method via adaptive weighted rejection sampling.\n\n        The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n        Returns:\n            (token, weight, np.nan): A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.\n        \"\"\"\n        logws = await self.potential.logw_next(context)\n        if self.prune_logws:\n            logws = self._prune_logws(logws)\n\n        logZ = logsumexp(logws.weights)\n        logps = logws.weights - logZ\n        toks = logws.decode\n\n        tok, nrej, logp0 = None, 0, []\n        for _ in range(2):\n            keys = logps - np.log(-np.log(self.rng.random((self.V,))))\n            order = np.argsort(-keys)\n            for rank in range(logps.size):\n                item = order[rank]\n                if keys[item] == -np.inf:\n                    break\n                if await self._accept(context, toks[item], verbosity):\n                    if tok is None:\n                        tok = toks[item]\n                    break\n                else:\n                    nrej += 1\n                    if tok is None:\n                        logp0.append(logps[item])\n                    logps[item] = -np.inf\n\n            if not self.proper_weights:\n                if tok is None:\n                    return self.target.eos, float(\"-inf\"), np.nan\n                return tok, 0, np.nan\n\n        if tok is None:  # No token was accepted, return EOS and kill the particle.\n            return self.target.eos, float(\"-inf\"), np.nan\n\n        if not logp0:  # Success on first try.\n            logw = logZ - np.log(nrej + 1)\n        else:\n            logw = logZ + log1mexp(logsumexp(logp0)) - np.log(nrej + 1)\n\n        return tok, logw, np.nan\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.AWRS.sample","title":"<code>sample(context, verbosity=0)</code>  <code>async</code>","text":"<p>Sample a token and weight that are properly weighted with respect to the target potential's <code>logw_next</code> method via adaptive weighted rejection sampling.</p> <p>The returned weight corresponds to the log normalizing constant of \\(\\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1})\\).</p> <p>Returns:</p> Type Description <code>(token, weight, nan)</code> <p>A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def sample(self, context, verbosity=0):\n    \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method via adaptive weighted rejection sampling.\n\n    The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n    Returns:\n        (token, weight, np.nan): A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.\n    \"\"\"\n    logws = await self.potential.logw_next(context)\n    if self.prune_logws:\n        logws = self._prune_logws(logws)\n\n    logZ = logsumexp(logws.weights)\n    logps = logws.weights - logZ\n    toks = logws.decode\n\n    tok, nrej, logp0 = None, 0, []\n    for _ in range(2):\n        keys = logps - np.log(-np.log(self.rng.random((self.V,))))\n        order = np.argsort(-keys)\n        for rank in range(logps.size):\n            item = order[rank]\n            if keys[item] == -np.inf:\n                break\n            if await self._accept(context, toks[item], verbosity):\n                if tok is None:\n                    tok = toks[item]\n                break\n            else:\n                nrej += 1\n                if tok is None:\n                    logp0.append(logps[item])\n                logps[item] = -np.inf\n\n        if not self.proper_weights:\n            if tok is None:\n                return self.target.eos, float(\"-inf\"), np.nan\n            return tok, 0, np.nan\n\n    if tok is None:  # No token was accepted, return EOS and kill the particle.\n        return self.target.eos, float(\"-inf\"), np.nan\n\n    if not logp0:  # Success on first try.\n        logw = logZ - np.log(nrej + 1)\n    else:\n        logw = logZ + log1mexp(logsumexp(logp0)) - np.log(nrej + 1)\n\n    return tok, logw, np.nan\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.InferenceVisualizer","title":"<code>InferenceVisualizer</code>","text":"<p>Web-based visualization server for SMC inference results.</p> <p>This class is intended to be used in conjunction with the <code>InferenceEngine</code> class.</p> Example <pre><code>from genlm.control import InferenceVisualizer\n# create the visualizer\nviz = InferenceVisualizer()\n# run inference and save the record to a JSON file\nsequences = await token_sampler.smc(\n    n_particles=10,\n    max_tokens=20,\n    ess_threshold=0.5,\n    json_path=\"smc_record.json\" # save the record to a JSON file\n)\n# visualize the inference run\nviz.visualize(\"smc_record.json\")\n# clean up visualization server\nviz.shutdown_server()\n</code></pre> Source code in <code>genlm/control/viz.py</code> <pre><code>class InferenceVisualizer:\n    \"\"\"Web-based visualization server for SMC inference results.\n\n    This class is intended to be used in conjunction with the `InferenceEngine` class.\n\n    Example:\n        ```python\n        from genlm.control import InferenceVisualizer\n        # create the visualizer\n        viz = InferenceVisualizer()\n        # run inference and save the record to a JSON file\n        sequences = await token_sampler.smc(\n            n_particles=10,\n            max_tokens=20,\n            ess_threshold=0.5,\n            json_path=\"smc_record.json\" # save the record to a JSON file\n        )\n        # visualize the inference run\n        viz.visualize(\"smc_record.json\")\n        # clean up visualization server\n        viz.shutdown_server()\n        ```\n    \"\"\"\n\n    def __init__(self, port=8000, serve_dir=None):\n        \"\"\"Initialize the visualization server.\n\n        Args:\n            port (int): Port to run the server on.\n            serve_dir (str | Path, optional): Directory to serve files from.\n                If None, creates a temporary directory.\n\n        Raises:\n            OSError: If the port is already in use\n        \"\"\"\n        self._server = None\n        self._server_thread = None\n        self._port = port\n        self._html_dir = Path(__file__).parent / \"html\"\n\n        # Set up serve directory\n        if serve_dir is None:\n            self._serve_dir = Path(tempfile.mkdtemp(prefix=\"smc_viz_\"))\n            self._using_temp_dir = True\n        else:\n            self._serve_dir = Path(serve_dir).resolve()\n            self._using_temp_dir = False\n            self._serve_dir.mkdir(exist_ok=True)\n\n        # Create handler that serves from both directories\n        class Handler(http.server.SimpleHTTPRequestHandler):\n            def translate_path(self_, path):\n                # Remove query parameters for file lookup\n                clean_path = path.split(\"?\")[0]\n                # HTML files come from package\n                if clean_path.endswith(\".html\"):\n                    return str(self._html_dir / clean_path.lstrip(\"/\"))\n                # JSON files come from serve directory\n                return str(self._serve_dir / clean_path.lstrip(\"/\"))\n\n        self._start_server(Handler)\n\n    def visualize(self, json_path, auto_open=False):\n        \"\"\"Visualize the inference run in a browser.\n\n        Args:\n            json_path (str | Path): Path to the JSON file to visualize. If the file is not\n                in the serve directory, it will be copied there. For efficiency, you can\n                write JSON files directly to the serve directory\n            auto_open (bool): Whether to automatically open in browser\n\n        Returns:\n            (str): URL where visualization can be accessed\n        \"\"\"\n        if self._server is None:\n            raise RuntimeError(\"Server is not running\")\n\n        json_path = Path(json_path)\n        if not json_path.exists():\n            raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n\n        # If file isn't in serve directory, copy it there\n        dest_path = self._serve_dir / json_path.name\n        if json_path.resolve() != dest_path.resolve():\n            shutil.copy2(json_path, dest_path)\n\n        url = f\"http://localhost:{self._port}/smc.html?path={json_path.name}\"\n\n        if auto_open:\n            webbrowser.open(url)\n\n        return url\n\n    def _start_server(self, handler_class):\n        \"\"\"Start the HTTP server.\"\"\"\n        try:\n            self._server = socketserver.TCPServer(\n                (\"\", self._port), handler_class, bind_and_activate=False\n            )\n            self._server.allow_reuse_address = True\n            self._server.server_bind()\n            self._server.server_activate()\n        except OSError as e:\n            if e.errno == 48 or e.errno == 98:  # Address already in use\n                raise OSError(f\"Port {self._port} is already in use\") from None\n            raise\n\n        self._server_thread = threading.Thread(target=self._server.serve_forever)\n        self._server_thread.daemon = True\n        self._server_thread.start()\n\n    def shutdown_server(self):\n        \"\"\"Shut down the visualization server.\"\"\"\n        if self._server is not None:\n            if self._server_thread is not None and self._server_thread.is_alive():\n                self._server.shutdown()\n                self._server_thread.join()\n            self._server.server_close()\n            self._server = None\n            self._server_thread = None\n\n        # Clean up any temporary files\n        if self._using_temp_dir and self._serve_dir.exists():\n            shutil.rmtree(self._serve_dir)\n\n    def __del__(self):\n        \"\"\"Ensure server is shut down when object is deleted.\"\"\"\n        self.shutdown_server()\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.InferenceVisualizer.__init__","title":"<code>__init__(port=8000, serve_dir=None)</code>","text":"<p>Initialize the visualization server.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>Port to run the server on.</p> <code>8000</code> <code>serve_dir</code> <code>str | Path</code> <p>Directory to serve files from. If None, creates a temporary directory.</p> <code>None</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the port is already in use</p> Source code in <code>genlm/control/viz.py</code> <pre><code>def __init__(self, port=8000, serve_dir=None):\n    \"\"\"Initialize the visualization server.\n\n    Args:\n        port (int): Port to run the server on.\n        serve_dir (str | Path, optional): Directory to serve files from.\n            If None, creates a temporary directory.\n\n    Raises:\n        OSError: If the port is already in use\n    \"\"\"\n    self._server = None\n    self._server_thread = None\n    self._port = port\n    self._html_dir = Path(__file__).parent / \"html\"\n\n    # Set up serve directory\n    if serve_dir is None:\n        self._serve_dir = Path(tempfile.mkdtemp(prefix=\"smc_viz_\"))\n        self._using_temp_dir = True\n    else:\n        self._serve_dir = Path(serve_dir).resolve()\n        self._using_temp_dir = False\n        self._serve_dir.mkdir(exist_ok=True)\n\n    # Create handler that serves from both directories\n    class Handler(http.server.SimpleHTTPRequestHandler):\n        def translate_path(self_, path):\n            # Remove query parameters for file lookup\n            clean_path = path.split(\"?\")[0]\n            # HTML files come from package\n            if clean_path.endswith(\".html\"):\n                return str(self._html_dir / clean_path.lstrip(\"/\"))\n            # JSON files come from serve directory\n            return str(self._serve_dir / clean_path.lstrip(\"/\"))\n\n    self._start_server(Handler)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.InferenceVisualizer.visualize","title":"<code>visualize(json_path, auto_open=False)</code>","text":"<p>Visualize the inference run in a browser.</p> <p>Parameters:</p> Name Type Description Default <code>json_path</code> <code>str | Path</code> <p>Path to the JSON file to visualize. If the file is not in the serve directory, it will be copied there. For efficiency, you can write JSON files directly to the serve directory</p> required <code>auto_open</code> <code>bool</code> <p>Whether to automatically open in browser</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>URL where visualization can be accessed</p> Source code in <code>genlm/control/viz.py</code> <pre><code>def visualize(self, json_path, auto_open=False):\n    \"\"\"Visualize the inference run in a browser.\n\n    Args:\n        json_path (str | Path): Path to the JSON file to visualize. If the file is not\n            in the serve directory, it will be copied there. For efficiency, you can\n            write JSON files directly to the serve directory\n        auto_open (bool): Whether to automatically open in browser\n\n    Returns:\n        (str): URL where visualization can be accessed\n    \"\"\"\n    if self._server is None:\n        raise RuntimeError(\"Server is not running\")\n\n    json_path = Path(json_path)\n    if not json_path.exists():\n        raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n\n    # If file isn't in serve directory, copy it there\n    dest_path = self._serve_dir / json_path.name\n    if json_path.resolve() != dest_path.resolve():\n        shutil.copy2(json_path, dest_path)\n\n    url = f\"http://localhost:{self._port}/smc.html?path={json_path.name}\"\n\n    if auto_open:\n        webbrowser.open(url)\n\n    return url\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.InferenceVisualizer.shutdown_server","title":"<code>shutdown_server()</code>","text":"<p>Shut down the visualization server.</p> Source code in <code>genlm/control/viz.py</code> <pre><code>def shutdown_server(self):\n    \"\"\"Shut down the visualization server.\"\"\"\n    if self._server is not None:\n        if self._server_thread is not None and self._server_thread.is_alive():\n            self._server.shutdown()\n            self._server_thread.join()\n        self._server.server_close()\n        self._server = None\n        self._server_thread = None\n\n    # Clean up any temporary files\n    if self._using_temp_dir and self._serve_dir.exists():\n        shutil.rmtree(self._serve_dir)\n</code></pre>"},{"location":"reference/genlm/control/__init__/#genlm.control.InferenceVisualizer.__del__","title":"<code>__del__()</code>","text":"<p>Ensure server is shut down when object is deleted.</p> Source code in <code>genlm/control/viz.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure server is shut down when object is deleted.\"\"\"\n    self.shutdown_server()\n</code></pre>"},{"location":"reference/genlm/control/constant/","title":"constant","text":""},{"location":"reference/genlm/control/constant/#genlm.control.constant.EndOfSequence","title":"<code>EndOfSequence</code>","text":"<p>End-of-sequence tokens.</p> Source code in <code>genlm/control/constant.py</code> <pre><code>class EndOfSequence:\n    \"\"\"End-of-sequence tokens.\"\"\"\n\n    def __init__(self, type_=\"EOS\"):\n        self.type_ = type_\n\n    def __repr__(self):\n        return self.type_\n\n    def __eq__(self, other):\n        return isinstance(other, EndOfSequence) and self.type_ == other.type_\n\n    def __radd__(self, other):\n        if isinstance(other, (str, bytes)):\n            return [*list(other), self]\n        elif isinstance(other, (list, tuple)):\n            return type(other)(list(other) + [self])\n        else:\n            raise TypeError(f\"Cannot concatenate {type(other)} with {type(self)}\")\n\n    def __hash__(self):\n        return hash(self.type_)\n\n    def __iter__(self):\n        return iter([self])\n\n    def __len__(self):\n        return 1\n</code></pre>"},{"location":"reference/genlm/control/typing/","title":"typing","text":""},{"location":"reference/genlm/control/typing/#genlm.control.typing.TokenType","title":"<code>TokenType</code>  <code>dataclass</code>","text":"<p>Base class representing the type of a token</p> Source code in <code>genlm/control/typing.py</code> <pre><code>@dataclass\nclass TokenType:\n    \"\"\"Base class representing the type of a token\"\"\"\n\n    def check(self, value):\n        \"\"\"Check if a value matches this type\"\"\"\n        raise NotImplementedError()  # pragma: no cover\n\n    def is_iterable_of(self, element_type):\n        \"\"\"Check if this type can be interpreted as an iterable of element_type.\n\n        Args:\n            element_type (TokenType): The type to check if this is an iterable of\n\n        Examples:\n            &gt;&gt;&gt; Sequence(Atomic(int)).is_iterable_of(Atomic(int))\n            True\n            &gt;&gt;&gt; Atomic(bytes).is_iterable_of(Atomic(int))\n            True\n        \"\"\"\n        if isinstance(self, Sequence):\n            return self.element_type == element_type\n\n        if isinstance(self, Atomic):\n            # Special cases for built-in iterables\n            if (\n                self.type is bytes\n                and isinstance(element_type, Atomic)\n                and element_type.type is int\n            ):\n                return True\n            if (\n                self.type is str\n                and isinstance(element_type, Atomic)\n                and element_type.type is str\n            ):\n                return True\n\n        return False\n</code></pre>"},{"location":"reference/genlm/control/typing/#genlm.control.typing.TokenType.check","title":"<code>check(value)</code>","text":"<p>Check if a value matches this type</p> Source code in <code>genlm/control/typing.py</code> <pre><code>def check(self, value):\n    \"\"\"Check if a value matches this type\"\"\"\n    raise NotImplementedError()  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/typing/#genlm.control.typing.TokenType.is_iterable_of","title":"<code>is_iterable_of(element_type)</code>","text":"<p>Check if this type can be interpreted as an iterable of element_type.</p> <p>Parameters:</p> Name Type Description Default <code>element_type</code> <code>TokenType</code> <p>The type to check if this is an iterable of</p> required <p>Examples:</p> <pre><code>&gt;&gt;&gt; Sequence(Atomic(int)).is_iterable_of(Atomic(int))\nTrue\n&gt;&gt;&gt; Atomic(bytes).is_iterable_of(Atomic(int))\nTrue\n</code></pre> Source code in <code>genlm/control/typing.py</code> <pre><code>def is_iterable_of(self, element_type):\n    \"\"\"Check if this type can be interpreted as an iterable of element_type.\n\n    Args:\n        element_type (TokenType): The type to check if this is an iterable of\n\n    Examples:\n        &gt;&gt;&gt; Sequence(Atomic(int)).is_iterable_of(Atomic(int))\n        True\n        &gt;&gt;&gt; Atomic(bytes).is_iterable_of(Atomic(int))\n        True\n    \"\"\"\n    if isinstance(self, Sequence):\n        return self.element_type == element_type\n\n    if isinstance(self, Atomic):\n        # Special cases for built-in iterables\n        if (\n            self.type is bytes\n            and isinstance(element_type, Atomic)\n            and element_type.type is int\n        ):\n            return True\n        if (\n            self.type is str\n            and isinstance(element_type, Atomic)\n            and element_type.type is str\n        ):\n            return True\n\n    return False\n</code></pre>"},{"location":"reference/genlm/control/typing/#genlm.control.typing.Atomic","title":"<code>Atomic</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TokenType</code></p> <p>Represents a simple type like int or str</p> Source code in <code>genlm/control/typing.py</code> <pre><code>@dataclass\nclass Atomic(TokenType):\n    \"\"\"Represents a simple type like int or str\"\"\"\n\n    type: type  # The Python type (int, str, etc.)\n\n    def check(self, value):\n        return isinstance(value, self.type) or isinstance(value, EndOfSequence)\n\n    def convert(self, value):\n        return self.type(value)\n\n    def __repr__(self):\n        return f\"Atomic({self.type.__name__})\"\n</code></pre>"},{"location":"reference/genlm/control/typing/#genlm.control.typing.Sequence","title":"<code>Sequence</code>  <code>dataclass</code>","text":"<p>               Bases: <code>TokenType</code></p> <p>Represents a list/sequence of another type</p> Source code in <code>genlm/control/typing.py</code> <pre><code>@dataclass\nclass Sequence(TokenType):\n    \"\"\"Represents a list/sequence of another type\"\"\"\n\n    element_type: TokenType  # The type of elements in the sequence\n\n    def check(self, value):\n        return isinstance(value, (list, tuple)) and all(\n            self.element_type.check(x) for x in value\n        )\n\n    def convert(self, value):\n        return tuple(self.element_type.convert(x) for x in value)\n\n    def __repr__(self):\n        return f\"Sequence({self.element_type!r})\"\n</code></pre>"},{"location":"reference/genlm/control/typing/#genlm.control.typing.infer_type","title":"<code>infer_type(value)</code>","text":"<p>Infer the TokenType from a value.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>Any</code> <p>A sample value to infer type from</p> required <p>Returns:</p> Type Description <code>TokenType</code> <p>The inferred type</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; infer_type(42)\nAtomic(type=int)\n&gt;&gt;&gt; infer_type([1, 2, 3])\nSequence(element_type=Atomic(type=int))\n&gt;&gt;&gt; infer_type([[1, 2], [3, 4]])\nSequence(element_type=Sequence(element_type=Atomic(type=int)))\n</code></pre> Source code in <code>genlm/control/typing.py</code> <pre><code>def infer_type(value):\n    \"\"\"Infer the TokenType from a value.\n\n    Args:\n        value (Any): A sample value to infer type from\n\n    Returns:\n        (TokenType): The inferred type\n\n    Examples:\n        &gt;&gt;&gt; infer_type(42)\n        Atomic(type=int)\n        &gt;&gt;&gt; infer_type([1, 2, 3])\n        Sequence(element_type=Atomic(type=int))\n        &gt;&gt;&gt; infer_type([[1, 2], [3, 4]])\n        Sequence(element_type=Sequence(element_type=Atomic(type=int)))\n    \"\"\"\n    if isinstance(value, SequenceABC) and not isinstance(value, (bytes, str)):\n        if not value:\n            raise ValueError(\"Cannot infer type from empty sequence\")\n        element_type = infer_type(value[0])\n        if not all(element_type.check(x) for x in value):\n            raise ValueError(\"Inconsistent types in sequence\")\n        return Sequence(element_type)\n\n    return Atomic(type(value))\n</code></pre>"},{"location":"reference/genlm/control/typing/#genlm.control.typing.infer_vocabulary_type","title":"<code>infer_vocabulary_type(vocabulary)</code>","text":"<p>Infer the TokenType from a vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>vocabulary</code> <code>List[Any]</code> <p>A list of tokens to infer type from</p> required <p>Returns:</p> Type Description <code>TokenType</code> <p>The inferred type</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If vocabulary is empty or contains inconsistent types</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; infer_vocabulary_type([1, 2, 3])\nAtomic(type=int)\n&gt;&gt;&gt; infer_vocabulary_type([[1, 2], [3, 4]])\nSequence(element_type=Atomic(type=int))\n</code></pre> Source code in <code>genlm/control/typing.py</code> <pre><code>def infer_vocabulary_type(vocabulary):\n    \"\"\"Infer the TokenType from a vocabulary.\n\n    Args:\n        vocabulary (List[Any]): A list of tokens to infer type from\n\n    Returns:\n        (TokenType): The inferred type\n\n    Raises:\n        ValueError: If vocabulary is empty or contains inconsistent types\n\n    Examples:\n        &gt;&gt;&gt; infer_vocabulary_type([1, 2, 3])\n        Atomic(type=int)\n        &gt;&gt;&gt; infer_vocabulary_type([[1, 2], [3, 4]])\n        Sequence(element_type=Atomic(type=int))\n    \"\"\"\n    if not vocabulary:\n        raise ValueError(\"Cannot infer type from empty vocabulary\")\n\n    token_type = infer_type(vocabulary[0])\n    if not all(token_type.check(x) for x in vocabulary):\n        raise ValueError(\"Inconsistent types in vocabulary\")\n\n    return token_type\n</code></pre>"},{"location":"reference/genlm/control/util/","title":"util","text":""},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights","title":"<code>LazyWeights</code>","text":"<p>A class to represent weights in a lazy manner, allowing for efficient operations on potentially large weight arrays without immediate materialization.</p> <p>Attributes:</p> Name Type Description <code>weights</code> <code>ndarray</code> <p>The weights associated with the tokens.</p> <code>encode</code> <code>dict</code> <p>A mapping from tokens to their corresponding indices in the weights array.</p> <code>decode</code> <code>list</code> <p>A list of tokens corresponding to the weights.</p> <code>is_log</code> <code>bool</code> <p>A flag indicating whether the weights are in log space.</p> Source code in <code>genlm/control/util.py</code> <pre><code>class LazyWeights:\n    \"\"\"\n    A class to represent weights in a lazy manner, allowing for efficient operations\n    on potentially large weight arrays without immediate materialization.\n\n    Attributes:\n        weights (np.ndarray): The weights associated with the tokens.\n        encode (dict): A mapping from tokens to their corresponding indices in the weights array.\n        decode (list): A list of tokens corresponding to the weights.\n        is_log (bool): A flag indicating whether the weights are in log space.\n    \"\"\"\n\n    def __init__(self, weights, encode, decode, log=True):\n        \"\"\"\n        Initialize the LazyWeights instance.\n\n        Args:\n            weights (np.ndarray): The weights associated with the tokens.\n            encode (dict): A mapping from tokens to their corresponding indices in the weights array.\n            decode (list): A list of tokens corresponding to the weights.\n            log (bool, optional): Indicates if the weights are in log space. Defaults to True.\n\n        Raises:\n            AssertionError: If the lengths of weights and decode or encode do not match.\n        \"\"\"\n        assert len(weights) == len(decode)\n        assert len(encode) == len(decode)\n\n        self.weights = weights\n        self.encode = encode\n        self.decode = decode\n        self.is_log = log\n\n    def __getitem__(self, token):\n        \"\"\"\n        Retrieve the weight for a given token.\n\n        Args:\n            token (Any): The token for which to retrieve the weight.\n\n        Returns:\n            (float): The weight of the token, or -inf/0 if the token is not found.\n        \"\"\"\n        if token not in self.encode:\n            return float(\"-inf\") if self.is_log else 0\n        return self.weights[self.encode[token]]\n\n    def __len__(self):\n        return len(self.weights)\n\n    def __array__(self):\n        raise NotImplementedError(\n            \"LazyWeights cannot be converted to a numpy array. \"\n            \"If you want to combine multiple LazyWeights, use their weights attribute directly.\"\n        )\n\n    def keys(self):\n        \"\"\"Return the list of tokens (keys) in the vocabulary.\"\"\"\n        return self.decode\n\n    def values(self):\n        \"\"\"Return the weights associated with the tokens.\"\"\"\n        return self.weights\n\n    def items(self):\n        \"\"\"Return a zip of tokens and weights.\"\"\"\n        return zip(self.keys(), self.values())\n\n    def normalize(self):\n        \"\"\"\n        Normalize the weights.\n\n        Normalization is performed using log-space arithmetic when weights are logarithmic,\n        or standard arithmetic otherwise.\n\n        Returns:\n            (LazyWeights): A new LazyWeights instance with normalized weights.\n        \"\"\"\n        if self.is_log:\n            return self.spawn(self.weights - logsumexp(self.weights))\n        else:\n            return self.spawn(self.weights / np.sum(self.weights))\n\n    def exp(self):\n        \"\"\"\n        Exponentiate the weights. This operation can only be performed when weights are in log space.\n\n        Returns:\n            (LazyWeights): A new LazyWeights instance with exponentiated weights.\n\n        Raises:\n            AssertionError: If the weights are not in log space.\n        \"\"\"\n        assert self.is_log, \"Weights must be in log space to exponentiate\"\n        return self.spawn(np.exp(self.weights), log=False)\n\n    def log(self):\n        \"\"\"\n        Take the logarithm of the weights. This operation can only be performed when weights are in regular space.\n\n        Returns:\n            (LazyWeights): A new LazyWeights instance with logarithmic weights.\n\n        Raises:\n            AssertionError: If the weights are already in log space.\n        \"\"\"\n        assert not self.is_log, \"Weights must be in regular space to take the logarithm\"\n        return self.spawn(np.log(self.weights), log=True)\n\n    def sum(self):\n        \"\"\"\n        Sum the weights.\n\n        Summation is performed using log-space arithmetic when weights are logarithmic,\n        or standard arithmetic otherwise.\n\n        Returns:\n            (float): The sum of the weights, either in log space or regular space.\n        \"\"\"\n        if self.is_log:\n            return logsumexp(self.weights)\n        else:\n            return np.sum(self.weights)\n\n    def spawn(self, new_weights, log=None):\n        \"\"\"\n        Create a new LazyWeights instance over the same vocabulary with new weights.\n\n        Args:\n            new_weights (np.ndarray): The new weights for the LazyWeights instance.\n            log (bool, optional): Indicates if the new weights are in log space. Defaults to None.\n\n        Returns:\n            (LazyWeights): A new LazyWeights instance.\n        \"\"\"\n        if log is None:\n            log = self.is_log\n        return LazyWeights(\n            weights=new_weights, encode=self.encode, decode=self.decode, log=log\n        )\n\n    def materialize(self, top=None):\n        \"\"\"\n        Materialize the weights into a chart.\n\n        Args:\n            top (int, optional): The number of top weights to materialize. Defaults to None.\n\n        Returns:\n            (Chart): A chart representation of the weights.\n        \"\"\"\n        weights = self.weights\n        if top is not None:\n            top_ws = weights.argsort()[-int(top) :]\n        else:\n            top_ws = weights.argsort()\n\n        semiring = Log if self.is_log else Float\n\n        chart = semiring.chart()\n        for i in reversed(top_ws):\n            chart[self.decode[i]] = weights[i]\n\n        return chart\n\n    def __repr__(self):\n        return repr(self.materialize())\n\n    def assert_equal(self, other, **kwargs):\n        \"\"\"\n        Assert that two LazyWeights instances are equal.\n\n        This method asserts that the two LazyWeights instances have the same vocabulary\n        (in identical order) and that their weights are numerically close.\n\n        Args:\n            other (LazyWeights): The other LazyWeights instance to compare.\n            **kwargs (dict): Additional arguments for np.testing.assert_allclose (e.g., rtol, atol).\n        \"\"\"\n        assert self.decode == other.decode\n        np.testing.assert_allclose(self.weights, other.weights, **kwargs)\n\n    def assert_equal_unordered(self, other, **kwargs):\n        \"\"\"\n        Assert that two LazyWeights instances are equal, ignoring vocabularyorder.\n\n        Args:\n            other (LazyWeights): The other LazyWeights instance to compare.\n            **kwargs (dict): Additional arguments for np.isclose (e.g., rtol, atol).\n        \"\"\"\n        assert set(self.decode) == set(other.decode), \"keys do not match\"\n\n        for x in self.decode:\n            have, want = self[x], other[x]\n            assert np.isclose(have, want, **kwargs), f\"{x}: {have} != {want}\"\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.__init__","title":"<code>__init__(weights, encode, decode, log=True)</code>","text":"<p>Initialize the LazyWeights instance.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>ndarray</code> <p>The weights associated with the tokens.</p> required <code>encode</code> <code>dict</code> <p>A mapping from tokens to their corresponding indices in the weights array.</p> required <code>decode</code> <code>list</code> <p>A list of tokens corresponding to the weights.</p> required <code>log</code> <code>bool</code> <p>Indicates if the weights are in log space. Defaults to True.</p> <code>True</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the lengths of weights and decode or encode do not match.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def __init__(self, weights, encode, decode, log=True):\n    \"\"\"\n    Initialize the LazyWeights instance.\n\n    Args:\n        weights (np.ndarray): The weights associated with the tokens.\n        encode (dict): A mapping from tokens to their corresponding indices in the weights array.\n        decode (list): A list of tokens corresponding to the weights.\n        log (bool, optional): Indicates if the weights are in log space. Defaults to True.\n\n    Raises:\n        AssertionError: If the lengths of weights and decode or encode do not match.\n    \"\"\"\n    assert len(weights) == len(decode)\n    assert len(encode) == len(decode)\n\n    self.weights = weights\n    self.encode = encode\n    self.decode = decode\n    self.is_log = log\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.__getitem__","title":"<code>__getitem__(token)</code>","text":"<p>Retrieve the weight for a given token.</p> <p>Parameters:</p> Name Type Description Default <code>token</code> <code>Any</code> <p>The token for which to retrieve the weight.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The weight of the token, or -inf/0 if the token is not found.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def __getitem__(self, token):\n    \"\"\"\n    Retrieve the weight for a given token.\n\n    Args:\n        token (Any): The token for which to retrieve the weight.\n\n    Returns:\n        (float): The weight of the token, or -inf/0 if the token is not found.\n    \"\"\"\n    if token not in self.encode:\n        return float(\"-inf\") if self.is_log else 0\n    return self.weights[self.encode[token]]\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.keys","title":"<code>keys()</code>","text":"<p>Return the list of tokens (keys) in the vocabulary.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def keys(self):\n    \"\"\"Return the list of tokens (keys) in the vocabulary.\"\"\"\n    return self.decode\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.values","title":"<code>values()</code>","text":"<p>Return the weights associated with the tokens.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def values(self):\n    \"\"\"Return the weights associated with the tokens.\"\"\"\n    return self.weights\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.items","title":"<code>items()</code>","text":"<p>Return a zip of tokens and weights.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def items(self):\n    \"\"\"Return a zip of tokens and weights.\"\"\"\n    return zip(self.keys(), self.values())\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.normalize","title":"<code>normalize()</code>","text":"<p>Normalize the weights.</p> <p>Normalization is performed using log-space arithmetic when weights are logarithmic, or standard arithmetic otherwise.</p> <p>Returns:</p> Type Description <code>LazyWeights</code> <p>A new LazyWeights instance with normalized weights.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def normalize(self):\n    \"\"\"\n    Normalize the weights.\n\n    Normalization is performed using log-space arithmetic when weights are logarithmic,\n    or standard arithmetic otherwise.\n\n    Returns:\n        (LazyWeights): A new LazyWeights instance with normalized weights.\n    \"\"\"\n    if self.is_log:\n        return self.spawn(self.weights - logsumexp(self.weights))\n    else:\n        return self.spawn(self.weights / np.sum(self.weights))\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.exp","title":"<code>exp()</code>","text":"<p>Exponentiate the weights. This operation can only be performed when weights are in log space.</p> <p>Returns:</p> Type Description <code>LazyWeights</code> <p>A new LazyWeights instance with exponentiated weights.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weights are not in log space.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def exp(self):\n    \"\"\"\n    Exponentiate the weights. This operation can only be performed when weights are in log space.\n\n    Returns:\n        (LazyWeights): A new LazyWeights instance with exponentiated weights.\n\n    Raises:\n        AssertionError: If the weights are not in log space.\n    \"\"\"\n    assert self.is_log, \"Weights must be in log space to exponentiate\"\n    return self.spawn(np.exp(self.weights), log=False)\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.log","title":"<code>log()</code>","text":"<p>Take the logarithm of the weights. This operation can only be performed when weights are in regular space.</p> <p>Returns:</p> Type Description <code>LazyWeights</code> <p>A new LazyWeights instance with logarithmic weights.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the weights are already in log space.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def log(self):\n    \"\"\"\n    Take the logarithm of the weights. This operation can only be performed when weights are in regular space.\n\n    Returns:\n        (LazyWeights): A new LazyWeights instance with logarithmic weights.\n\n    Raises:\n        AssertionError: If the weights are already in log space.\n    \"\"\"\n    assert not self.is_log, \"Weights must be in regular space to take the logarithm\"\n    return self.spawn(np.log(self.weights), log=True)\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.sum","title":"<code>sum()</code>","text":"<p>Sum the weights.</p> <p>Summation is performed using log-space arithmetic when weights are logarithmic, or standard arithmetic otherwise.</p> <p>Returns:</p> Type Description <code>float</code> <p>The sum of the weights, either in log space or regular space.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def sum(self):\n    \"\"\"\n    Sum the weights.\n\n    Summation is performed using log-space arithmetic when weights are logarithmic,\n    or standard arithmetic otherwise.\n\n    Returns:\n        (float): The sum of the weights, either in log space or regular space.\n    \"\"\"\n    if self.is_log:\n        return logsumexp(self.weights)\n    else:\n        return np.sum(self.weights)\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.spawn","title":"<code>spawn(new_weights, log=None)</code>","text":"<p>Create a new LazyWeights instance over the same vocabulary with new weights.</p> <p>Parameters:</p> Name Type Description Default <code>new_weights</code> <code>ndarray</code> <p>The new weights for the LazyWeights instance.</p> required <code>log</code> <code>bool</code> <p>Indicates if the new weights are in log space. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>LazyWeights</code> <p>A new LazyWeights instance.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def spawn(self, new_weights, log=None):\n    \"\"\"\n    Create a new LazyWeights instance over the same vocabulary with new weights.\n\n    Args:\n        new_weights (np.ndarray): The new weights for the LazyWeights instance.\n        log (bool, optional): Indicates if the new weights are in log space. Defaults to None.\n\n    Returns:\n        (LazyWeights): A new LazyWeights instance.\n    \"\"\"\n    if log is None:\n        log = self.is_log\n    return LazyWeights(\n        weights=new_weights, encode=self.encode, decode=self.decode, log=log\n    )\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.materialize","title":"<code>materialize(top=None)</code>","text":"<p>Materialize the weights into a chart.</p> <p>Parameters:</p> Name Type Description Default <code>top</code> <code>int</code> <p>The number of top weights to materialize. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Chart</code> <p>A chart representation of the weights.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def materialize(self, top=None):\n    \"\"\"\n    Materialize the weights into a chart.\n\n    Args:\n        top (int, optional): The number of top weights to materialize. Defaults to None.\n\n    Returns:\n        (Chart): A chart representation of the weights.\n    \"\"\"\n    weights = self.weights\n    if top is not None:\n        top_ws = weights.argsort()[-int(top) :]\n    else:\n        top_ws = weights.argsort()\n\n    semiring = Log if self.is_log else Float\n\n    chart = semiring.chart()\n    for i in reversed(top_ws):\n        chart[self.decode[i]] = weights[i]\n\n    return chart\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.assert_equal","title":"<code>assert_equal(other, **kwargs)</code>","text":"<p>Assert that two LazyWeights instances are equal.</p> <p>This method asserts that the two LazyWeights instances have the same vocabulary (in identical order) and that their weights are numerically close.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>LazyWeights</code> <p>The other LazyWeights instance to compare.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments for np.testing.assert_allclose (e.g., rtol, atol).</p> <code>{}</code> Source code in <code>genlm/control/util.py</code> <pre><code>def assert_equal(self, other, **kwargs):\n    \"\"\"\n    Assert that two LazyWeights instances are equal.\n\n    This method asserts that the two LazyWeights instances have the same vocabulary\n    (in identical order) and that their weights are numerically close.\n\n    Args:\n        other (LazyWeights): The other LazyWeights instance to compare.\n        **kwargs (dict): Additional arguments for np.testing.assert_allclose (e.g., rtol, atol).\n    \"\"\"\n    assert self.decode == other.decode\n    np.testing.assert_allclose(self.weights, other.weights, **kwargs)\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.LazyWeights.assert_equal_unordered","title":"<code>assert_equal_unordered(other, **kwargs)</code>","text":"<p>Assert that two LazyWeights instances are equal, ignoring vocabularyorder.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>LazyWeights</code> <p>The other LazyWeights instance to compare.</p> required <code>**kwargs</code> <code>dict</code> <p>Additional arguments for np.isclose (e.g., rtol, atol).</p> <code>{}</code> Source code in <code>genlm/control/util.py</code> <pre><code>def assert_equal_unordered(self, other, **kwargs):\n    \"\"\"\n    Assert that two LazyWeights instances are equal, ignoring vocabularyorder.\n\n    Args:\n        other (LazyWeights): The other LazyWeights instance to compare.\n        **kwargs (dict): Additional arguments for np.isclose (e.g., rtol, atol).\n    \"\"\"\n    assert set(self.decode) == set(other.decode), \"keys do not match\"\n\n    for x in self.decode:\n        have, want = self[x], other[x]\n        assert np.isclose(have, want, **kwargs), f\"{x}: {have} != {want}\"\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.load_trie","title":"<code>load_trie(V, backend=None, **kwargs)</code>","text":"<p>Load a TokenCharacterTrie.</p> <p>Parameters:</p> Name Type Description Default <code>V</code> <code>list</code> <p>The vocabulary.</p> required <code>backend</code> <code>str</code> <p>The backend to use for trie construction. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments for the trie construction.</p> <code>{}</code> <p>Returns:</p> Type Description <code>TokenCharacterTrie</code> <p>A trie instance.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def load_trie(V, backend=None, **kwargs):\n    \"\"\"\n    Load a TokenCharacterTrie.\n\n    Args:\n        V (list): The vocabulary.\n        backend (str, optional): The backend to use for trie construction. Defaults to None.\n        **kwargs (dict): Additional arguments for the trie construction.\n\n    Returns:\n        (TokenCharacterTrie): A trie instance.\n    \"\"\"\n    import torch\n\n    if backend is None:\n        backend = \"parallel\" if torch.cuda.is_available() else \"sequential\"\n\n    if backend == \"parallel\":\n        from genlm.backend.trie import ParallelTokenCharacterTrie\n\n        return ParallelTokenCharacterTrie(V, **kwargs)\n    else:\n        from genlm.backend.trie import TokenCharacterTrie\n\n        return TokenCharacterTrie(V, **kwargs)\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.load_async_trie","title":"<code>load_async_trie(V, backend=None, **kwargs)</code>","text":"<p>Load an AsyncTokenCharacterTrie. This is a TokenCharacterTrie that automatically batches weight_sum and weight_max requests.</p> <p>Parameters:</p> Name Type Description Default <code>V</code> <code>list</code> <p>The vocabulary.</p> required <code>backend</code> <code>str</code> <p>The backend to use for trie construction. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments for the trie construction.</p> <code>{}</code> <p>Returns:</p> Type Description <code>AsyncTokenCharacterTrie</code> <p>An async trie instance.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def load_async_trie(V, backend=None, **kwargs):\n    \"\"\"\n    Load an AsyncTokenCharacterTrie. This is a TokenCharacterTrie that\n    automatically batches weight_sum and weight_max requests.\n\n    Args:\n        V (list): The vocabulary.\n        backend (str, optional): The backend to use for trie construction. Defaults to None.\n        **kwargs (dict): Additional arguments for the trie construction.\n\n    Returns:\n        (AsyncTokenCharacterTrie): An async trie instance.\n    \"\"\"\n    from genlm.backend.trie import AsyncTokenCharacterTrie\n\n    return AsyncTokenCharacterTrie(load_trie(V, backend, **kwargs))\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.fast_sample_logprobs","title":"<code>fast_sample_logprobs(logprobs, size=1)</code>","text":"<p>Sample indices from an array of log probabilities using the Gumbel-max trick.</p> <p>Parameters:</p> Name Type Description Default <code>logprobs</code> <code>ndarray</code> <p>Array of log probabilities</p> required <code>size</code> <code>int</code> <p>Number of samples to draw</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>Array of sampled indices</p> Note <p>This is much faster than np.random.choice for large arrays since it avoids normalizing probabilities and uses vectorized operations.</p> Source code in <code>genlm/control/util.py</code> <pre><code>def fast_sample_logprobs(logprobs: np.ndarray, size: int = 1) -&gt; np.ndarray:\n    \"\"\"Sample indices from an array of log probabilities using the Gumbel-max trick.\n\n    Args:\n        logprobs: Array of log probabilities\n        size (int): Number of samples to draw\n\n    Returns:\n        (np.ndarray): Array of sampled indices\n\n    Note:\n        This is much faster than np.random.choice for large arrays since it avoids\n        normalizing probabilities and uses vectorized operations.\n    \"\"\"\n    noise = -np.log(-np.log(np.random.random((size, len(logprobs)))))\n    return (logprobs + noise).argmax(axis=1)\n</code></pre>"},{"location":"reference/genlm/control/util/#genlm.control.util.fast_sample_lazyweights","title":"<code>fast_sample_lazyweights(lazyweights)</code>","text":"<p>Sample a token from a LazyWeights instance using the Gumbel-max trick.</p> <p>Parameters:</p> Name Type Description Default <code>lazyweights</code> <code>LazyWeights</code> <p>A LazyWeights instance</p> required <p>Returns:</p> Type Description <code>Any</code> <p>Sampled token</p> Source code in <code>genlm/control/util.py</code> <pre><code>def fast_sample_lazyweights(lazyweights):\n    \"\"\"Sample a token from a LazyWeights instance using the Gumbel-max trick.\n\n    Args:\n        lazyweights (LazyWeights): A LazyWeights instance\n\n    Returns:\n        (Any): Sampled token\n    \"\"\"\n    assert lazyweights.is_log\n    token_id = fast_sample_logprobs(lazyweights.weights, size=1)[0]\n    return lazyweights.decode[token_id]\n</code></pre>"},{"location":"reference/genlm/control/viz/","title":"viz","text":""},{"location":"reference/genlm/control/viz/#genlm.control.viz.InferenceVisualizer","title":"<code>InferenceVisualizer</code>","text":"<p>Web-based visualization server for SMC inference results.</p> <p>This class is intended to be used in conjunction with the <code>InferenceEngine</code> class.</p> Example <pre><code>from genlm.control import InferenceVisualizer\n# create the visualizer\nviz = InferenceVisualizer()\n# run inference and save the record to a JSON file\nsequences = await token_sampler.smc(\n    n_particles=10,\n    max_tokens=20,\n    ess_threshold=0.5,\n    json_path=\"smc_record.json\" # save the record to a JSON file\n)\n# visualize the inference run\nviz.visualize(\"smc_record.json\")\n# clean up visualization server\nviz.shutdown_server()\n</code></pre> Source code in <code>genlm/control/viz.py</code> <pre><code>class InferenceVisualizer:\n    \"\"\"Web-based visualization server for SMC inference results.\n\n    This class is intended to be used in conjunction with the `InferenceEngine` class.\n\n    Example:\n        ```python\n        from genlm.control import InferenceVisualizer\n        # create the visualizer\n        viz = InferenceVisualizer()\n        # run inference and save the record to a JSON file\n        sequences = await token_sampler.smc(\n            n_particles=10,\n            max_tokens=20,\n            ess_threshold=0.5,\n            json_path=\"smc_record.json\" # save the record to a JSON file\n        )\n        # visualize the inference run\n        viz.visualize(\"smc_record.json\")\n        # clean up visualization server\n        viz.shutdown_server()\n        ```\n    \"\"\"\n\n    def __init__(self, port=8000, serve_dir=None):\n        \"\"\"Initialize the visualization server.\n\n        Args:\n            port (int): Port to run the server on.\n            serve_dir (str | Path, optional): Directory to serve files from.\n                If None, creates a temporary directory.\n\n        Raises:\n            OSError: If the port is already in use\n        \"\"\"\n        self._server = None\n        self._server_thread = None\n        self._port = port\n        self._html_dir = Path(__file__).parent / \"html\"\n\n        # Set up serve directory\n        if serve_dir is None:\n            self._serve_dir = Path(tempfile.mkdtemp(prefix=\"smc_viz_\"))\n            self._using_temp_dir = True\n        else:\n            self._serve_dir = Path(serve_dir).resolve()\n            self._using_temp_dir = False\n            self._serve_dir.mkdir(exist_ok=True)\n\n        # Create handler that serves from both directories\n        class Handler(http.server.SimpleHTTPRequestHandler):\n            def translate_path(self_, path):\n                # Remove query parameters for file lookup\n                clean_path = path.split(\"?\")[0]\n                # HTML files come from package\n                if clean_path.endswith(\".html\"):\n                    return str(self._html_dir / clean_path.lstrip(\"/\"))\n                # JSON files come from serve directory\n                return str(self._serve_dir / clean_path.lstrip(\"/\"))\n\n        self._start_server(Handler)\n\n    def visualize(self, json_path, auto_open=False):\n        \"\"\"Visualize the inference run in a browser.\n\n        Args:\n            json_path (str | Path): Path to the JSON file to visualize. If the file is not\n                in the serve directory, it will be copied there. For efficiency, you can\n                write JSON files directly to the serve directory\n            auto_open (bool): Whether to automatically open in browser\n\n        Returns:\n            (str): URL where visualization can be accessed\n        \"\"\"\n        if self._server is None:\n            raise RuntimeError(\"Server is not running\")\n\n        json_path = Path(json_path)\n        if not json_path.exists():\n            raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n\n        # If file isn't in serve directory, copy it there\n        dest_path = self._serve_dir / json_path.name\n        if json_path.resolve() != dest_path.resolve():\n            shutil.copy2(json_path, dest_path)\n\n        url = f\"http://localhost:{self._port}/smc.html?path={json_path.name}\"\n\n        if auto_open:\n            webbrowser.open(url)\n\n        return url\n\n    def _start_server(self, handler_class):\n        \"\"\"Start the HTTP server.\"\"\"\n        try:\n            self._server = socketserver.TCPServer(\n                (\"\", self._port), handler_class, bind_and_activate=False\n            )\n            self._server.allow_reuse_address = True\n            self._server.server_bind()\n            self._server.server_activate()\n        except OSError as e:\n            if e.errno == 48 or e.errno == 98:  # Address already in use\n                raise OSError(f\"Port {self._port} is already in use\") from None\n            raise\n\n        self._server_thread = threading.Thread(target=self._server.serve_forever)\n        self._server_thread.daemon = True\n        self._server_thread.start()\n\n    def shutdown_server(self):\n        \"\"\"Shut down the visualization server.\"\"\"\n        if self._server is not None:\n            if self._server_thread is not None and self._server_thread.is_alive():\n                self._server.shutdown()\n                self._server_thread.join()\n            self._server.server_close()\n            self._server = None\n            self._server_thread = None\n\n        # Clean up any temporary files\n        if self._using_temp_dir and self._serve_dir.exists():\n            shutil.rmtree(self._serve_dir)\n\n    def __del__(self):\n        \"\"\"Ensure server is shut down when object is deleted.\"\"\"\n        self.shutdown_server()\n</code></pre>"},{"location":"reference/genlm/control/viz/#genlm.control.viz.InferenceVisualizer.__init__","title":"<code>__init__(port=8000, serve_dir=None)</code>","text":"<p>Initialize the visualization server.</p> <p>Parameters:</p> Name Type Description Default <code>port</code> <code>int</code> <p>Port to run the server on.</p> <code>8000</code> <code>serve_dir</code> <code>str | Path</code> <p>Directory to serve files from. If None, creates a temporary directory.</p> <code>None</code> <p>Raises:</p> Type Description <code>OSError</code> <p>If the port is already in use</p> Source code in <code>genlm/control/viz.py</code> <pre><code>def __init__(self, port=8000, serve_dir=None):\n    \"\"\"Initialize the visualization server.\n\n    Args:\n        port (int): Port to run the server on.\n        serve_dir (str | Path, optional): Directory to serve files from.\n            If None, creates a temporary directory.\n\n    Raises:\n        OSError: If the port is already in use\n    \"\"\"\n    self._server = None\n    self._server_thread = None\n    self._port = port\n    self._html_dir = Path(__file__).parent / \"html\"\n\n    # Set up serve directory\n    if serve_dir is None:\n        self._serve_dir = Path(tempfile.mkdtemp(prefix=\"smc_viz_\"))\n        self._using_temp_dir = True\n    else:\n        self._serve_dir = Path(serve_dir).resolve()\n        self._using_temp_dir = False\n        self._serve_dir.mkdir(exist_ok=True)\n\n    # Create handler that serves from both directories\n    class Handler(http.server.SimpleHTTPRequestHandler):\n        def translate_path(self_, path):\n            # Remove query parameters for file lookup\n            clean_path = path.split(\"?\")[0]\n            # HTML files come from package\n            if clean_path.endswith(\".html\"):\n                return str(self._html_dir / clean_path.lstrip(\"/\"))\n            # JSON files come from serve directory\n            return str(self._serve_dir / clean_path.lstrip(\"/\"))\n\n    self._start_server(Handler)\n</code></pre>"},{"location":"reference/genlm/control/viz/#genlm.control.viz.InferenceVisualizer.visualize","title":"<code>visualize(json_path, auto_open=False)</code>","text":"<p>Visualize the inference run in a browser.</p> <p>Parameters:</p> Name Type Description Default <code>json_path</code> <code>str | Path</code> <p>Path to the JSON file to visualize. If the file is not in the serve directory, it will be copied there. For efficiency, you can write JSON files directly to the serve directory</p> required <code>auto_open</code> <code>bool</code> <p>Whether to automatically open in browser</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>URL where visualization can be accessed</p> Source code in <code>genlm/control/viz.py</code> <pre><code>def visualize(self, json_path, auto_open=False):\n    \"\"\"Visualize the inference run in a browser.\n\n    Args:\n        json_path (str | Path): Path to the JSON file to visualize. If the file is not\n            in the serve directory, it will be copied there. For efficiency, you can\n            write JSON files directly to the serve directory\n        auto_open (bool): Whether to automatically open in browser\n\n    Returns:\n        (str): URL where visualization can be accessed\n    \"\"\"\n    if self._server is None:\n        raise RuntimeError(\"Server is not running\")\n\n    json_path = Path(json_path)\n    if not json_path.exists():\n        raise FileNotFoundError(f\"JSON file not found: {json_path}\")\n\n    # If file isn't in serve directory, copy it there\n    dest_path = self._serve_dir / json_path.name\n    if json_path.resolve() != dest_path.resolve():\n        shutil.copy2(json_path, dest_path)\n\n    url = f\"http://localhost:{self._port}/smc.html?path={json_path.name}\"\n\n    if auto_open:\n        webbrowser.open(url)\n\n    return url\n</code></pre>"},{"location":"reference/genlm/control/viz/#genlm.control.viz.InferenceVisualizer.shutdown_server","title":"<code>shutdown_server()</code>","text":"<p>Shut down the visualization server.</p> Source code in <code>genlm/control/viz.py</code> <pre><code>def shutdown_server(self):\n    \"\"\"Shut down the visualization server.\"\"\"\n    if self._server is not None:\n        if self._server_thread is not None and self._server_thread.is_alive():\n            self._server.shutdown()\n            self._server_thread.join()\n        self._server.server_close()\n        self._server = None\n        self._server_thread = None\n\n    # Clean up any temporary files\n    if self._using_temp_dir and self._serve_dir.exists():\n        shutil.rmtree(self._serve_dir)\n</code></pre>"},{"location":"reference/genlm/control/viz/#genlm.control.viz.InferenceVisualizer.__del__","title":"<code>__del__()</code>","text":"<p>Ensure server is shut down when object is deleted.</p> Source code in <code>genlm/control/viz.py</code> <pre><code>def __del__(self):\n    \"\"\"Ensure server is shut down when object is deleted.\"\"\"\n    self.shutdown_server()\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/","title":"potential","text":""},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential","title":"<code>Potential</code>","text":"<p>               Bases: <code>ABC</code>, <code>PotentialOps</code>, <code>PotentialTests</code></p> <p>Abstract base class for potentials.</p> <p>A Potential is a function that maps sequences of tokens in a vocabulary to non-negative real numbers (weights).</p> <p>Potentials assign weights to sequences of tokens based on whether they are complete sequences or prefixes of complete sequences.</p> <ul> <li><code>complete</code>: Assess the log weight of a sequence of tokens in the vocabulary as a complete sequence.</li> <li><code>prefix</code>: Assess the log weight of a sequence of tokens in the vocabulary as a prefix.</li> </ul> <p>Potentials additionally implement a <code>logw_next</code> method:</p> <ul> <li><code>logw_next</code>: Compute the next-token log weights of each token in the vocabulary and a special EOS (end-of-sequence) token given a context.</li> </ul> <p>Subclasses must minimally implement <code>complete</code> and <code>prefix</code>. <code>logw_next</code> and batched versions of the above methods come with default implementations, but may be overridden by subclasses for improved performance.</p> <p>All Potentials must satisfy a set of properties which can be tested using PotentialTests.</p> <p>Attributes:</p> Name Type Description <code>token_type</code> <code>TokenType</code> <p>The type of tokens in the vocabulary.</p> <code>vocab</code> <code>list</code> <p>List of tokens making up the vocabulary.</p> <code>eos</code> <code>EndOfSequence</code> <p>Special token to use as end-of-sequence.</p> <code>vocab_eos</code> <code>list</code> <p>List of tokens in <code>vocab</code> and <code>eos</code>. <code>eos</code> is assumed to be the last token in <code>vocab_eos</code>.</p> <code>lookup</code> <code>dict</code> <p>Mapping from tokens and <code>eos</code> to their indices in <code>vocab_eos</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>class Potential(ABC, PotentialOps, PotentialTests):\n    \"\"\"Abstract base class for potentials.\n\n    A Potential is a function that maps sequences of tokens in a vocabulary to non-negative real numbers (weights).\n\n    Potentials assign weights to sequences of tokens based on whether they are complete sequences or prefixes of complete sequences.\n\n    - `complete`: Assess the log weight of a sequence of tokens in the vocabulary as a complete sequence.\n    - `prefix`: Assess the log weight of a sequence of tokens in the vocabulary as a prefix.\n\n    Potentials additionally implement a `logw_next` method:\n\n    - `logw_next`: Compute the next-token log weights of each token in the vocabulary and a special EOS (end-of-sequence) token given a context.\n\n    Subclasses must minimally implement `complete` and `prefix`. `logw_next` and batched versions of the above methods\n    come with default implementations, but may be overridden by subclasses for improved performance.\n\n    All Potentials must satisfy a set of properties which can be tested using [PotentialTests][genlm.control.potential.testing.PotentialTests].\n\n    Attributes:\n        token_type (TokenType): The type of tokens in the vocabulary.\n        vocab (list): List of tokens making up the vocabulary.\n        eos (EndOfSequence): Special token to use as end-of-sequence.\n        vocab_eos (list): List of tokens in `vocab` and `eos`. `eos` is assumed to be the last token in `vocab_eos`.\n        lookup (dict): Mapping from tokens and `eos` to their indices in `vocab_eos`.\n    \"\"\"\n\n    def __init__(self, vocabulary, token_type=None, eos=None):\n        \"\"\"\n        Initialize the potential.\n\n        Args:\n            vocabulary (list): List of tokens that make up the vocabulary.\n            token_type (TokenType, optional): Optional TokenType of all elements of the vocabulary.\n                If None, will be inferred from vocabulary.\n            eos (EndOfSequence, optional): Special token to use as end-of-sequence. Defaults to `EOS`.\n                In general, this should not be set by users.\n\n        Raises:\n            ValueError: If vocabulary is empty.\n            TypeError: If vocabulary contains tokens which are not of `token_type`.\n        \"\"\"\n        if not vocabulary:\n            raise ValueError(\"vocabulary cannot be empty\")\n\n        if token_type is None:\n            token_type = infer_vocabulary_type(vocabulary)\n        elif not isinstance(token_type, TokenType):\n            raise ValueError(f\"token_type must be a TokenType, got {token_type!r}.\")\n\n        if not all(token_type.check(x) for x in vocabulary):\n            raise TypeError(f\"Tokens in vocabulary must be of type {token_type}.\")\n\n        if eos and not isinstance(eos, EndOfSequence):\n            raise ValueError(f\"EOS must be an instance of EndOfSequence, got {eos!r}.\")\n\n        self.eos = eos or EOS\n\n        self.token_type = token_type\n        self.vocab = vocabulary\n        self.vocab_eos = self.vocab + [self.eos]\n        self.lookup = {}\n        for i, x in enumerate(vocabulary):\n            if x in self.lookup:\n                raise ValueError(f\"Duplicate token {x!r} found in vocabulary\")\n            self.lookup[x] = i\n        self.lookup[self.eos] = len(self.vocab)\n\n    ####################\n    # Instance methods #\n    ####################\n\n    @abstractmethod\n    async def complete(self, context):\n        \"\"\"Assess the weight of `context` as a complete sequence.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context under the language.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @abstractmethod\n    async def prefix(self, context):\n        \"\"\"Assess the weight of `context` as a prefix.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context as a prefix.\n        \"\"\"\n        pass  # pragma: no cover\n\n    async def score(self, context):\n        \"\"\"Assess the weight of `context` based on EOS-termination.\n\n        This is a convenience method which dispatches to `complete` if `context` ends with `self.eos`, otherwise to `prefix`.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context, either as a prefix or complete sequence.\n        \"\"\"\n        if context and context[-1] == self.eos:\n            return await self.complete(context[:-1])\n        else:\n            return await self.prefix(context)\n\n    async def logw_next(self, context):\n        \"\"\"Compute the next-token weights of each token in `self.vocab_eos` given `context`.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (LazyWeights): Weights of each token in the vocabulary and EOS.\n        \"\"\"\n        ctx_log_w = await self.prefix(context)\n\n        if ctx_log_w == float(\"-inf\"):\n            raise ValueError(f\"Context {context!r} has weight zero under `prefix`.\")\n\n        scores = await self.batch_score([[*context, x] for x in self.vocab_eos])\n        logws = scores - ctx_log_w\n\n        return self.make_lazy_weights(logws)\n\n    ###################\n    # Batched methods #\n    ###################\n\n    async def batch_complete(self, contexts):\n        \"\"\"Batched equivalent to `complete`.\n\n        Assess the weight of each context as a complete sequence.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return np.array(\n            await asyncio.gather(*[self.complete(context) for context in contexts])\n        )\n\n    async def batch_prefix(self, contexts):\n        \"\"\"Batched equivalent to `prefix`.\n\n        Assess the weight of each context as a prefix.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return np.array(\n            await asyncio.gather(*[self.prefix(context) for context in contexts])\n        )\n\n    async def batch_score(self, contexts):\n        \"\"\"Batched equivalent to `score`.\n\n        Assess the weight of each context based on EOS-termination.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        complete, prefix = [], []\n        complete_indices, prefix_indices = [], []\n\n        for i, context in enumerate(contexts):\n            # We want == here instead of `is`.\n            if context and context[-1] == self.eos:\n                complete.append(context[:-1])\n                complete_indices.append(i)\n            else:\n                prefix.append(context)\n                prefix_indices.append(i)\n\n        complete_scores = (\n            await self.batch_complete(complete) if complete else np.array([])\n        )\n        prefix_scores = await self.batch_prefix(prefix) if prefix else np.array([])\n\n        results = np.empty(len(contexts))\n        if len(complete_scores) &gt; 0:\n            results[complete_indices] = complete_scores\n        if len(prefix_scores) &gt; 0:\n            results[prefix_indices] = prefix_scores\n\n        return results\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"Batched equivalent to `logw_next`.\n\n        Computes the next-token weights of each token in `self.vocab_eos` given each context in the batch.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (list): List of LazyWeights objects, one for each context.\n\n        Raises:\n            ValueError: If any context has zero weight (log weight of -inf) under `prefix`.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return await asyncio.gather(*[self.logw_next(context) for context in contexts])\n\n    #############\n    # Utilities #\n    #############\n\n    def make_lazy_weights(self, weights, log=True):\n        \"\"\"Helper method to create a LazyWeights object over the potential's vocabulary and EOS.\n\n        Args:\n            weights (np.array): Array of weights.\n            log (bool, optional): Whether the weights are in log space. Defaults to True.\n\n        Returns:\n            (LazyWeights): LazyWeights object defined over `self.vocab_eos`.\n        \"\"\"\n        return LazyWeights(\n            weights=weights, encode=self.lookup, decode=self.vocab_eos, log=log\n        )\n\n    def alloc_logws(self, default=float(\"-inf\")):\n        \"\"\"Allocate a new array of log weights for the potential's vocabulary and EOS.\n\n        Args:\n            default (float, optional): Default log weight. Defaults to -inf.\n\n        Returns:\n            (np.array): Array of length `len(self.vocab_eos)` filled with `default`.\n        \"\"\"\n        return np.full((len(self.vocab_eos),), default)\n\n    def spawn(self):\n        \"\"\"\n        Spawn a fresh instance of the potential.\n\n        This method is not required by default, but may be implemented by subclasses\n        to support CPU-parallelism using (`MultiProcPotential`)[genlm.control.potential.multi_proc.MultiProcPotential].\n        \"\"\"\n        raise NotImplementedError(\n            \"Potential.spawn() must be implemented by subclasses.\"\n        )\n\n    async def cleanup(self):\n        \"\"\"\n        Cleanup the potential.\n\n        This method may be implemented by subclasses to release resources.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.__init__","title":"<code>__init__(vocabulary, token_type=None, eos=None)</code>","text":"<p>Initialize the potential.</p> <p>Parameters:</p> Name Type Description Default <code>vocabulary</code> <code>list</code> <p>List of tokens that make up the vocabulary.</p> required <code>token_type</code> <code>TokenType</code> <p>Optional TokenType of all elements of the vocabulary. If None, will be inferred from vocabulary.</p> <code>None</code> <code>eos</code> <code>EndOfSequence</code> <p>Special token to use as end-of-sequence. Defaults to <code>EOS</code>. In general, this should not be set by users.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If vocabulary is empty.</p> <code>TypeError</code> <p>If vocabulary contains tokens which are not of <code>token_type</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def __init__(self, vocabulary, token_type=None, eos=None):\n    \"\"\"\n    Initialize the potential.\n\n    Args:\n        vocabulary (list): List of tokens that make up the vocabulary.\n        token_type (TokenType, optional): Optional TokenType of all elements of the vocabulary.\n            If None, will be inferred from vocabulary.\n        eos (EndOfSequence, optional): Special token to use as end-of-sequence. Defaults to `EOS`.\n            In general, this should not be set by users.\n\n    Raises:\n        ValueError: If vocabulary is empty.\n        TypeError: If vocabulary contains tokens which are not of `token_type`.\n    \"\"\"\n    if not vocabulary:\n        raise ValueError(\"vocabulary cannot be empty\")\n\n    if token_type is None:\n        token_type = infer_vocabulary_type(vocabulary)\n    elif not isinstance(token_type, TokenType):\n        raise ValueError(f\"token_type must be a TokenType, got {token_type!r}.\")\n\n    if not all(token_type.check(x) for x in vocabulary):\n        raise TypeError(f\"Tokens in vocabulary must be of type {token_type}.\")\n\n    if eos and not isinstance(eos, EndOfSequence):\n        raise ValueError(f\"EOS must be an instance of EndOfSequence, got {eos!r}.\")\n\n    self.eos = eos or EOS\n\n    self.token_type = token_type\n    self.vocab = vocabulary\n    self.vocab_eos = self.vocab + [self.eos]\n    self.lookup = {}\n    for i, x in enumerate(vocabulary):\n        if x in self.lookup:\n            raise ValueError(f\"Duplicate token {x!r} found in vocabulary\")\n        self.lookup[x] = i\n    self.lookup[self.eos] = len(self.vocab)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.complete","title":"<code>complete(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Assess the weight of <code>context</code> as a complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context under the language.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>@abstractmethod\nasync def complete(self, context):\n    \"\"\"Assess the weight of `context` as a complete sequence.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context under the language.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.prefix","title":"<code>prefix(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Assess the weight of <code>context</code> as a prefix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context as a prefix.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>@abstractmethod\nasync def prefix(self, context):\n    \"\"\"Assess the weight of `context` as a prefix.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context as a prefix.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.score","title":"<code>score(context)</code>  <code>async</code>","text":"<p>Assess the weight of <code>context</code> based on EOS-termination.</p> <p>This is a convenience method which dispatches to <code>complete</code> if <code>context</code> ends with <code>self.eos</code>, otherwise to <code>prefix</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context, either as a prefix or complete sequence.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def score(self, context):\n    \"\"\"Assess the weight of `context` based on EOS-termination.\n\n    This is a convenience method which dispatches to `complete` if `context` ends with `self.eos`, otherwise to `prefix`.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context, either as a prefix or complete sequence.\n    \"\"\"\n    if context and context[-1] == self.eos:\n        return await self.complete(context[:-1])\n    else:\n        return await self.prefix(context)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next-token weights of each token in <code>self.vocab_eos</code> given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Weights of each token in the vocabulary and EOS.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Compute the next-token weights of each token in `self.vocab_eos` given `context`.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (LazyWeights): Weights of each token in the vocabulary and EOS.\n    \"\"\"\n    ctx_log_w = await self.prefix(context)\n\n    if ctx_log_w == float(\"-inf\"):\n        raise ValueError(f\"Context {context!r} has weight zero under `prefix`.\")\n\n    scores = await self.batch_score([[*context, x] for x in self.vocab_eos])\n    logws = scores - ctx_log_w\n\n    return self.make_lazy_weights(logws)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.batch_complete","title":"<code>batch_complete(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>complete</code>.</p> <p>Assess the weight of each context as a complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_complete(self, contexts):\n    \"\"\"Batched equivalent to `complete`.\n\n    Assess the weight of each context as a complete sequence.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return np.array(\n        await asyncio.gather(*[self.complete(context) for context in contexts])\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.batch_prefix","title":"<code>batch_prefix(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>prefix</code>.</p> <p>Assess the weight of each context as a prefix.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_prefix(self, contexts):\n    \"\"\"Batched equivalent to `prefix`.\n\n    Assess the weight of each context as a prefix.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return np.array(\n        await asyncio.gather(*[self.prefix(context) for context in contexts])\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.batch_score","title":"<code>batch_score(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>score</code>.</p> <p>Assess the weight of each context based on EOS-termination.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_score(self, contexts):\n    \"\"\"Batched equivalent to `score`.\n\n    Assess the weight of each context based on EOS-termination.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    complete, prefix = [], []\n    complete_indices, prefix_indices = [], []\n\n    for i, context in enumerate(contexts):\n        # We want == here instead of `is`.\n        if context and context[-1] == self.eos:\n            complete.append(context[:-1])\n            complete_indices.append(i)\n        else:\n            prefix.append(context)\n            prefix_indices.append(i)\n\n    complete_scores = (\n        await self.batch_complete(complete) if complete else np.array([])\n    )\n    prefix_scores = await self.batch_prefix(prefix) if prefix else np.array([])\n\n    results = np.empty(len(contexts))\n    if len(complete_scores) &gt; 0:\n        results[complete_indices] = complete_scores\n    if len(prefix_scores) &gt; 0:\n        results[prefix_indices] = prefix_scores\n\n    return results\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>logw_next</code>.</p> <p>Computes the next-token weights of each token in <code>self.vocab_eos</code> given each context in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of LazyWeights objects, one for each context.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any context has zero weight (log weight of -inf) under <code>prefix</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"Batched equivalent to `logw_next`.\n\n    Computes the next-token weights of each token in `self.vocab_eos` given each context in the batch.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (list): List of LazyWeights objects, one for each context.\n\n    Raises:\n        ValueError: If any context has zero weight (log weight of -inf) under `prefix`.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return await asyncio.gather(*[self.logw_next(context) for context in contexts])\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.make_lazy_weights","title":"<code>make_lazy_weights(weights, log=True)</code>","text":"<p>Helper method to create a LazyWeights object over the potential's vocabulary and EOS.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>array</code> <p>Array of weights.</p> required <code>log</code> <code>bool</code> <p>Whether the weights are in log space. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LazyWeights</code> <p>LazyWeights object defined over <code>self.vocab_eos</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def make_lazy_weights(self, weights, log=True):\n    \"\"\"Helper method to create a LazyWeights object over the potential's vocabulary and EOS.\n\n    Args:\n        weights (np.array): Array of weights.\n        log (bool, optional): Whether the weights are in log space. Defaults to True.\n\n    Returns:\n        (LazyWeights): LazyWeights object defined over `self.vocab_eos`.\n    \"\"\"\n    return LazyWeights(\n        weights=weights, encode=self.lookup, decode=self.vocab_eos, log=log\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.alloc_logws","title":"<code>alloc_logws(default=float('-inf'))</code>","text":"<p>Allocate a new array of log weights for the potential's vocabulary and EOS.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>float</code> <p>Default log weight. Defaults to -inf.</p> <code>float('-inf')</code> <p>Returns:</p> Type Description <code>array</code> <p>Array of length <code>len(self.vocab_eos)</code> filled with <code>default</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def alloc_logws(self, default=float(\"-inf\")):\n    \"\"\"Allocate a new array of log weights for the potential's vocabulary and EOS.\n\n    Args:\n        default (float, optional): Default log weight. Defaults to -inf.\n\n    Returns:\n        (np.array): Array of length `len(self.vocab_eos)` filled with `default`.\n    \"\"\"\n    return np.full((len(self.vocab_eos),), default)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a fresh instance of the potential.</p> <p>This method is not required by default, but may be implemented by subclasses to support CPU-parallelism using (<code>MultiProcPotential</code>)[genlm.control.potential.multi_proc.MultiProcPotential].</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def spawn(self):\n    \"\"\"\n    Spawn a fresh instance of the potential.\n\n    This method is not required by default, but may be implemented by subclasses\n    to support CPU-parallelism using (`MultiProcPotential`)[genlm.control.potential.multi_proc.MultiProcPotential].\n    \"\"\"\n    raise NotImplementedError(\n        \"Potential.spawn() must be implemented by subclasses.\"\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Potential.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleanup the potential.</p> <p>This method may be implemented by subclasses to release resources.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def cleanup(self):\n    \"\"\"\n    Cleanup the potential.\n\n    This method may be implemented by subclasses to release resources.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.AutoBatchedPotential","title":"<code>AutoBatchedPotential</code>","text":"<p>               Bases: <code>Potential</code></p> <p>AutoBatchedPotential is a wrapper around a Potential that enables automatic batching of concurrent requests.</p> <p>This class manages a background loop that collects concurrent requests to instance methods (<code>complete</code>, <code>prefix</code>, <code>score</code>, <code>logw_next</code>) and batches them together before delegating to the corresponding batch methods of the underlying potential (<code>batch_complete</code>, <code>batch_prefix</code>, <code>batch_score</code>, <code>batch_logw_next</code>).</p> <p>This class inherits all methods from <code>Potential</code>.</p> <p>Attributes:</p> Name Type Description <code>potential</code> <code>Potential</code> <p>The underlying potential instance that is being wrapped.</p> <code>background_loop</code> <code>AsyncBatchLoop</code> <p>An asynchronous loop that manages batch requests.</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>class AutoBatchedPotential(Potential):\n    \"\"\"\n    AutoBatchedPotential is a wrapper around a Potential that enables automatic batching of concurrent requests.\n\n    This class manages a background loop that collects concurrent requests to instance methods\n    (`complete`, `prefix`, `score`, `logw_next`) and batches them together before\n    delegating to the corresponding batch methods of the underlying potential\n    (`batch_complete`, `batch_prefix`, `batch_score`, `batch_logw_next`).\n\n    This class inherits all methods from [`Potential`][genlm.control.potential.base.Potential].\n\n    Attributes:\n        potential (Potential): The underlying potential instance that is being wrapped.\n        background_loop (AsyncBatchLoop): An asynchronous loop that manages batch requests.\n    \"\"\"\n\n    def __init__(self, potential):\n        self.potential = potential\n        self.background_loop = AsyncBatchLoop(potential)\n        self.background_loop.start()\n        super().__init__(potential.vocab)\n\n    async def complete(self, context):\n        return await self.background_loop.queue_request(\n            \"batch_complete\", lambda args: ([*args[0], context],)\n        )\n\n    async def prefix(self, context):\n        return await self.background_loop.queue_request(\n            \"batch_prefix\", lambda args: ([*args[0], context],)\n        )\n\n    async def score(self, context):\n        return await self.background_loop.queue_request(\n            \"batch_score\", lambda args: ([*args[0], context],)\n        )\n\n    async def logw_next(self, context):\n        return await self.background_loop.queue_request(\n            \"batch_logw_next\", lambda args: ([*args[0], context],)\n        )\n\n    async def batch_complete(self, contexts):\n        return await self.potential.batch_complete(contexts)\n\n    async def batch_prefix(self, contexts):\n        return await self.potential.batch_prefix(contexts)\n\n    async def batch_score(self, contexts):\n        return await self.potential.batch_score(contexts)\n\n    async def batch_logw_next(self, contexts):\n        return await self.potential.batch_logw_next(contexts)\n\n    def spawn(self, *args, **kwargs):\n        # creates a new background loop.\n        return AutoBatchedPotential(self.potential.spawn(*args, **kwargs))\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.potential!r})\"\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        await self.background_loop.cleanup()\n\n    def __del__(self):\n        if loop := getattr(self, \"background_loop\", None):\n            loop.close()\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.AutoBatchedPotential.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    await self.background_loop.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.MultiProcPotential","title":"<code>MultiProcPotential</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A Potential that adds parallel processing capabilities to any base Potential implementation.</p> <p>Creates a process pool of worker processes, each containing an instance of the potential.</p> <p>This class inherits all methods from <code>Potential</code>. Each method delegates to a corresponding method of the potential instances running in the worker processes, distributing work across multiple processes for improved performance.</p> Source code in <code>genlm/control/potential/multi_proc.py</code> <pre><code>class MultiProcPotential(Potential):\n    \"\"\"A Potential that adds parallel processing capabilities to any base Potential implementation.\n\n    Creates a process pool of worker processes, each containing an instance of the potential.\n\n    This class inherits all methods from [`Potential`][genlm.control.potential.base.Potential].\n    Each method delegates to a corresponding method of the potential instances running in the\n    worker processes, distributing work across multiple processes for improved performance.\n    \"\"\"\n\n    def __init__(self, potential_factory, factory_args, num_workers=2):\n        \"\"\"\n        Initialize the MultiProcPotential.\n\n        Args:\n            potential_factory (callable): A factory function that creates a potential instance.\n            factory_args (tuple): Arguments to pass to the potential factory.\n            num_workers (int): The number of worker processes to spawn. Each will contain an instance of the potential.\n        \"\"\"\n        self.num_workers = num_workers\n        self.executor = ProcessPoolExecutor(\n            max_workers=num_workers,\n            initializer=self._init_worker,\n            initargs=(potential_factory, factory_args),\n        )\n        # Get vocab and eos from one of the workers\n        vocab, eos = self.executor.submit(self._get_vocab_and_eos).result()\n        super().__init__(vocab, eos=eos)\n\n    @staticmethod\n    def _init_worker(factory, args):\n        global _worker_potential, _worker_event_loop\n        _worker_potential = factory(*args)\n        _worker_event_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(_worker_event_loop)\n\n    @staticmethod\n    def _get_vocab_and_eos():\n        return _worker_potential.vocab, _worker_potential.eos\n\n    @staticmethod\n    def _run_coroutine(coroutine):\n        global _worker_event_loop\n        return _worker_event_loop.run_until_complete(coroutine)\n\n    @staticmethod\n    def _worker_logw_next(context):\n        return MultiProcPotential._run_coroutine(\n            _worker_potential.logw_next(context)\n        ).weights\n\n    @staticmethod\n    def _worker_prefix(context):\n        return MultiProcPotential._run_coroutine(_worker_potential.prefix(context))\n\n    @staticmethod\n    def _worker_complete(context):\n        return MultiProcPotential._run_coroutine(_worker_potential.complete(context))\n\n    # @staticmethod\n    # def _worker_score(context):\n    #    return MultiProcPotential._run_coroutine(_worker_potential.score(context))\n\n    async def _run_in_executor(self, func, *args):\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, func, *args)\n\n    async def logw_next(self, context):\n        result = await self._run_in_executor(self._worker_logw_next, context)\n        return self.make_lazy_weights(result)\n\n    async def prefix(self, context):\n        return await self._run_in_executor(self._worker_prefix, context)\n\n    async def complete(self, context):\n        return await self._run_in_executor(self._worker_complete, context)\n\n    async def batch_logw_next(self, contexts):\n        results = await asyncio.gather(\n            *(\n                self._run_in_executor(self._worker_logw_next, context)\n                for context in contexts\n            )\n        )\n        return [self.make_lazy_weights(result) for result in results]\n\n    async def batch_complete(self, contexts):\n        results = await asyncio.gather(\n            *(\n                self._run_in_executor(self._worker_complete, context)\n                for context in contexts\n            )\n        )\n        return np.array(results)\n\n    async def batch_prefix(self, contexts):\n        results = await asyncio.gather(\n            *(\n                self._run_in_executor(self._worker_prefix, context)\n                for context in contexts\n            )\n        )\n        return np.array(results)\n\n    def __del__(self):\n        if self.executor is not None:\n            self.executor.shutdown()\n            self.executor = None\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.num_workers=})\"\n\n    def spawn(self):\n        raise ValueError(\"MultiProcPotentials are not spawnable.\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.MultiProcPotential.__init__","title":"<code>__init__(potential_factory, factory_args, num_workers=2)</code>","text":"<p>Initialize the MultiProcPotential.</p> <p>Parameters:</p> Name Type Description Default <code>potential_factory</code> <code>callable</code> <p>A factory function that creates a potential instance.</p> required <code>factory_args</code> <code>tuple</code> <p>Arguments to pass to the potential factory.</p> required <code>num_workers</code> <code>int</code> <p>The number of worker processes to spawn. Each will contain an instance of the potential.</p> <code>2</code> Source code in <code>genlm/control/potential/multi_proc.py</code> <pre><code>def __init__(self, potential_factory, factory_args, num_workers=2):\n    \"\"\"\n    Initialize the MultiProcPotential.\n\n    Args:\n        potential_factory (callable): A factory function that creates a potential instance.\n        factory_args (tuple): Arguments to pass to the potential factory.\n        num_workers (int): The number of worker processes to spawn. Each will contain an instance of the potential.\n    \"\"\"\n    self.num_workers = num_workers\n    self.executor = ProcessPoolExecutor(\n        max_workers=num_workers,\n        initializer=self._init_worker,\n        initargs=(potential_factory, factory_args),\n    )\n    # Get vocab and eos from one of the workers\n    vocab, eos = self.executor.submit(self._get_vocab_and_eos).result()\n    super().__init__(vocab, eos=eos)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PotentialOps","title":"<code>PotentialOps</code>","text":"<p>Mixin providing operations for potential functions:</p> <ol> <li> <p>Product (<code>*</code>): Take the product of two potentials.</p> </li> <li> <p>Coercion (<code>coerce</code>): Coerce the potential to operate on another potential's vocabulary.</p> </li> <li> <p>Auto-batching (<code>to_autobatched</code>): Create a version that automatically batches concurrent requests to the instance methods.</p> </li> <li> <p>Parallelization (<code>to_multiprocess</code>): Create a version that parallelizes operations over multiple processes.</p> </li> </ol> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>class PotentialOps:\n    \"\"\"Mixin providing operations for potential functions:\n\n    1. Product (`*`): Take the product of two potentials.\\n\n    2. Coercion (`coerce`): Coerce the potential to operate on another potential's vocabulary.\\n\n    3. Auto-batching (`to_autobatched`): Create a version that automatically batches concurrent requests to the instance methods.\\n\n    4. Parallelization (`to_multiprocess`): Create a version that parallelizes operations over multiple processes.\\n\n    \"\"\"\n\n    def __mul__(self, other):\n        \"\"\"Take the product of two potentials.\n\n        See [`Product`][genlm.control.potential.product.Product] for more details.\n\n        Args:\n            other (Potential): Another potential instance to take the product with.\n\n        Returns:\n            (Product): A Product instance representing the unnormalized product of the two potentials.\n\n        Note:\n            Potentials must operate on the same token type and the intersection of their vocabularies must be non-empty.\n        \"\"\"\n        from genlm.control.potential.product import Product\n\n        return Product(self, other)\n\n    def coerce(self, other, f, prune=True):\n        \"\"\"Coerce the current potential to operate on the vocabulary of another potential.\n\n        See [`Coerced`][genlm.control.potential.coerce.Coerced] for more details.\n\n        Args:\n            other (Potential): The potential instance whose vocabulary will be used.\n            f (callable): A function mapping sequences of tokens from self's vocab to sequences of tokens from other's vocab.\n            prune (bool): Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary.\n                If `False`, the coerced potential's vocabulary will include all tokens from the target vocabulary.\n\n        Returns:\n            (Coerced): A Potential that operates on the vocabulary of `other`.\n        \"\"\"\n        from genlm.control.potential.coerce import Coerced\n\n        return Coerced(self, other.vocab, f=f, prune=prune)\n\n    def to_autobatched(self):\n        \"\"\"Create a new potential instance that automatically batches concurrent requests to the instance methods.\n\n        See [`AutoBatchedPotential`][genlm.control.potential.autobatch.AutoBatchedPotential] for more details.\n\n        Returns:\n            (AutoBatchedPotential): A new potential instance that wraps the current potential and automatically batches concurrent requests to the instance methods.\n        \"\"\"\n        from genlm.control.potential.autobatch import AutoBatchedPotential\n\n        return AutoBatchedPotential(self)\n\n    def to_multiprocess(self, num_workers=2, spawn_args=None):\n        \"\"\"Create a new potential instance that parallelizes operations using multiprocessing.\n\n        See [`MultiProcPotential`][genlm.control.potential.multi_proc.MultiProcPotential] for more details.\n\n        Args:\n            num_workers (int): The number of workers to use in the multiprocessing pool.\n            spawn_args (tuple): The positional arguments to pass to the potential's `spawn` method.\n\n        Returns:\n            (MultiProcPotential): A new potential instance that wraps the current potential and uses multiprocessing to parallelize operations.\n\n        Note:\n            For this method to be used, the potential must implement a picklable `spawn` method.\n        \"\"\"\n        from genlm.control.potential.multi_proc import MultiProcPotential\n\n        factory_args = spawn_args or ()\n        return MultiProcPotential(\n            potential_factory=self.spawn,\n            factory_args=factory_args,\n            num_workers=num_workers,\n        )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PotentialOps.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Take the product of two potentials.</p> <p>See <code>Product</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Potential</code> <p>Another potential instance to take the product with.</p> required <p>Returns:</p> Type Description <code>Product</code> <p>A Product instance representing the unnormalized product of the two potentials.</p> Note <p>Potentials must operate on the same token type and the intersection of their vocabularies must be non-empty.</p> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>def __mul__(self, other):\n    \"\"\"Take the product of two potentials.\n\n    See [`Product`][genlm.control.potential.product.Product] for more details.\n\n    Args:\n        other (Potential): Another potential instance to take the product with.\n\n    Returns:\n        (Product): A Product instance representing the unnormalized product of the two potentials.\n\n    Note:\n        Potentials must operate on the same token type and the intersection of their vocabularies must be non-empty.\n    \"\"\"\n    from genlm.control.potential.product import Product\n\n    return Product(self, other)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PotentialOps.coerce","title":"<code>coerce(other, f, prune=True)</code>","text":"<p>Coerce the current potential to operate on the vocabulary of another potential.</p> <p>See <code>Coerced</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Potential</code> <p>The potential instance whose vocabulary will be used.</p> required <code>f</code> <code>callable</code> <p>A function mapping sequences of tokens from self's vocab to sequences of tokens from other's vocab.</p> required <code>prune</code> <code>bool</code> <p>Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary. If <code>False</code>, the coerced potential's vocabulary will include all tokens from the target vocabulary.</p> <code>True</code> <p>Returns:</p> Type Description <code>Coerced</code> <p>A Potential that operates on the vocabulary of <code>other</code>.</p> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>def coerce(self, other, f, prune=True):\n    \"\"\"Coerce the current potential to operate on the vocabulary of another potential.\n\n    See [`Coerced`][genlm.control.potential.coerce.Coerced] for more details.\n\n    Args:\n        other (Potential): The potential instance whose vocabulary will be used.\n        f (callable): A function mapping sequences of tokens from self's vocab to sequences of tokens from other's vocab.\n        prune (bool): Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary.\n            If `False`, the coerced potential's vocabulary will include all tokens from the target vocabulary.\n\n    Returns:\n        (Coerced): A Potential that operates on the vocabulary of `other`.\n    \"\"\"\n    from genlm.control.potential.coerce import Coerced\n\n    return Coerced(self, other.vocab, f=f, prune=prune)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PotentialOps.to_autobatched","title":"<code>to_autobatched()</code>","text":"<p>Create a new potential instance that automatically batches concurrent requests to the instance methods.</p> <p>See <code>AutoBatchedPotential</code> for more details.</p> <p>Returns:</p> Type Description <code>AutoBatchedPotential</code> <p>A new potential instance that wraps the current potential and automatically batches concurrent requests to the instance methods.</p> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>def to_autobatched(self):\n    \"\"\"Create a new potential instance that automatically batches concurrent requests to the instance methods.\n\n    See [`AutoBatchedPotential`][genlm.control.potential.autobatch.AutoBatchedPotential] for more details.\n\n    Returns:\n        (AutoBatchedPotential): A new potential instance that wraps the current potential and automatically batches concurrent requests to the instance methods.\n    \"\"\"\n    from genlm.control.potential.autobatch import AutoBatchedPotential\n\n    return AutoBatchedPotential(self)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PotentialOps.to_multiprocess","title":"<code>to_multiprocess(num_workers=2, spawn_args=None)</code>","text":"<p>Create a new potential instance that parallelizes operations using multiprocessing.</p> <p>See <code>MultiProcPotential</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>num_workers</code> <code>int</code> <p>The number of workers to use in the multiprocessing pool.</p> <code>2</code> <code>spawn_args</code> <code>tuple</code> <p>The positional arguments to pass to the potential's <code>spawn</code> method.</p> <code>None</code> <p>Returns:</p> Type Description <code>MultiProcPotential</code> <p>A new potential instance that wraps the current potential and uses multiprocessing to parallelize operations.</p> Note <p>For this method to be used, the potential must implement a picklable <code>spawn</code> method.</p> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>def to_multiprocess(self, num_workers=2, spawn_args=None):\n    \"\"\"Create a new potential instance that parallelizes operations using multiprocessing.\n\n    See [`MultiProcPotential`][genlm.control.potential.multi_proc.MultiProcPotential] for more details.\n\n    Args:\n        num_workers (int): The number of workers to use in the multiprocessing pool.\n        spawn_args (tuple): The positional arguments to pass to the potential's `spawn` method.\n\n    Returns:\n        (MultiProcPotential): A new potential instance that wraps the current potential and uses multiprocessing to parallelize operations.\n\n    Note:\n        For this method to be used, the potential must implement a picklable `spawn` method.\n    \"\"\"\n    from genlm.control.potential.multi_proc import MultiProcPotential\n\n    factory_args = spawn_args or ()\n    return MultiProcPotential(\n        potential_factory=self.spawn,\n        factory_args=factory_args,\n        num_workers=num_workers,\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Product","title":"<code>Product</code>","text":"<p>               Bases: <code>Potential</code></p> <p>Combine two potential instances via element-wise multiplication (sum in log space).</p> <p>This class creates a new potential that is the element-wise product of two potentials: <pre><code>prefix(xs) = p1.prefix(xs) + p2.prefix(xs)\ncomplete(xs) = p1.complete(xs) + p2.complete(xs)\nlogw_next(x | xs) = p1.logw_next(x | xs) + p2.logw_next(x | xs)\n</code></pre></p> <p>The new potential's vocabulary is the intersection of the two potentials' vocabularies.</p> <p>This class inherits all methods from <code>Potential</code>, see there for method documentation.</p> <p>Attributes:</p> Name Type Description <code>p1</code> <code>Potential</code> <p>The first potential instance.</p> <code>p2</code> <code>Potential</code> <p>The second potential instance.</p> <code>token_type</code> <code>str</code> <p>The type of tokens that this product potential operates on.</p> <code>vocab</code> <code>list</code> <p>The common vocabulary shared between the two potentials.</p> Warning <p>Be careful when taking products of potentials with minimal vocabulary overlap. The resulting potential will only operate on tokens present in both vocabularies.</p> Source code in <code>genlm/control/potential/product.py</code> <pre><code>class Product(Potential):\n    \"\"\"\n    Combine two potential instances via element-wise multiplication (sum in log space).\n\n    This class creates a new potential that is the element-wise product of two potentials:\n    ```\n    prefix(xs) = p1.prefix(xs) + p2.prefix(xs)\n    complete(xs) = p1.complete(xs) + p2.complete(xs)\n    logw_next(x | xs) = p1.logw_next(x | xs) + p2.logw_next(x | xs)\n    ```\n\n    The new potential's vocabulary is the intersection of the two potentials' vocabularies.\n\n    This class inherits all methods from [`Potential`][genlm.control.potential.base.Potential],\n    see there for method documentation.\n\n    Attributes:\n        p1 (Potential): The first potential instance.\n        p2 (Potential): The second potential instance.\n        token_type (str): The type of tokens that this product potential operates on.\n        vocab (list): The common vocabulary shared between the two potentials.\n\n    Warning:\n        Be careful when taking products of potentials with minimal vocabulary overlap.\n        The resulting potential will only operate on tokens present in both vocabularies.\n    \"\"\"\n\n    def __init__(self, p1, p2):\n        \"\"\"Initialize a Product potential.\n\n        Args:\n            p1 (Potential): First potential\n            p2 (Potential): Second potential\n        \"\"\"\n        self.p1 = p1\n        self.p2 = p2\n\n        if self.p1.token_type == self.p2.token_type:\n            self.token_type = self.p1.token_type\n        else:\n            raise ValueError(\n                \"Potentials in product must have the same token type. \"\n                f\"Got {self.p1.token_type} and {self.p2.token_type}.\"\n                + (\n                    \"\\nMaybe you forgot to coerce the potentials to the same token type? See `Coerce`.\"\n                    if (\n                        self.p1.token_type.is_iterable_of(self.p2.token_type)\n                        or self.p2.token_type.is_iterable_of(self.p1.token_type)\n                    )\n                    else \"\"\n                )\n            )\n\n        common_vocab = list(set(p1.vocab) &amp; set(p2.vocab))\n        if not common_vocab:\n            raise ValueError(\"Potentials in product must share a common vocabulary\")\n\n        # Check for small vocabulary overlap\n        threshold = 0.1\n        for potential, name in [(p1, \"p1\"), (p2, \"p2\")]:\n            overlap_ratio = len(common_vocab) / len(potential.vocab)\n            if overlap_ratio &lt; threshold:\n                warnings.warn(\n                    f\"Common vocabulary ({len(common_vocab)} tokens) is less than {threshold * 100}% \"\n                    f\"of {name}'s ({potential!r}) vocabulary ({len(potential.vocab)} tokens). \"\n                    \"This Product potential only operates on this relatively small subset of tokens.\",\n                    RuntimeWarning,\n                )\n\n        super().__init__(common_vocab, token_type=self.token_type)\n\n        # For fast products of weights\n        self.v1_idxs = [p1.lookup[token] for token in self.vocab_eos]\n        self.v2_idxs = [p2.lookup[token] for token in self.vocab_eos]\n\n    async def prefix(self, context):\n        w1 = await self.p1.prefix(context)\n        if w1 == float(\"-inf\"):\n            return float(\"-inf\")\n        w2 = await self.p2.prefix(context)\n        return w1 + w2\n\n    async def complete(self, context):\n        w1 = await self.p1.complete(context)\n        if w1 == float(\"-inf\"):\n            return float(\"-inf\")\n        w2 = await self.p2.complete(context)\n        return w1 + w2\n\n    async def batch_complete(self, contexts):\n        W1, W2 = await asyncio.gather(\n            self.p1.batch_complete(contexts), self.p2.batch_complete(contexts)\n        )\n        return W1 + W2\n\n    async def batch_prefix(self, contexts):\n        W1, W2 = await asyncio.gather(\n            self.p1.batch_prefix(contexts), self.p2.batch_prefix(contexts)\n        )\n        return W1 + W2\n\n    async def logw_next(self, context):\n        W1, W2 = await asyncio.gather(\n            self.p1.logw_next(context), self.p2.logw_next(context)\n        )\n        return self.make_lazy_weights(\n            W1.weights[self.v1_idxs] + W2.weights[self.v2_idxs]\n        )\n\n    async def batch_logw_next(self, contexts):\n        Ws1, Ws2 = await asyncio.gather(\n            self.p1.batch_logw_next(contexts), self.p2.batch_logw_next(contexts)\n        )\n        return [\n            self.make_lazy_weights(\n                Ws1[n].weights[self.v1_idxs] + Ws2[n].weights[self.v2_idxs]\n            )\n            for n in range(len(contexts))\n        ]\n\n    def spawn(self, p1_opts=None, p2_opts=None):\n        return Product(\n            self.p1.spawn(**(p1_opts or {})),\n            self.p2.spawn(**(p2_opts or {})),\n        )\n\n    def __repr__(self):\n        return f\"Product({self.p1!r}, {self.p2!r})\"\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Product.__init__","title":"<code>__init__(p1, p2)</code>","text":"<p>Initialize a Product potential.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Potential</code> <p>First potential</p> required <code>p2</code> <code>Potential</code> <p>Second potential</p> required Source code in <code>genlm/control/potential/product.py</code> <pre><code>def __init__(self, p1, p2):\n    \"\"\"Initialize a Product potential.\n\n    Args:\n        p1 (Potential): First potential\n        p2 (Potential): Second potential\n    \"\"\"\n    self.p1 = p1\n    self.p2 = p2\n\n    if self.p1.token_type == self.p2.token_type:\n        self.token_type = self.p1.token_type\n    else:\n        raise ValueError(\n            \"Potentials in product must have the same token type. \"\n            f\"Got {self.p1.token_type} and {self.p2.token_type}.\"\n            + (\n                \"\\nMaybe you forgot to coerce the potentials to the same token type? See `Coerce`.\"\n                if (\n                    self.p1.token_type.is_iterable_of(self.p2.token_type)\n                    or self.p2.token_type.is_iterable_of(self.p1.token_type)\n                )\n                else \"\"\n            )\n        )\n\n    common_vocab = list(set(p1.vocab) &amp; set(p2.vocab))\n    if not common_vocab:\n        raise ValueError(\"Potentials in product must share a common vocabulary\")\n\n    # Check for small vocabulary overlap\n    threshold = 0.1\n    for potential, name in [(p1, \"p1\"), (p2, \"p2\")]:\n        overlap_ratio = len(common_vocab) / len(potential.vocab)\n        if overlap_ratio &lt; threshold:\n            warnings.warn(\n                f\"Common vocabulary ({len(common_vocab)} tokens) is less than {threshold * 100}% \"\n                f\"of {name}'s ({potential!r}) vocabulary ({len(potential.vocab)} tokens). \"\n                \"This Product potential only operates on this relatively small subset of tokens.\",\n                RuntimeWarning,\n            )\n\n    super().__init__(common_vocab, token_type=self.token_type)\n\n    # For fast products of weights\n    self.v1_idxs = [p1.lookup[token] for token in self.vocab_eos]\n    self.v2_idxs = [p2.lookup[token] for token in self.vocab_eos]\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Coerced","title":"<code>Coerced</code>","text":"<p>               Bases: <code>Potential</code></p> <p>Coerce a potential to operate on another vocabulary.</p> <p>This class allows a potential to be adapted to work with a different set of tokens, defined by a target vocabulary and coersion function.</p> <p>This class inherits all methods from <code>Potential</code>. Each method delegates to the corresponding method of the underlying potential, but first maps any input token sequences from the target vocabulary to the original potential's vocabulary using the coercion function.</p> <p>Formally, if \\(f\\) is the coercion function, then for any sequence \\(x_1, \\ldots, x_n\\) of tokens from the target vocabulary, $$ \\textsf{Coerced.prefix}(x_1, \\ldots, x_n) = \\textsf{Coerced.potential.prefix}(f(x_1, \\ldots, x_n)) $$</p> \\[ \\textsf{Coerced.complete}(x_1, \\ldots, x_n) = \\textsf{Coerced.potential.complete}(f(x_1, \\ldots, x_n)) \\] <p>Attributes:</p> Name Type Description <code>potential</code> <code>Potential</code> <p>The original potential instance that is being coerced.</p> <code>f</code> <code>callable</code> <p>A function that maps sequences of tokens from the target vocabulary to sequences of tokens from the original potential's vocabulary.</p> Note <p>The coerced potential's vocabulary will by default be pruned to only include tokens that can be mapped to the original potential's vocabulary via the coercion function (i.e. <code>set(f([x])) &lt;= set(potential.vocab)</code>). If no such tokens are found, a <code>ValueError</code> is raised. This behavior can be overridden by setting <code>prune=False</code>, in which case the coerced potential's vocabulary will include all tokens from the target vocabulary.</p> Source code in <code>genlm/control/potential/coerce.py</code> <pre><code>class Coerced(Potential):\n    \"\"\"\n    Coerce a potential to operate on another vocabulary.\n\n    This class allows a potential to be adapted to work with a different set of tokens,\n    defined by a target vocabulary and coersion function.\n\n    This class inherits all methods from [`Potential`][genlm.control.potential.base.Potential].\n    Each method delegates to the corresponding method of the underlying potential, but first\n    maps any input token sequences from the target vocabulary to the original potential's vocabulary\n    using the coercion function.\n\n    Formally, if $f$ is the coercion function, then for any sequence $x_1, \\\\ldots, x_n$ of tokens from the target vocabulary,\n    $$\n    \\\\textsf{Coerced.prefix}(x_1, \\\\ldots, x_n) = \\\\textsf{Coerced.potential.prefix}(f(x_1, \\\\ldots, x_n))\n    $$\n\n    $$\n    \\\\textsf{Coerced.complete}(x_1, \\\\ldots, x_n) = \\\\textsf{Coerced.potential.complete}(f(x_1, \\\\ldots, x_n))\n    $$\n\n    Attributes:\n        potential (Potential): The original potential instance that is being coerced.\n        f (callable): A function that maps sequences of tokens from the target vocabulary to sequences of tokens from\n            the original potential's vocabulary.\n\n    Note:\n        The coerced potential's vocabulary will by default be pruned to only include tokens that can be mapped to the original potential's vocabulary\n        via the coercion function (i.e. `set(f([x])) &lt;= set(potential.vocab)`). If no such tokens are found, a `ValueError` is raised.\n        This behavior can be overridden by setting `prune=False`, in which case the coerced potential's vocabulary will include all tokens from the target vocabulary.\n    \"\"\"\n\n    def __init__(self, potential, target_vocab, f, prune=True):\n        \"\"\"\n        Initialize a Coerced potential.\n\n        Args:\n            potential (Potential): The original potential instance that is being coerced.\n            target_vocab (list): The target vocabulary that the potential will operate on.\n                Each element of `target_vocab` must be hashable.\n            f (callable): A function that maps iterables of tokens from the target vocabulary\n                to the original potential's vocabulary.\n            prune (bool): Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary.\n                If `False`, the coerced potential's vocabulary will include all tokens from the target vocabulary.\n\n        Raises:\n            ValueError: If no valid tokens are found in the target vocabulary that can be mapped to the original potential's vocabulary.\n        \"\"\"\n        self.potential = potential\n        self.f = f\n\n        if prune:\n            tokens = []\n            for target_token in target_vocab:\n                base_token = f([target_token])\n                if set(base_token) &lt;= set(potential.vocab):\n                    tokens.append(target_token)\n        else:\n            tokens = target_vocab\n\n        if not tokens:\n            raise ValueError(\"No valid tokens found in target vocabulary\")\n\n        super().__init__(tokens)\n\n    def _batch_f(self, contexts):\n        return [self.f(context) for context in contexts]\n\n    async def complete(self, context):\n        return await self.potential.complete(context=self.f(context))\n\n    async def prefix(self, context):\n        return await self.potential.prefix(context=self.f(context))\n\n    async def logw_next(self, context):\n        Ws = self.alloc_logws()\n        ctx = self.f(context)\n        ctx_w = await self.potential.prefix(ctx)\n        Ws[-1] = await self.potential.complete(ctx) - ctx_w\n        exts = [self.f(chain(context, [x])) for x in self.vocab]  # slow!!\n        Ws[:-1] = await self.potential.batch_prefix(exts) - ctx_w\n        return self.make_lazy_weights(Ws)\n\n    async def batch_complete(self, contexts):\n        return await self.potential.batch_complete(contexts=self._batch_f(contexts))\n\n    async def batch_prefix(self, contexts):\n        return await self.potential.batch_prefix(contexts=self._batch_f(contexts))\n\n    async def batch_logw_next(self, contexts):\n        return await asyncio.gather(*[self.logw_next(context) for context in contexts])\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.potential!r})\"\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.Coerced.__init__","title":"<code>__init__(potential, target_vocab, f, prune=True)</code>","text":"<p>Initialize a Coerced potential.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The original potential instance that is being coerced.</p> required <code>target_vocab</code> <code>list</code> <p>The target vocabulary that the potential will operate on. Each element of <code>target_vocab</code> must be hashable.</p> required <code>f</code> <code>callable</code> <p>A function that maps iterables of tokens from the target vocabulary to the original potential's vocabulary.</p> required <code>prune</code> <code>bool</code> <p>Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary. If <code>False</code>, the coerced potential's vocabulary will include all tokens from the target vocabulary.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid tokens are found in the target vocabulary that can be mapped to the original potential's vocabulary.</p> Source code in <code>genlm/control/potential/coerce.py</code> <pre><code>def __init__(self, potential, target_vocab, f, prune=True):\n    \"\"\"\n    Initialize a Coerced potential.\n\n    Args:\n        potential (Potential): The original potential instance that is being coerced.\n        target_vocab (list): The target vocabulary that the potential will operate on.\n            Each element of `target_vocab` must be hashable.\n        f (callable): A function that maps iterables of tokens from the target vocabulary\n            to the original potential's vocabulary.\n        prune (bool): Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary.\n            If `False`, the coerced potential's vocabulary will include all tokens from the target vocabulary.\n\n    Raises:\n        ValueError: If no valid tokens are found in the target vocabulary that can be mapped to the original potential's vocabulary.\n    \"\"\"\n    self.potential = potential\n    self.f = f\n\n    if prune:\n        tokens = []\n        for target_token in target_vocab:\n            base_token = f([target_token])\n            if set(base_token) &lt;= set(potential.vocab):\n                tokens.append(target_token)\n    else:\n        tokens = target_vocab\n\n    if not tokens:\n        raise ValueError(\"No valid tokens found in target vocabulary\")\n\n    super().__init__(tokens)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM","title":"<code>PromptedLLM</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A potential representing a language model conditioned on a fixed prompt prefix.</p> <p><code>PromptedLLM</code>s operate on byte sequences.</p> <p>Notes on EOS Token Handling:</p> <ul> <li> <p>Tokens to treat as end-of-sequence tokens are specified via the <code>eos_tokens</code> argument.</p> </li> <li> <p>These tokens are excluded from the potential's vocabulary and as such do not appear in the <code>vocab</code> attribute.</p> <p>This means they cannot appear in any input contexts to the potential nor in the output of <code>logw_next</code>. They can be used in the prompt however.</p> </li> <li> <p>The log probability assigned to the <code>genlm.control</code>'s reserved <code>EOS</code> token is the sum of the log probabilities of all the specified EOS tokens.</p> </li> </ul> <p>This class wraps an <code>AsyncLM</code> instance.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>class PromptedLLM(Potential):\n    \"\"\"A potential representing a language model conditioned on a fixed prompt prefix.\n\n    `PromptedLLM`s operate on byte sequences.\n\n    Notes on EOS Token Handling:\\n\n    - Tokens to treat as end-of-sequence tokens are specified via the `eos_tokens` argument.\\n\n    - These tokens are excluded from the potential's vocabulary and as such do not appear in the `vocab` attribute.\\n\n        This means they cannot appear in any input contexts to the potential nor in the output of `logw_next`. They can be used in the prompt however.\\n\n    - The log probability assigned to the `genlm.control`'s reserved `EOS` token is the sum of the log probabilities of all the specified EOS tokens.\\n\n\n    This class wraps an `AsyncLM` instance.\n    \"\"\"\n\n    def __init__(self, llm, prompt_ids=None, eos_tokens=None, temperature=1):\n        \"\"\"`\n        Initializes the PromptedLLM potential.\n\n        Args:\n            llm (AsyncLM): The language model to use.\n            prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n                Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `prompt` or `prompt_ids`.\n            eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n                Defaults to the EOS token of the language model's tokenizer.\n            temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n\n        Raises:\n            ValueError: If any EOS token is not in the language model vocabulary.\n        \"\"\"\n        self.model = llm\n        self.prompt_ids = prompt_ids or []\n\n        if not eos_tokens:\n            self._eos_tokens = [llm.byte_vocab[self.model.tokenizer.eos_token_id]]\n        else:\n            self._eos_tokens = eos_tokens\n\n        assert len(set(self._eos_tokens)) == len(self._eos_tokens), (\n            \"duplicate eos tokens\"\n        )\n\n        self.token_maps = TokenMappings.create(\n            decode=llm.byte_vocab, eos_tokens=self._eos_tokens\n        )\n\n        self.temperature = temperature\n\n        V = [x for x in self.token_maps.decode if x not in self._eos_tokens]\n\n        super().__init__(vocabulary=V)\n\n    @classmethod\n    def from_name(\n        cls,\n        name,\n        backend=None,\n        eos_tokens=None,\n        prompt_ids=None,\n        temperature=1.0,\n        **kwargs,\n    ):\n        \"\"\"Create a `PromptedLLM` from a HugginFace model name.\n\n        Args:\n            name (str): Name of the model to load\n            backend (str, optional): `AsyncLM` backend to use:\\n\n                * 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\\n\n                * 'hf' for an `AsyncTransformer`; ideal for CPU usage\\n\n                * 'mock' for a `MockAsyncLM`; ideal for testing.\\n\n                Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n            eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n                Defaults to the EOS token of the language model's tokenizer.\n            prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n                Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `set_prompt_from_str` or `prompt_ids`.\n            temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n            **kwargs (dict): Additional arguments passed to AsyncLM constructor\n\n        Returns:\n            (PromptedLLM): An instance of PromptedLLM\n        \"\"\"\n        backend = backend or (\"vllm\" if torch.cuda.is_available() else \"hf\")\n        model = load_model_by_name(name, backend=backend, **kwargs)\n        return cls(\n            model, prompt_ids=prompt_ids, eos_tokens=eos_tokens, temperature=temperature\n        )\n\n    @property\n    def eos_tokens(self):\n        return self._eos_tokens\n\n    @eos_tokens.setter\n    def eos_tokens(self, value):\n        raise ValueError(\n            \"Cannot reset eos_tokens after initialization. \"\n            \"Use spawn_new_eos(new_eos_tokens) instead.\"\n        )\n\n    @property\n    def prompt(self):\n        \"\"\"\n        Get the current prompt as a list of byte sequences corresponding to the prompt token IDs.\n\n        Returns:\n            (list[bytes]|None): The current prompt as a list of bytes sequences or None if no prompt_ids are set.\n        \"\"\"\n        if not self.prompt_ids:\n            return  # pragma: no cover\n        return [self.token_maps.decode[x] for x in self.prompt_ids]\n\n    def set_prompt_from_str(self, prompt_str):\n        \"\"\"Set the fixed prompt from a string.\n\n        Modifies `prompt_ids` to be the token IDs of the input prompt according to the language model's tokenizer.\n\n        Args:\n            prompt_str (str): The prompt to set.\n        \"\"\"\n        # TODO: Handle race condition where prompt_ids reset concurrently.\n        if not isinstance(prompt_str, str):\n            raise ValueError(\n                f\"Prompt must a string got {type(prompt_str)}. \"\n                f\"To set the prompt from a list of token IDs, use prompt_ids.\"\n            )\n\n        if prompt_str.endswith(\" \"):\n            warnings.warn(\n                \"Prompt ends with whitespace, which may affect tokenization. \"\n                \"Consider removing trailing whitespace.\",\n                stacklevel=2,\n            )\n\n        self.prompt_ids = self.model.tokenizer.encode(prompt_str)\n\n    def encode_tokens(self, tokens):\n        \"\"\"Encode a list of byte tokens to a list of token IDs in\n        the underlying language model's vocabulary.\n\n        Args:\n            tokens (list[bytes]): List of byte tokens to encode\n\n        Returns:\n            (list[int]): A list of token IDs corresponding to the input tokens.\n\n        Raises:\n            ValueError: If any token is not in the vocabulary\n        \"\"\"\n        try:\n            return [self.token_maps.encode[x] for x in tokens]\n        except KeyError as e:\n            raise ValueError(f\"Token {e.args[0]} not in vocabulary\") from e\n\n    def decode_tokens(self, ids):\n        \"\"\"\n        Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.\n\n        Args:\n            ids (list[int]): A list of token IDs in the language model's vocabulary.\n\n        Returns:\n            (list[bytes]): A list of byte tokens corresponding to the input token IDs.\n        \"\"\"\n        return [self.token_maps.decode[x] for x in ids]\n\n    def tokenize(self, context_str):\n        \"\"\"Tokenize a string to a list of `bytes` objects, each corresponding to a token in the vocabulary.\n\n        Uses the language model's tokenizer to map `context_str` to a list of token IDs, and then decodes the token IDs to bytes.\n\n        Args:\n            context_str (str): A string to encode\n\n        Returns:\n            (List[bytes]): A list of byte tokens corresponding to the input string.\n        \"\"\"\n        return self.decode_tokens(self.model.tokenizer.encode(context_str))\n\n    async def log_probability(self, context):\n        \"\"\"\n        Compute the log probability of `context` given the prompt.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of `context`.\n        \"\"\"\n        if not context:\n            return 0\n\n        context_ids = self.encode_tokens(context)\n        return await self._log_probability(context_ids)\n\n    async def _log_probability(self, context_ids):\n        prefixes = [self.prompt_ids + context_ids[:i] for i in range(len(context_ids))]\n        log_ps = self._maybe_temper(\n            await self.model.batch_next_token_logprobs(prefixes)\n        )\n        target_ids = torch.tensor(context_ids, device=log_ps.device)\n        with torch.no_grad():\n            token_logprobs = torch.gather(log_ps, 1, target_ids.unsqueeze(1))\n            total_logprob = token_logprobs.sum().item()\n\n        return total_logprob\n\n    def _maybe_temper(self, logps):\n        if self.temperature == 1:\n            return logps\n        return torch.log_softmax(logps / self.temperature, dim=-1)\n\n    async def prefix(self, context):\n        \"\"\"\n        Compute the log probability of `context` given the prompt.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of `context`.\n        \"\"\"\n        return await self.log_probability(context)\n\n    async def complete(self, context):\n        \"\"\"\n        Compute the log probability of `context` and the eos tokens given the prompt.\n\n        If the model has multiple eos tokens, their probabilities will be summed.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of the context.\n        \"\"\"\n        context_ids = self.encode_tokens(context)\n        logp_context = await self._log_probability(context_ids)\n        logp_next = self._maybe_temper(\n            await self.model.next_token_logprobs(self.prompt_ids + context_ids)\n        )\n        logp_eos = torch.logsumexp(logp_next[self.token_maps.eos_idxs], dim=0).item()\n        return logp_context + logp_eos\n\n    def _process_logw_next(self, logw_next):\n        \"\"\"Process the log probabilities for the next tokens.\n\n        This function rearranges the log probabilities such that the end-of-sequence (EOS) token's log probability\n        is the sum of the log probabilities of `self.eos_tokens`.\n\n        Args:\n            logw_next (torch.tensor): The log probabilities for the next tokens.\n\n        Returns:\n            (LazyWeights): Processed log probabilities for the next tokens.\n        \"\"\"\n        # This is ugly, but it's useful for all potentials to adhere to the convention\n        # of keeping the EOS token at the end of the weights array.\n        logw_next = logw_next[: len(self.token_maps.decode)]\n        logw_next = logw_next.log_softmax(dim=0)\n        _logw_next = torch.full((len(self.vocab) + 1,), float('-inf'), dtype=logw_next.dtype, device=logw_next.device)\n        _logw_next[: len(self.vocab)] = logw_next[\n            ~torch.isin(torch.arange(len(logw_next)), torch.tensor(self.token_maps.eos_idxs))\n        ]\n        _logw_next[-1] = torch.logsumexp(logw_next[self.token_maps.eos_idxs], dim=0).item()\n        return self.make_lazy_weights(_logw_next.float().cpu().numpy())\n\n    async def logw_next(self, context):\n        \"\"\"Get log probabilities for next tokens given the prompt and `context`.\n\n        Args:\n            context (List[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (LazyWeights): Log probabilities for next tokens and EOS.\n        \"\"\"\n        logw_next = self._maybe_temper(\n            await self.model.next_token_logprobs(\n                self.prompt_ids + self.encode_tokens(context)\n            )\n        )\n        return self._process_logw_next(logw_next)\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"Get log probabilities for next tokens given the prompt and `context`, for a batch of contexts.\n\n        Args:\n            contexts (list[list[bytes]]): A list of sequences of bytes tokens.\n\n        Returns:\n            (List[LazyWeights]): Log probabilities for next tokens and EOS for each context.\n        \"\"\"\n        logw_nexts = self._maybe_temper(\n            await self.model.batch_next_token_logprobs(\n                [self.prompt_ids + self.encode_tokens(context) for context in contexts]\n            )\n        )\n        return [\n            self._process_logw_next(logw_next)\n            for logw_next in logw_nexts\n        ]\n\n    def __repr__(self):\n        return f\"PromptedLLM(prompt={self.prompt!r})\"\n\n    def spawn(self):\n        \"\"\"\n        Spawn a new PromptedLLM with the same prompt and eos tokens.\n\n        Returns:\n            (PromptedLLM): A new PromptedLLM with the same prompt and eos tokens.\n\n        Note:\n            This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.\n        \"\"\"\n        return PromptedLLM(\n            self.model,\n            prompt_ids=self.prompt_ids.copy(),\n            eos_tokens=self._eos_tokens.copy(),\n            temperature=self.temperature,\n        )\n\n    def spawn_new_eos(self, eos_tokens):\n        \"\"\"\n        Create a new PromptedLLM with a different set of end-of-sequence tokens.\n\n        Args:\n            eos_tokens (list[bytes]): A list of tokens to treat as end-of-sequence tokens.\n\n        Returns:\n            (PromptedLLM): A new PromptedLLM with the specified end-of-sequence tokens.\n                The new model will have the same prompt_ids as `self`.\n        \"\"\"\n        return PromptedLLM(\n            self.model,\n            prompt_ids=self.prompt_ids.copy(),\n            eos_tokens=eos_tokens.copy(),\n            temperature=self.temperature,\n        )\n\n    def to_autobatched(self):\n        raise ValueError(\"PromptedLLMs are autobatched by default.\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.__init__","title":"<code>__init__(llm, prompt_ids=None, eos_tokens=None, temperature=1)</code>","text":"<p>` Initializes the PromptedLLM potential.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>AsyncLM</code> <p>The language model to use.</p> required <code>prompt_ids</code> <code>list[int]</code> <p>Optional prompt to use as a prompt prefix for all input contexts. Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via <code>prompt</code> or <code>prompt_ids</code>.</p> <code>None</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens to treat as end-of-sequence tokens. Defaults to the EOS token of the language model's tokenizer.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to apply to the language model's logits. Defaults to 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any EOS token is not in the language model vocabulary.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def __init__(self, llm, prompt_ids=None, eos_tokens=None, temperature=1):\n    \"\"\"`\n    Initializes the PromptedLLM potential.\n\n    Args:\n        llm (AsyncLM): The language model to use.\n        prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n            Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `prompt` or `prompt_ids`.\n        eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n            Defaults to the EOS token of the language model's tokenizer.\n        temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n\n    Raises:\n        ValueError: If any EOS token is not in the language model vocabulary.\n    \"\"\"\n    self.model = llm\n    self.prompt_ids = prompt_ids or []\n\n    if not eos_tokens:\n        self._eos_tokens = [llm.byte_vocab[self.model.tokenizer.eos_token_id]]\n    else:\n        self._eos_tokens = eos_tokens\n\n    assert len(set(self._eos_tokens)) == len(self._eos_tokens), (\n        \"duplicate eos tokens\"\n    )\n\n    self.token_maps = TokenMappings.create(\n        decode=llm.byte_vocab, eos_tokens=self._eos_tokens\n    )\n\n    self.temperature = temperature\n\n    V = [x for x in self.token_maps.decode if x not in self._eos_tokens]\n\n    super().__init__(vocabulary=V)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.from_name","title":"<code>from_name(name, backend=None, eos_tokens=None, prompt_ids=None, temperature=1.0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>PromptedLLM</code> from a HugginFace model name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model to load</p> required <code>backend</code> <code>str</code> <p><code>AsyncLM</code> backend to use:</p> <ul> <li> <p>'vllm' to instantiate an <code>AsyncVirtualLM</code>; ideal for GPU usage</p> </li> <li> <p>'hf' for an <code>AsyncTransformer</code>; ideal for CPU usage</p> </li> <li> <p>'mock' for a <code>MockAsyncLM</code>; ideal for testing.</p> </li> </ul> <p>Defaults to 'vllm' if CUDA is available, otherwise 'hf'.</p> <code>None</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens to treat as end-of-sequence tokens. Defaults to the EOS token of the language model's tokenizer.</p> <code>None</code> <code>prompt_ids</code> <code>list[int]</code> <p>Optional prompt to use as a prompt prefix for all input contexts. Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via <code>set_prompt_from_str</code> or <code>prompt_ids</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to apply to the language model's logits. Defaults to 1.</p> <code>1.0</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to AsyncLM constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>An instance of PromptedLLM</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>@classmethod\ndef from_name(\n    cls,\n    name,\n    backend=None,\n    eos_tokens=None,\n    prompt_ids=None,\n    temperature=1.0,\n    **kwargs,\n):\n    \"\"\"Create a `PromptedLLM` from a HugginFace model name.\n\n    Args:\n        name (str): Name of the model to load\n        backend (str, optional): `AsyncLM` backend to use:\\n\n            * 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\\n\n            * 'hf' for an `AsyncTransformer`; ideal for CPU usage\\n\n            * 'mock' for a `MockAsyncLM`; ideal for testing.\\n\n            Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n        eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n            Defaults to the EOS token of the language model's tokenizer.\n        prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n            Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `set_prompt_from_str` or `prompt_ids`.\n        temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n        **kwargs (dict): Additional arguments passed to AsyncLM constructor\n\n    Returns:\n        (PromptedLLM): An instance of PromptedLLM\n    \"\"\"\n    backend = backend or (\"vllm\" if torch.cuda.is_available() else \"hf\")\n    model = load_model_by_name(name, backend=backend, **kwargs)\n    return cls(\n        model, prompt_ids=prompt_ids, eos_tokens=eos_tokens, temperature=temperature\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.prompt","title":"<code>prompt</code>  <code>property</code>","text":"<p>Get the current prompt as a list of byte sequences corresponding to the prompt token IDs.</p> <p>Returns:</p> Type Description <code>list[bytes] | None</code> <p>The current prompt as a list of bytes sequences or None if no prompt_ids are set.</p>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.set_prompt_from_str","title":"<code>set_prompt_from_str(prompt_str)</code>","text":"<p>Set the fixed prompt from a string.</p> <p>Modifies <code>prompt_ids</code> to be the token IDs of the input prompt according to the language model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_str</code> <code>str</code> <p>The prompt to set.</p> required Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def set_prompt_from_str(self, prompt_str):\n    \"\"\"Set the fixed prompt from a string.\n\n    Modifies `prompt_ids` to be the token IDs of the input prompt according to the language model's tokenizer.\n\n    Args:\n        prompt_str (str): The prompt to set.\n    \"\"\"\n    # TODO: Handle race condition where prompt_ids reset concurrently.\n    if not isinstance(prompt_str, str):\n        raise ValueError(\n            f\"Prompt must a string got {type(prompt_str)}. \"\n            f\"To set the prompt from a list of token IDs, use prompt_ids.\"\n        )\n\n    if prompt_str.endswith(\" \"):\n        warnings.warn(\n            \"Prompt ends with whitespace, which may affect tokenization. \"\n            \"Consider removing trailing whitespace.\",\n            stacklevel=2,\n        )\n\n    self.prompt_ids = self.model.tokenizer.encode(prompt_str)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.encode_tokens","title":"<code>encode_tokens(tokens)</code>","text":"<p>Encode a list of byte tokens to a list of token IDs in the underlying language model's vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[bytes]</code> <p>List of byte tokens to encode</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of token IDs corresponding to the input tokens.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any token is not in the vocabulary</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def encode_tokens(self, tokens):\n    \"\"\"Encode a list of byte tokens to a list of token IDs in\n    the underlying language model's vocabulary.\n\n    Args:\n        tokens (list[bytes]): List of byte tokens to encode\n\n    Returns:\n        (list[int]): A list of token IDs corresponding to the input tokens.\n\n    Raises:\n        ValueError: If any token is not in the vocabulary\n    \"\"\"\n    try:\n        return [self.token_maps.encode[x] for x in tokens]\n    except KeyError as e:\n        raise ValueError(f\"Token {e.args[0]} not in vocabulary\") from e\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.decode_tokens","title":"<code>decode_tokens(ids)</code>","text":"<p>Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[int]</code> <p>A list of token IDs in the language model's vocabulary.</p> required <p>Returns:</p> Type Description <code>list[bytes]</code> <p>A list of byte tokens corresponding to the input token IDs.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def decode_tokens(self, ids):\n    \"\"\"\n    Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.\n\n    Args:\n        ids (list[int]): A list of token IDs in the language model's vocabulary.\n\n    Returns:\n        (list[bytes]): A list of byte tokens corresponding to the input token IDs.\n    \"\"\"\n    return [self.token_maps.decode[x] for x in ids]\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.tokenize","title":"<code>tokenize(context_str)</code>","text":"<p>Tokenize a string to a list of <code>bytes</code> objects, each corresponding to a token in the vocabulary.</p> <p>Uses the language model's tokenizer to map <code>context_str</code> to a list of token IDs, and then decodes the token IDs to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>context_str</code> <code>str</code> <p>A string to encode</p> required <p>Returns:</p> Type Description <code>List[bytes]</code> <p>A list of byte tokens corresponding to the input string.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def tokenize(self, context_str):\n    \"\"\"Tokenize a string to a list of `bytes` objects, each corresponding to a token in the vocabulary.\n\n    Uses the language model's tokenizer to map `context_str` to a list of token IDs, and then decodes the token IDs to bytes.\n\n    Args:\n        context_str (str): A string to encode\n\n    Returns:\n        (List[bytes]): A list of byte tokens corresponding to the input string.\n    \"\"\"\n    return self.decode_tokens(self.model.tokenizer.encode(context_str))\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.log_probability","title":"<code>log_probability(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> given the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def log_probability(self, context):\n    \"\"\"\n    Compute the log probability of `context` given the prompt.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of `context`.\n    \"\"\"\n    if not context:\n        return 0\n\n    context_ids = self.encode_tokens(context)\n    return await self._log_probability(context_ids)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> given the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Compute the log probability of `context` given the prompt.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of `context`.\n    \"\"\"\n    return await self.log_probability(context)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> and the eos tokens given the prompt.</p> <p>If the model has multiple eos tokens, their probabilities will be summed.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of the context.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Compute the log probability of `context` and the eos tokens given the prompt.\n\n    If the model has multiple eos tokens, their probabilities will be summed.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of the context.\n    \"\"\"\n    context_ids = self.encode_tokens(context)\n    logp_context = await self._log_probability(context_ids)\n    logp_next = self._maybe_temper(\n        await self.model.next_token_logprobs(self.prompt_ids + context_ids)\n    )\n    logp_eos = torch.logsumexp(logp_next[self.token_maps.eos_idxs], dim=0).item()\n    return logp_context + logp_eos\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Get log probabilities for next tokens given the prompt and <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>List[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Log probabilities for next tokens and EOS.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Get log probabilities for next tokens given the prompt and `context`.\n\n    Args:\n        context (List[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (LazyWeights): Log probabilities for next tokens and EOS.\n    \"\"\"\n    logw_next = self._maybe_temper(\n        await self.model.next_token_logprobs(\n            self.prompt_ids + self.encode_tokens(context)\n        )\n    )\n    return self._process_logw_next(logw_next)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Get log probabilities for next tokens given the prompt and <code>context</code>, for a batch of contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list[list[bytes]]</code> <p>A list of sequences of bytes tokens.</p> required <p>Returns:</p> Type Description <code>List[LazyWeights]</code> <p>Log probabilities for next tokens and EOS for each context.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"Get log probabilities for next tokens given the prompt and `context`, for a batch of contexts.\n\n    Args:\n        contexts (list[list[bytes]]): A list of sequences of bytes tokens.\n\n    Returns:\n        (List[LazyWeights]): Log probabilities for next tokens and EOS for each context.\n    \"\"\"\n    logw_nexts = self._maybe_temper(\n        await self.model.batch_next_token_logprobs(\n            [self.prompt_ids + self.encode_tokens(context) for context in contexts]\n        )\n    )\n    return [\n        self._process_logw_next(logw_next)\n        for logw_next in logw_nexts\n    ]\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new PromptedLLM with the same prompt and eos tokens.</p> <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>A new PromptedLLM with the same prompt and eos tokens.</p> Note <p>This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def spawn(self):\n    \"\"\"\n    Spawn a new PromptedLLM with the same prompt and eos tokens.\n\n    Returns:\n        (PromptedLLM): A new PromptedLLM with the same prompt and eos tokens.\n\n    Note:\n        This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.\n    \"\"\"\n    return PromptedLLM(\n        self.model,\n        prompt_ids=self.prompt_ids.copy(),\n        eos_tokens=self._eos_tokens.copy(),\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.PromptedLLM.spawn_new_eos","title":"<code>spawn_new_eos(eos_tokens)</code>","text":"<p>Create a new PromptedLLM with a different set of end-of-sequence tokens.</p> <p>Parameters:</p> Name Type Description Default <code>eos_tokens</code> <code>list[bytes]</code> <p>A list of tokens to treat as end-of-sequence tokens.</p> required <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>A new PromptedLLM with the specified end-of-sequence tokens. The new model will have the same prompt_ids as <code>self</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def spawn_new_eos(self, eos_tokens):\n    \"\"\"\n    Create a new PromptedLLM with a different set of end-of-sequence tokens.\n\n    Args:\n        eos_tokens (list[bytes]): A list of tokens to treat as end-of-sequence tokens.\n\n    Returns:\n        (PromptedLLM): A new PromptedLLM with the specified end-of-sequence tokens.\n            The new model will have the same prompt_ids as `self`.\n    \"\"\"\n    return PromptedLLM(\n        self.model,\n        prompt_ids=self.prompt_ids.copy(),\n        eos_tokens=eos_tokens.copy(),\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WCFG","title":"<code>WCFG</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A weighted context-free grammar potential.</p> <p>This class wraps a <code>genlm_grammar.CFG</code> and provides methods for computing the log-weight of a sequence, the prefix log-weight of a sequence, and the log-weights of the next token given a sequence.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>class WCFG(Potential):\n    \"\"\"\n    A weighted context-free grammar potential.\n\n    This class wraps a `genlm_grammar.CFG` and provides methods for computing the log-weight of a sequence,\n    the prefix log-weight of a sequence, and the log-weights of the next token given a sequence.\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Initialize the WCFG potential.\n\n        Args:\n            cfg (genlm_grammar.CFG): The context-free grammar configuration to use.\n                The CFG must in the Float semiring.\n        \"\"\"\n        # TODO: convert to LogSemiring to handle underflow\n        if cfg.R is not Float:\n            raise ValueError(\"cfg semiring must be Float\")\n        self.cfg = cfg  # cfg before prefix transform\n        self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n        self.model = Earley(self.cfg_eos.prefix_grammar)\n        super().__init__(vocabulary=list(cfg.V))\n\n    @classmethod\n    def from_string(cls, grammar, to_bytes=True, **kwargs):\n        \"\"\"Create a WCFG from a string.\n\n        Args:\n            grammar (str): The string grammar specification to create the WCFG from.\n            to_bytes (bool, optional): Whether to convert the WCFG terminals to indivudual bytes.\n                Defaults to True.\n            **kwargs (dict): Additional arguments passed to the WCFG constructor.\n\n        Returns:\n            (WCFG): The created WCFG.\n        \"\"\"\n        cfg = CFG.from_string(grammar, Float)\n        if to_bytes:\n            cfg = cfg.to_bytes()\n        return cls(cfg, **kwargs)\n\n    async def complete(self, context):\n        \"\"\"\n        Compute the log weight of `context` under the WCFG.\n\n        For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\\n\n        - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (float): The log weight of `context` under the WCFG.\n        \"\"\"\n        w = self.model([*context, EOS])\n        return np.log(w) if w &gt; 0 else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Compute the log prefix weight of `context` under the WCFG.\n\n        This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n        For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n        - `prefix(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `prefix(\"d\")` returns $-\\\\infty$ since the WCFG does not accept any sequences with prefix \"d\"\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (float): The log prefix weight of `context` under the WCFG.\n        \"\"\"\n        w = self.model(context)\n        return np.log(w) if w &gt; 0 else float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute the next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (LazyWeights): The log weights for the next tokens and EOS given `context`.\n        \"\"\"\n        ws = self.model.next_token_weights(self.model.chart(context))\n        ws = ws.trim().normalize()\n\n        ws_array = np.array([ws[x] for x in self.vocab_eos])\n        mask = ws_array &gt; 0\n        log_ws = np.full_like(ws_array, float(\"-inf\"), dtype=np.float64)\n        log_ws[mask] = np.log(ws_array[mask])\n\n        return self.make_lazy_weights(log_ws)\n\n    def clear_cache(self):\n        \"\"\"Clear the internal cache of the parser.\"\"\"\n        self.model.clear_cache()\n\n    def __repr__(self):\n        return f\"WCFG(cfg={self.cfg!r})\"\n\n    def _repr_html_(self):\n        return self.cfg._repr_html_()\n\n    def spawn(self):\n        \"\"\"Spawn a new WCFG.\"\"\"\n        return WCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WCFG.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Initialize the WCFG potential.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CFG</code> <p>The context-free grammar configuration to use. The CFG must in the Float semiring.</p> required Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def __init__(self, cfg):\n    \"\"\"\n    Initialize the WCFG potential.\n\n    Args:\n        cfg (genlm_grammar.CFG): The context-free grammar configuration to use.\n            The CFG must in the Float semiring.\n    \"\"\"\n    # TODO: convert to LogSemiring to handle underflow\n    if cfg.R is not Float:\n        raise ValueError(\"cfg semiring must be Float\")\n    self.cfg = cfg  # cfg before prefix transform\n    self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n    self.model = Earley(self.cfg_eos.prefix_grammar)\n    super().__init__(vocabulary=list(cfg.V))\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WCFG.from_string","title":"<code>from_string(grammar, to_bytes=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a WCFG from a string.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The string grammar specification to create the WCFG from.</p> required <code>to_bytes</code> <code>bool</code> <p>Whether to convert the WCFG terminals to indivudual bytes. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the WCFG constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>WCFG</code> <p>The created WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>@classmethod\ndef from_string(cls, grammar, to_bytes=True, **kwargs):\n    \"\"\"Create a WCFG from a string.\n\n    Args:\n        grammar (str): The string grammar specification to create the WCFG from.\n        to_bytes (bool, optional): Whether to convert the WCFG terminals to indivudual bytes.\n            Defaults to True.\n        **kwargs (dict): Additional arguments passed to the WCFG constructor.\n\n    Returns:\n        (WCFG): The created WCFG.\n    \"\"\"\n    cfg = CFG.from_string(grammar, Float)\n    if to_bytes:\n        cfg = cfg.to_bytes()\n    return cls(cfg, **kwargs)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WCFG.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Compute the log weight of <code>context</code> under the WCFG.</p> <p>For example, if the WCFG accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>complete(\"c\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WCFG</p> </li> <li> <p><code>complete(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>complete(\"d\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WCFG</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log weight of <code>context</code> under the WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Compute the log weight of `context` under the WCFG.\n\n    For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\\n\n    - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (float): The log weight of `context` under the WCFG.\n    \"\"\"\n    w = self.model([*context, EOS])\n    return np.log(w) if w &gt; 0 else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WCFG.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Compute the log prefix weight of <code>context</code> under the WCFG.</p> <p>This corresponds to the log of the sum of the weights of all sequences with prefix <code>context</code>.</p> <p>For example, if the WCFG accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>prefix(\"c\")</code> returns \\(\\log(w_{cat} + w_{car})\\)</p> </li> <li> <p><code>prefix(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>prefix(\"d\")</code> returns \\(-\\infty\\) since the WCFG does not accept any sequences with prefix \"d\"</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log prefix weight of <code>context</code> under the WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Compute the log prefix weight of `context` under the WCFG.\n\n    This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n    For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n    - `prefix(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `prefix(\"d\")` returns $-\\\\infty$ since the WCFG does not accept any sequences with prefix \"d\"\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (float): The log prefix weight of `context` under the WCFG.\n    \"\"\"\n    w = self.model(context)\n    return np.log(w) if w &gt; 0 else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WCFG.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>The log weights for the next tokens and EOS given <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute the next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (LazyWeights): The log weights for the next tokens and EOS given `context`.\n    \"\"\"\n    ws = self.model.next_token_weights(self.model.chart(context))\n    ws = ws.trim().normalize()\n\n    ws_array = np.array([ws[x] for x in self.vocab_eos])\n    mask = ws_array &gt; 0\n    log_ws = np.full_like(ws_array, float(\"-inf\"), dtype=np.float64)\n    log_ws[mask] = np.log(ws_array[mask])\n\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WCFG.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the internal cache of the parser.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the internal cache of the parser.\"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WCFG.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def spawn(self):\n    \"\"\"Spawn a new WCFG.\"\"\"\n    return WCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolCFG","title":"<code>BoolCFG</code>","text":"<p>               Bases: <code>Potential</code></p> <p>BoolCFG represents a boolean context-free grammar.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>class BoolCFG(Potential):\n    \"\"\"BoolCFG represents a boolean context-free grammar.\"\"\"\n\n    def __init__(self, cfg):\n        if cfg.R != Boolean:\n            cfg = cfg.map_values(lambda x: Boolean(x &gt; 0), Boolean)\n        self.cfg = cfg  # cfg before prefix transform\n        self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n        self.model = Earley(self.cfg_eos.prefix_grammar)\n        super().__init__(vocabulary=list(cfg.V))\n\n    @classmethod\n    def from_lark(cls, lark_string, charset=\"core\"):\n        \"\"\"\n        Create a BoolCFG instance from a Lark grammar string.\n\n        The output grammar will be defined at the byte-level.\n\n        Args:\n            lark_string (str): The Lark grammar string to parse. See Lark documentation for correct syntax.\n            charset (str): The character set to use. Defaults to \"core\".\n                See `genlm-grammar` documentation for more details.\n\n        Returns:\n            (BoolCFG): An instance of BoolCFG created from the provided Lark grammar.\n        \"\"\"\n        byte_cfg = LarkStuff(lark_string).byte_cfg(charset=charset)\n        return cls(byte_cfg)\n\n    async def complete(self, context):\n        \"\"\"\n        Checks whether the context is accepted by the CFG.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (float): Log weight for whether `context` is accepted by the CFG.\n        \"\"\"\n        w = self.model([*context, EOS])\n        return 0 if w.score else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Checks whether `context` is accepted as a prefix by the CFG, i.e.,\n        whether there exists a completion to `context` that is accepted by the CFG.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (float): Log weight for whether `context` is accepted as a prefix by the CFG.\n        \"\"\"\n        if not context:  # FIX: this is a hack to handle the empty string because genlm-grammar doesn't support it\n            return 0\n        w = self.model(context)\n        return 0 if w.score else float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute the next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (LazyWeights): The log weights for the next tokens and EOS given `context`.\n        \"\"\"\n        ws = self.model.next_token_weights(self.model.chart(context))\n        log_ws = np.array([0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos])\n        return self.make_lazy_weights(log_ws)\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"\n        Batch version of `logw_next`.\n\n        Args:\n            contexts (list): A list of sequences of tokens in the CFG's alphabet.\n\n        Returns:\n            (list): A list of log-weights for next token, one per context.\n        \"\"\"\n        Ws = []\n        for context in contexts:\n            ws = self.model.next_token_weights(self.model.chart(context))\n            log_ws = np.array(\n                [0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos]\n            )\n            Ws.append(self.make_lazy_weights(log_ws))\n        return Ws\n\n    def spawn(self):\n        \"\"\"Spawn a new BoolCFG.\"\"\"\n        return BoolCFG(self.cfg)\n\n    def clear_cache(self):\n        \"\"\"Clear the internal cache of the parser.\"\"\"\n        self.model.clear_cache()\n\n    def __repr__(self):\n        return f\"BoolCFG(cfg={self.cfg!r})\"\n\n    def _repr_html_(self):\n        return self.cfg._repr_html_()\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolCFG.from_lark","title":"<code>from_lark(lark_string, charset='core')</code>  <code>classmethod</code>","text":"<p>Create a BoolCFG instance from a Lark grammar string.</p> <p>The output grammar will be defined at the byte-level.</p> <p>Parameters:</p> Name Type Description Default <code>lark_string</code> <code>str</code> <p>The Lark grammar string to parse. See Lark documentation for correct syntax.</p> required <code>charset</code> <code>str</code> <p>The character set to use. Defaults to \"core\". See <code>genlm-grammar</code> documentation for more details.</p> <code>'core'</code> <p>Returns:</p> Type Description <code>BoolCFG</code> <p>An instance of BoolCFG created from the provided Lark grammar.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>@classmethod\ndef from_lark(cls, lark_string, charset=\"core\"):\n    \"\"\"\n    Create a BoolCFG instance from a Lark grammar string.\n\n    The output grammar will be defined at the byte-level.\n\n    Args:\n        lark_string (str): The Lark grammar string to parse. See Lark documentation for correct syntax.\n        charset (str): The character set to use. Defaults to \"core\".\n            See `genlm-grammar` documentation for more details.\n\n    Returns:\n        (BoolCFG): An instance of BoolCFG created from the provided Lark grammar.\n    \"\"\"\n    byte_cfg = LarkStuff(lark_string).byte_cfg(charset=charset)\n    return cls(byte_cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolCFG.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Checks whether the context is accepted by the CFG.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight for whether <code>context</code> is accepted by the CFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Checks whether the context is accepted by the CFG.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (float): Log weight for whether `context` is accepted by the CFG.\n    \"\"\"\n    w = self.model([*context, EOS])\n    return 0 if w.score else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolCFG.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Checks whether <code>context</code> is accepted as a prefix by the CFG, i.e., whether there exists a completion to <code>context</code> that is accepted by the CFG.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight for whether <code>context</code> is accepted as a prefix by the CFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Checks whether `context` is accepted as a prefix by the CFG, i.e.,\n    whether there exists a completion to `context` that is accepted by the CFG.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (float): Log weight for whether `context` is accepted as a prefix by the CFG.\n    \"\"\"\n    if not context:  # FIX: this is a hack to handle the empty string because genlm-grammar doesn't support it\n        return 0\n    w = self.model(context)\n    return 0 if w.score else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolCFG.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>The log weights for the next tokens and EOS given <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute the next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (LazyWeights): The log weights for the next tokens and EOS given `context`.\n    \"\"\"\n    ws = self.model.next_token_weights(self.model.chart(context))\n    log_ws = np.array([0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos])\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolCFG.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Batch version of <code>logw_next</code>.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>A list of sequences of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of log-weights for next token, one per context.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"\n    Batch version of `logw_next`.\n\n    Args:\n        contexts (list): A list of sequences of tokens in the CFG's alphabet.\n\n    Returns:\n        (list): A list of log-weights for next token, one per context.\n    \"\"\"\n    Ws = []\n    for context in contexts:\n        ws = self.model.next_token_weights(self.model.chart(context))\n        log_ws = np.array(\n            [0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos]\n        )\n        Ws.append(self.make_lazy_weights(log_ws))\n    return Ws\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolCFG.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new BoolCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def spawn(self):\n    \"\"\"Spawn a new BoolCFG.\"\"\"\n    return BoolCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolCFG.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the internal cache of the parser.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the internal cache of the parser.\"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WFSA","title":"<code>WFSA</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A weighted finite state automaton (WFSA) potential.</p> <p>This class wraps a <code>genlm_grammar.WFSA</code> and provides methods for computing the log-weight of a context, the prefix log-weight of a context, and the log-weights of the next token given a context.</p> <p>Attributes:</p> Name Type Description <code>wfsa</code> <code>WFSA</code> <p>The weighted finite state automaton used for potential calculations.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>class WFSA(Potential):\n    \"\"\"\n    A weighted finite state automaton (WFSA) potential.\n\n    This class wraps a `genlm_grammar.WFSA` and provides methods for computing the log-weight of a context,\n    the prefix log-weight of a context, and the log-weights of the next token given a context.\n\n    Attributes:\n        wfsa (genlm_grammar.WFSA): The weighted finite state automaton used for potential calculations.\n    \"\"\"\n\n    def __init__(self, wfsa):\n        \"\"\"\n        Initializes the WFSA potential.\n\n        Args:\n            wfsa (genlm_grammar.WFSA): The weighted finite state automaton.\n\n        Raises:\n            ValueError: If the semiring of the provided WFSA is not Float or Log.\n\n        Note:\n            The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.\n        \"\"\"\n        if wfsa.R not in (Float, Log):\n            raise ValueError(f\"Unsupported semiring: {wfsa.R}\")\n\n        if wfsa.R is Float:\n            self.wfsa = self._convert_to_log(wfsa)\n        else:\n            self.wfsa = wfsa\n\n        self.cache = {(): self.wfsa.epsremove.start}\n        super().__init__(vocabulary=list(self.wfsa.alphabet))\n\n    @classmethod\n    def from_regex(cls, pattern, charset=None, to_bytes=True):\n        \"\"\"\n        Create a WFSA from a regex pattern.\n\n        Args:\n            pattern (str): The regex pattern to convert into a WFSA.\n            charset (set): The character set to use for negative character classes.\n                Defaults to characters in string.printable.\n            to_bytes (bool): Whether to convert the WFSA transitions to bytes.\n                Defaults to True. When set to False, the WFSA transitions will be strings.\n\n        Returns:\n            (WFSA): An instance of the WFSA class.\n\n        Note:\n            The transition weights are automatically normalized to form a probability distribution.\n            For each state, the weights of all outgoing transitions (including final state transitions)\n            sum to 1.0. This means if a state has n possible transitions, each transition will have\n            weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use `BoolFSA`.\n        \"\"\"\n        charset = charset or set(string.printable)\n        wfsa = interegular_to_wfsa(pattern, charset=charset)\n        if to_bytes:\n            wfsa = wfsa.to_bytes()\n        return cls(wfsa=wfsa)\n\n    @staticmethod\n    def _convert_to_log(wfsa):\n        \"\"\"Convert a WFSA from the Float semiring to the Log semiring.\"\"\"\n        assert wfsa.R is Float\n        assert isinstance(wfsa, BaseWFSA)\n        new = BaseWFSA(Log)\n\n        for i, w in wfsa.I:\n            new.add_I(i, Log(np.log(w)))\n\n        for i, w in wfsa.F:\n            new.add_F(i, Log(np.log(w)))\n\n        for i, a, j, w in wfsa.arcs():\n            new.add_arc(i, a, j, Log(np.log(w)))\n\n        return new\n\n    def _consume(self, bs):\n        # XXX implement cache eviction\n        bs = tuple(bs)\n\n        try:\n            return self.cache[bs]\n        except KeyError:\n            pass\n\n        wfsa = self.wfsa.epsremove\n        curr = wfsa.R.chart()\n        prev = self._consume(bs[:-1])\n        for i in prev:\n            for j, w in wfsa.arcs(i, bs[-1]):\n                curr[j] += prev[i] * w\n\n        self.cache[bs] = curr\n\n        return curr\n\n    async def complete(self, context):\n        \"\"\"\n        Computes the log weight of the context under the weighted language represented by the WFSA.\n\n        For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\\n\n        - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): Log weight of context under the WFSA.\n        \"\"\"\n        # TODO: optimize to use _consume cache\n        return self.wfsa(context).score\n\n    def _prefix(self, context):\n        curr = self._consume(context)\n\n        if not curr:\n            return float(\"-inf\"), curr\n\n        bkwd = self.wfsa.epsremove.backward\n        log_ctx_w = logsumexp([(curr[i] * bkwd[i]).score for i in curr])\n\n        if np.isnan(log_ctx_w):\n            return float(\"-inf\"), curr\n\n        return log_ctx_w, curr\n\n    async def prefix(self, context):\n        \"\"\"\n        Computes the prefix log weight of `context` under the WFSA.\n\n        This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n        For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n        - `prefix(\"ca\")` returns $\\\\log(w_{cat})$\\n\n        - `prefix(\"d\")` returns $-\\\\infty$ since the WFSA does not accept any sequences with prefix \"d\"\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): Log weight of `context` as a prefix under the WFSA.\n        \"\"\"\n        return self._prefix(context)[0]\n\n    async def logw_next(self, context):\n        \"\"\"Returns next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (LazyWeights): Log-weights for next token and EOS.\n        \"\"\"\n        log_ctx_w, curr = self._prefix(context)\n\n        if log_ctx_w == float(\"-inf\"):\n            raise ValueError(f\"Context {context!r} has zero weight.\")\n\n        bkwd = self.wfsa.epsremove.backward\n\n        ws = self.wfsa.R.chart()\n        for i in curr:\n            for b, j, w in self.wfsa.epsremove.arcs(i=i):\n                ws[b] += curr[i] * w * bkwd[j]\n\n        ws[self.eos] = self.wfsa.R.zero\n        for j, w in self.wfsa.epsremove.F:\n            ws[self.eos] += curr[j] * w\n\n        log_ws = np.array([ws[b].score for b in self.vocab_eos]) - log_ctx_w\n\n        return self.make_lazy_weights(log_ws)\n\n    def _repr_svg_(self):\n        return self.wfsa._repr_svg_()\n\n    def __repr__(self):\n        return f\"WFSA(wfsa={self.wfsa!r})\"\n\n    def spawn(self):\n        cls = type(self)\n        return cls(wfsa=self.wfsa)\n\n    def clear_cache(self):\n        self.cache = {(): self.wfsa.epsremove.start}\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WFSA.__init__","title":"<code>__init__(wfsa)</code>","text":"<p>Initializes the WFSA potential.</p> <p>Parameters:</p> Name Type Description Default <code>wfsa</code> <code>WFSA</code> <p>The weighted finite state automaton.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the semiring of the provided WFSA is not Float or Log.</p> Note <p>The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>def __init__(self, wfsa):\n    \"\"\"\n    Initializes the WFSA potential.\n\n    Args:\n        wfsa (genlm_grammar.WFSA): The weighted finite state automaton.\n\n    Raises:\n        ValueError: If the semiring of the provided WFSA is not Float or Log.\n\n    Note:\n        The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.\n    \"\"\"\n    if wfsa.R not in (Float, Log):\n        raise ValueError(f\"Unsupported semiring: {wfsa.R}\")\n\n    if wfsa.R is Float:\n        self.wfsa = self._convert_to_log(wfsa)\n    else:\n        self.wfsa = wfsa\n\n    self.cache = {(): self.wfsa.epsremove.start}\n    super().__init__(vocabulary=list(self.wfsa.alphabet))\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WFSA.from_regex","title":"<code>from_regex(pattern, charset=None, to_bytes=True)</code>  <code>classmethod</code>","text":"<p>Create a WFSA from a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regex pattern to convert into a WFSA.</p> required <code>charset</code> <code>set</code> <p>The character set to use for negative character classes. Defaults to characters in string.printable.</p> <code>None</code> <code>to_bytes</code> <code>bool</code> <p>Whether to convert the WFSA transitions to bytes. Defaults to True. When set to False, the WFSA transitions will be strings.</p> <code>True</code> <p>Returns:</p> Type Description <code>WFSA</code> <p>An instance of the WFSA class.</p> Note <p>The transition weights are automatically normalized to form a probability distribution. For each state, the weights of all outgoing transitions (including final state transitions) sum to 1.0. This means if a state has n possible transitions, each transition will have weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use <code>BoolFSA</code>.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>@classmethod\ndef from_regex(cls, pattern, charset=None, to_bytes=True):\n    \"\"\"\n    Create a WFSA from a regex pattern.\n\n    Args:\n        pattern (str): The regex pattern to convert into a WFSA.\n        charset (set): The character set to use for negative character classes.\n            Defaults to characters in string.printable.\n        to_bytes (bool): Whether to convert the WFSA transitions to bytes.\n            Defaults to True. When set to False, the WFSA transitions will be strings.\n\n    Returns:\n        (WFSA): An instance of the WFSA class.\n\n    Note:\n        The transition weights are automatically normalized to form a probability distribution.\n        For each state, the weights of all outgoing transitions (including final state transitions)\n        sum to 1.0. This means if a state has n possible transitions, each transition will have\n        weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use `BoolFSA`.\n    \"\"\"\n    charset = charset or set(string.printable)\n    wfsa = interegular_to_wfsa(pattern, charset=charset)\n    if to_bytes:\n        wfsa = wfsa.to_bytes()\n    return cls(wfsa=wfsa)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WFSA.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Computes the log weight of the context under the weighted language represented by the WFSA.</p> <p>For example, if the WFSA accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>complete(\"c\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WFSA</p> </li> <li> <p><code>complete(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>complete(\"d\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WFSA</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of context under the WFSA.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Computes the log weight of the context under the weighted language represented by the WFSA.\n\n    For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\\n\n    - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): Log weight of context under the WFSA.\n    \"\"\"\n    # TODO: optimize to use _consume cache\n    return self.wfsa(context).score\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WFSA.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Computes the prefix log weight of <code>context</code> under the WFSA.</p> <p>This corresponds to the log of the sum of the weights of all sequences with prefix <code>context</code>.</p> <p>For example, if the WFSA accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>prefix(\"c\")</code> returns \\(\\log(w_{cat} + w_{car})\\)</p> </li> <li> <p><code>prefix(\"ca\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>prefix(\"d\")</code> returns \\(-\\infty\\) since the WFSA does not accept any sequences with prefix \"d\"</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of <code>context</code> as a prefix under the WFSA.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Computes the prefix log weight of `context` under the WFSA.\n\n    This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n    For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n    - `prefix(\"ca\")` returns $\\\\log(w_{cat})$\\n\n    - `prefix(\"d\")` returns $-\\\\infty$ since the WFSA does not accept any sequences with prefix \"d\"\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): Log weight of `context` as a prefix under the WFSA.\n    \"\"\"\n    return self._prefix(context)[0]\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.WFSA.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Returns next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Log-weights for next token and EOS.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Returns next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (LazyWeights): Log-weights for next token and EOS.\n    \"\"\"\n    log_ctx_w, curr = self._prefix(context)\n\n    if log_ctx_w == float(\"-inf\"):\n        raise ValueError(f\"Context {context!r} has zero weight.\")\n\n    bkwd = self.wfsa.epsremove.backward\n\n    ws = self.wfsa.R.chart()\n    for i in curr:\n        for b, j, w in self.wfsa.epsremove.arcs(i=i):\n            ws[b] += curr[i] * w * bkwd[j]\n\n    ws[self.eos] = self.wfsa.R.zero\n    for j, w in self.wfsa.epsremove.F:\n        ws[self.eos] += curr[j] * w\n\n    log_ws = np.array([ws[b].score for b in self.vocab_eos]) - log_ctx_w\n\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolFSA","title":"<code>BoolFSA</code>","text":"<p>               Bases: <code>WFSA</code></p> <p>Boolean FSA potential.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>class BoolFSA(WFSA):\n    \"\"\"Boolean FSA potential.\"\"\"\n\n    async def prefix(self, context):\n        \"\"\"\n        Computes whether the context is accepted as a prefix by the FSA.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): `0` if the context is accepted as a prefix, `-inf` otherwise.\n        \"\"\"\n        prefix_w = await super().prefix(context)\n        if prefix_w &gt; float(\"-inf\"):\n            return 0\n        return float(\"-inf\")\n\n    async def complete(self, context):\n        \"\"\"\n        Computes whether the context is accepted by the FSA.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): `0` if the context is accepted, `-inf` otherwise.\n        \"\"\"\n        complete_w = await super().complete(context)\n        if complete_w &gt; float(\"-inf\"):\n            return 0\n        return float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Returns next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (LazyWeights): Boolean log-weights for next token.\n        \"\"\"\n        logw_next = await super().logw_next(context)\n        return logw_next.spawn(\n            new_weights=np.where(\n                logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n            )\n        )\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"\n        Returns next token log weights for a batch of contexts.\n\n        Args:\n            contexts (list): The list of contexts.\n\n        Returns:\n            (list): List of log-weights for next token, one per context.\n        \"\"\"\n        logw_nexts = await super().batch_logw_next(contexts)\n        return [\n            logw_next.spawn(\n                new_weights=np.where(\n                    logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n                )\n            )\n            for logw_next in logw_nexts\n        ]\n\n    def __repr__(self):\n        return f\"BoolFSA(wfsa={self.wfsa!r})\"\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolFSA.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Computes whether the context is accepted as a prefix by the FSA.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p><code>0</code> if the context is accepted as a prefix, <code>-inf</code> otherwise.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Computes whether the context is accepted as a prefix by the FSA.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): `0` if the context is accepted as a prefix, `-inf` otherwise.\n    \"\"\"\n    prefix_w = await super().prefix(context)\n    if prefix_w &gt; float(\"-inf\"):\n        return 0\n    return float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolFSA.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Computes whether the context is accepted by the FSA.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p><code>0</code> if the context is accepted, <code>-inf</code> otherwise.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Computes whether the context is accepted by the FSA.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): `0` if the context is accepted, `-inf` otherwise.\n    \"\"\"\n    complete_w = await super().complete(context)\n    if complete_w &gt; float(\"-inf\"):\n        return 0\n    return float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolFSA.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Returns next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Boolean log-weights for next token.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Returns next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (LazyWeights): Boolean log-weights for next token.\n    \"\"\"\n    logw_next = await super().logw_next(context)\n    return logw_next.spawn(\n        new_weights=np.where(\n            logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n        )\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.BoolFSA.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Returns next token log weights for a batch of contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>The list of contexts.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of log-weights for next token, one per context.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"\n    Returns next token log weights for a batch of contexts.\n\n    Args:\n        contexts (list): The list of contexts.\n\n    Returns:\n        (list): List of log-weights for next token, one per context.\n    \"\"\"\n    logw_nexts = await super().batch_logw_next(contexts)\n    return [\n        logw_next.spawn(\n            new_weights=np.where(\n                logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n            )\n        )\n        for logw_next in logw_nexts\n    ]\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.CanonicalTokenization","title":"<code>CanonicalTokenization</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A custom potential that enforces canonical BPE tokenization.</p> <p>This potential ensures that tokens follow the canonical tokenization rules by using the FastCanonicalityFilterBPE under the hood.</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>class CanonicalTokenization(Potential):\n    \"\"\"\n    A custom potential that enforces canonical BPE tokenization.\n\n    This potential ensures that tokens follow the canonical tokenization rules\n    by using the FastCanonicalityFilterBPE under the hood.\n    \"\"\"\n\n    def __init__(self, canonicality_filter):\n        \"\"\"\n        Initialize the Canonical Potential\n\n        Args:\n            canonicality_filter (FastCanonicalityFilterBPE): An initialized FastCanonicalityFilterBPE instance.\n        \"\"\"\n        # Store the pre-initialized filter and tokenizer\n        self.canonicality_filter = canonicality_filter\n\n        # IMPORTANT: In the base Potential class, EOS will be added to vocab automatically\n        # So we should NOT add it ourselves to the vocabulary we pass to super().__init__\n        vocabulary = self.canonicality_filter._decode\n        super().__init__(vocabulary)\n\n    @classmethod\n    def from_llm(cls, llm):\n        \"\"\"\n        Factory method to create CanonicalTokenization from a PromptedLLM instance.\n\n        Args:\n            llm (PromptedLLM): An instance of PromptedLLM containing the model and tokenizer.\n\n        Returns:\n            (CanonicalTokenization): An initialized CanonicalTokenization instance.\n        \"\"\"\n        if not isinstance(llm, PromptedLLM):\n            raise TypeError(\n                f\"Expected llm to be an instance of PromptedLLM, got {type(llm)}\"\n            )\n\n        # Extract necessary components from llm\n        tokenizer = llm.model.tokenizer\n        eos_token_ids = llm.token_maps.eos_idxs\n        model_name = tokenizer.name_or_path\n\n        # Create the filter using its factory method\n        canonicality_filter = FastCanonicalityFilterBPE.from_tokenizer(\n            tokenizer, eos_token_ids\n        )\n\n        # Set overrides on the filter\n        canonicality_filter.set_overrides(model_name)\n\n        # Call __init__ with the created filter and tokenizer\n        return cls(canonicality_filter)\n\n    async def complete(self, context):\n        \"\"\"\n        Assess if a complete sequence follows canonical tokenization.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (float): 0.0 if canonical, float('-inf') otherwise\n        \"\"\"\n        # Empty sequences are considered canonical\n        if not context:\n            return 0.0\n\n        # Check if the sequence is canonical\n        is_canonical = self._check_canonicality(context)\n        return 0.0 if is_canonical else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Assess if a prefix sequence could potentially extend to a canonical sequence.\n        For canonicality, this is the same as complete.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (float): 0.0 if potentially canonical, float('-inf') otherwise\n        \"\"\"\n        return await self.complete(context)\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute weights for each possible next token given the context.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (LazyWeights): Weights for each token in the vocabulary and EOS\n        \"\"\"\n        # Get the prefix weight (to check if context itself is canonical)\n        ctx_log_w = await self.prefix(context)\n\n        if ctx_log_w == float(\"-inf\"):\n            raise ValueError(\"Context is non-canonical\")\n        else:\n            if context:\n                t = (None, context[-1])\n                filter_mask = self.canonicality_filter(t)\n            else:\n                filter_mask = np.ones(len(self.canonicality_filter._decode), dtype=bool)\n\n            # Create log weights directly instead of using np.log(filter_mask)\n            # This is more efficient, avoids torch (with torch can't combine with other potentials!)\n            logws_no_eos = np.where(filter_mask, 0.0, float(\"-inf\")).astype(np.float32)\n\n            # append eos to the logws, always allow eos.\n            # NOTE: concat is because ._decode does not include eos while .vocab_eos does\n            logws = np.concatenate([logws_no_eos, np.array([0.0], dtype=np.float32)])\n\n        return self.make_lazy_weights(logws)\n\n    def _check_canonicality(self, context):\n        \"\"\"\n        Check if a sequence follows canonical tokenization.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (bool): True if the sequence is canonical, False otherwise\n        \"\"\"\n        # If we're checking a single token, it's always canonical\n        if len(context) &lt;= 1:\n            return True\n\n        # Check all adjacent token pairs for canonicality\n        for i in range(1, len(context)):\n            prev_token = context[i - 1]\n            current_token = context[i]\n\n            # Format expected by the filter: (None, previous_token)\n            t = (None, prev_token)\n            mask = self.canonicality_filter(t)\n            # print(\"percent of mask: \", np.sum(mask)*100 / len(mask))\n\n            # Find token_id in the canonicality filter's vocabulary\n            token_id = self.canonicality_filter._encode[current_token]\n            if not mask[token_id]:\n                return False\n\n        return True\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.CanonicalTokenization.__init__","title":"<code>__init__(canonicality_filter)</code>","text":"<p>Initialize the Canonical Potential</p> <p>Parameters:</p> Name Type Description Default <code>canonicality_filter</code> <code>FastCanonicalityFilterBPE</code> <p>An initialized FastCanonicalityFilterBPE instance.</p> required Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>def __init__(self, canonicality_filter):\n    \"\"\"\n    Initialize the Canonical Potential\n\n    Args:\n        canonicality_filter (FastCanonicalityFilterBPE): An initialized FastCanonicalityFilterBPE instance.\n    \"\"\"\n    # Store the pre-initialized filter and tokenizer\n    self.canonicality_filter = canonicality_filter\n\n    # IMPORTANT: In the base Potential class, EOS will be added to vocab automatically\n    # So we should NOT add it ourselves to the vocabulary we pass to super().__init__\n    vocabulary = self.canonicality_filter._decode\n    super().__init__(vocabulary)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.CanonicalTokenization.from_llm","title":"<code>from_llm(llm)</code>  <code>classmethod</code>","text":"<p>Factory method to create CanonicalTokenization from a PromptedLLM instance.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>PromptedLLM</code> <p>An instance of PromptedLLM containing the model and tokenizer.</p> required <p>Returns:</p> Type Description <code>CanonicalTokenization</code> <p>An initialized CanonicalTokenization instance.</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>@classmethod\ndef from_llm(cls, llm):\n    \"\"\"\n    Factory method to create CanonicalTokenization from a PromptedLLM instance.\n\n    Args:\n        llm (PromptedLLM): An instance of PromptedLLM containing the model and tokenizer.\n\n    Returns:\n        (CanonicalTokenization): An initialized CanonicalTokenization instance.\n    \"\"\"\n    if not isinstance(llm, PromptedLLM):\n        raise TypeError(\n            f\"Expected llm to be an instance of PromptedLLM, got {type(llm)}\"\n        )\n\n    # Extract necessary components from llm\n    tokenizer = llm.model.tokenizer\n    eos_token_ids = llm.token_maps.eos_idxs\n    model_name = tokenizer.name_or_path\n\n    # Create the filter using its factory method\n    canonicality_filter = FastCanonicalityFilterBPE.from_tokenizer(\n        tokenizer, eos_token_ids\n    )\n\n    # Set overrides on the filter\n    canonicality_filter.set_overrides(model_name)\n\n    # Call __init__ with the created filter and tokenizer\n    return cls(canonicality_filter)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.CanonicalTokenization.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Assess if a complete sequence follows canonical tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>float</code> <p>0.0 if canonical, float('-inf') otherwise</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Assess if a complete sequence follows canonical tokenization.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (float): 0.0 if canonical, float('-inf') otherwise\n    \"\"\"\n    # Empty sequences are considered canonical\n    if not context:\n        return 0.0\n\n    # Check if the sequence is canonical\n    is_canonical = self._check_canonicality(context)\n    return 0.0 if is_canonical else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.CanonicalTokenization.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Assess if a prefix sequence could potentially extend to a canonical sequence. For canonicality, this is the same as complete.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>float</code> <p>0.0 if potentially canonical, float('-inf') otherwise</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Assess if a prefix sequence could potentially extend to a canonical sequence.\n    For canonicality, this is the same as complete.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (float): 0.0 if potentially canonical, float('-inf') otherwise\n    \"\"\"\n    return await self.complete(context)\n</code></pre>"},{"location":"reference/genlm/control/potential/__init__/#genlm.control.potential.CanonicalTokenization.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute weights for each possible next token given the context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Weights for each token in the vocabulary and EOS</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute weights for each possible next token given the context.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (LazyWeights): Weights for each token in the vocabulary and EOS\n    \"\"\"\n    # Get the prefix weight (to check if context itself is canonical)\n    ctx_log_w = await self.prefix(context)\n\n    if ctx_log_w == float(\"-inf\"):\n        raise ValueError(\"Context is non-canonical\")\n    else:\n        if context:\n            t = (None, context[-1])\n            filter_mask = self.canonicality_filter(t)\n        else:\n            filter_mask = np.ones(len(self.canonicality_filter._decode), dtype=bool)\n\n        # Create log weights directly instead of using np.log(filter_mask)\n        # This is more efficient, avoids torch (with torch can't combine with other potentials!)\n        logws_no_eos = np.where(filter_mask, 0.0, float(\"-inf\")).astype(np.float32)\n\n        # append eos to the logws, always allow eos.\n        # NOTE: concat is because ._decode does not include eos while .vocab_eos does\n        logws = np.concatenate([logws_no_eos, np.array([0.0], dtype=np.float32)])\n\n    return self.make_lazy_weights(logws)\n</code></pre>"},{"location":"reference/genlm/control/potential/autobatch/","title":"autobatch","text":""},{"location":"reference/genlm/control/potential/autobatch/#genlm.control.potential.autobatch.AutoBatchedPotential","title":"<code>AutoBatchedPotential</code>","text":"<p>               Bases: <code>Potential</code></p> <p>AutoBatchedPotential is a wrapper around a Potential that enables automatic batching of concurrent requests.</p> <p>This class manages a background loop that collects concurrent requests to instance methods (<code>complete</code>, <code>prefix</code>, <code>score</code>, <code>logw_next</code>) and batches them together before delegating to the corresponding batch methods of the underlying potential (<code>batch_complete</code>, <code>batch_prefix</code>, <code>batch_score</code>, <code>batch_logw_next</code>).</p> <p>This class inherits all methods from <code>Potential</code>.</p> <p>Attributes:</p> Name Type Description <code>potential</code> <code>Potential</code> <p>The underlying potential instance that is being wrapped.</p> <code>background_loop</code> <code>AsyncBatchLoop</code> <p>An asynchronous loop that manages batch requests.</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>class AutoBatchedPotential(Potential):\n    \"\"\"\n    AutoBatchedPotential is a wrapper around a Potential that enables automatic batching of concurrent requests.\n\n    This class manages a background loop that collects concurrent requests to instance methods\n    (`complete`, `prefix`, `score`, `logw_next`) and batches them together before\n    delegating to the corresponding batch methods of the underlying potential\n    (`batch_complete`, `batch_prefix`, `batch_score`, `batch_logw_next`).\n\n    This class inherits all methods from [`Potential`][genlm.control.potential.base.Potential].\n\n    Attributes:\n        potential (Potential): The underlying potential instance that is being wrapped.\n        background_loop (AsyncBatchLoop): An asynchronous loop that manages batch requests.\n    \"\"\"\n\n    def __init__(self, potential):\n        self.potential = potential\n        self.background_loop = AsyncBatchLoop(potential)\n        self.background_loop.start()\n        super().__init__(potential.vocab)\n\n    async def complete(self, context):\n        return await self.background_loop.queue_request(\n            \"batch_complete\", lambda args: ([*args[0], context],)\n        )\n\n    async def prefix(self, context):\n        return await self.background_loop.queue_request(\n            \"batch_prefix\", lambda args: ([*args[0], context],)\n        )\n\n    async def score(self, context):\n        return await self.background_loop.queue_request(\n            \"batch_score\", lambda args: ([*args[0], context],)\n        )\n\n    async def logw_next(self, context):\n        return await self.background_loop.queue_request(\n            \"batch_logw_next\", lambda args: ([*args[0], context],)\n        )\n\n    async def batch_complete(self, contexts):\n        return await self.potential.batch_complete(contexts)\n\n    async def batch_prefix(self, contexts):\n        return await self.potential.batch_prefix(contexts)\n\n    async def batch_score(self, contexts):\n        return await self.potential.batch_score(contexts)\n\n    async def batch_logw_next(self, contexts):\n        return await self.potential.batch_logw_next(contexts)\n\n    def spawn(self, *args, **kwargs):\n        # creates a new background loop.\n        return AutoBatchedPotential(self.potential.spawn(*args, **kwargs))\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.potential!r})\"\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        await self.background_loop.cleanup()\n\n    def __del__(self):\n        if loop := getattr(self, \"background_loop\", None):\n            loop.close()\n</code></pre>"},{"location":"reference/genlm/control/potential/autobatch/#genlm.control.potential.autobatch.AutoBatchedPotential.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    await self.background_loop.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/potential/autobatch/#genlm.control.potential.autobatch.AsyncBatchLoop","title":"<code>AsyncBatchLoop</code>","text":"<p>Asynchronous batch processing loop for potential methods.</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>class AsyncBatchLoop:\n    \"\"\"Asynchronous batch processing loop for potential methods.\"\"\"\n\n    def __init__(self, potential, history=None):\n        self.potential = potential\n        self.q = asyncio.Queue()\n        self.task = None\n        self.history = history\n\n    def start(self):\n        \"\"\"Start the background processing task.\"\"\"\n        self.task = asyncio.create_task(self._background_loop())\n\n    def queue_request(self, batch_method_name, arg_accumulator):\n        \"\"\"Queue a request for batch processing.\"\"\"\n        future = asyncio.Future()\n        self.q.put_nowait(Request(batch_method_name, arg_accumulator, future))\n        return future\n\n    async def _background_loop(self):\n        \"\"\"Background task that processes queued requests.\"\"\"\n        while True:\n            try:\n                method_groups = defaultdict(list)\n                req = await self.q.get()\n                method_groups[req.batch_method_name].append(req)\n\n                try:\n                    while True:\n                        req = self.q.get_nowait()\n                        method_groups[req.batch_method_name].append(req)\n                except asyncio.QueueEmpty:\n                    pass\n\n                for method_name, requests in method_groups.items():\n                    try:\n                        batch_args = ([],)\n                        for req in requests:\n                            batch_args = req.args_accumulator(batch_args)\n\n                        results = await getattr(self.potential, method_name)(\n                            *batch_args\n                        )\n\n                        assert len(results) == len(requests)\n                        for i, req in enumerate(requests):\n                            req.future.set_result(results[i])\n\n                    except Exception as e:\n                        for req in requests:\n                            if not req.future.done():\n                                req.future.set_exception(e)\n\n            except asyncio.CancelledError:\n                break\n\n    def close(self):\n        \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n        if task := getattr(self, \"task\", None):\n            try:\n                task.cancel()\n            except RuntimeError:  # pragma: no cover\n                pass  # pragma: no cover\n            self.task = None\n\n    async def cleanup(self):\n        \"\"\"Async cleanup - preferred method\"\"\"\n        if self.task and not self.task.done():\n            self.task.cancel()\n            try:\n                await self.task\n            except asyncio.CancelledError:\n                pass\n            self.task = None\n\n    def __del__(self):\n        self.close()\n</code></pre>"},{"location":"reference/genlm/control/potential/autobatch/#genlm.control.potential.autobatch.AsyncBatchLoop.start","title":"<code>start()</code>","text":"<p>Start the background processing task.</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>def start(self):\n    \"\"\"Start the background processing task.\"\"\"\n    self.task = asyncio.create_task(self._background_loop())\n</code></pre>"},{"location":"reference/genlm/control/potential/autobatch/#genlm.control.potential.autobatch.AsyncBatchLoop.queue_request","title":"<code>queue_request(batch_method_name, arg_accumulator)</code>","text":"<p>Queue a request for batch processing.</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>def queue_request(self, batch_method_name, arg_accumulator):\n    \"\"\"Queue a request for batch processing.\"\"\"\n    future = asyncio.Future()\n    self.q.put_nowait(Request(batch_method_name, arg_accumulator, future))\n    return future\n</code></pre>"},{"location":"reference/genlm/control/potential/autobatch/#genlm.control.potential.autobatch.AsyncBatchLoop.close","title":"<code>close()</code>","text":"<p>Stop the background processing task and cleanup resources.</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>def close(self):\n    \"\"\"Stop the background processing task and cleanup resources.\"\"\"\n    if task := getattr(self, \"task\", None):\n        try:\n            task.cancel()\n        except RuntimeError:  # pragma: no cover\n            pass  # pragma: no cover\n        self.task = None\n</code></pre>"},{"location":"reference/genlm/control/potential/autobatch/#genlm.control.potential.autobatch.AsyncBatchLoop.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Async cleanup - preferred method</p> Source code in <code>genlm/control/potential/autobatch.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Async cleanup - preferred method\"\"\"\n    if self.task and not self.task.done():\n        self.task.cancel()\n        try:\n            await self.task\n        except asyncio.CancelledError:\n            pass\n        self.task = None\n</code></pre>"},{"location":"reference/genlm/control/potential/base/","title":"base","text":""},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential","title":"<code>Potential</code>","text":"<p>               Bases: <code>ABC</code>, <code>PotentialOps</code>, <code>PotentialTests</code></p> <p>Abstract base class for potentials.</p> <p>A Potential is a function that maps sequences of tokens in a vocabulary to non-negative real numbers (weights).</p> <p>Potentials assign weights to sequences of tokens based on whether they are complete sequences or prefixes of complete sequences.</p> <ul> <li><code>complete</code>: Assess the log weight of a sequence of tokens in the vocabulary as a complete sequence.</li> <li><code>prefix</code>: Assess the log weight of a sequence of tokens in the vocabulary as a prefix.</li> </ul> <p>Potentials additionally implement a <code>logw_next</code> method:</p> <ul> <li><code>logw_next</code>: Compute the next-token log weights of each token in the vocabulary and a special EOS (end-of-sequence) token given a context.</li> </ul> <p>Subclasses must minimally implement <code>complete</code> and <code>prefix</code>. <code>logw_next</code> and batched versions of the above methods come with default implementations, but may be overridden by subclasses for improved performance.</p> <p>All Potentials must satisfy a set of properties which can be tested using PotentialTests.</p> <p>Attributes:</p> Name Type Description <code>token_type</code> <code>TokenType</code> <p>The type of tokens in the vocabulary.</p> <code>vocab</code> <code>list</code> <p>List of tokens making up the vocabulary.</p> <code>eos</code> <code>EndOfSequence</code> <p>Special token to use as end-of-sequence.</p> <code>vocab_eos</code> <code>list</code> <p>List of tokens in <code>vocab</code> and <code>eos</code>. <code>eos</code> is assumed to be the last token in <code>vocab_eos</code>.</p> <code>lookup</code> <code>dict</code> <p>Mapping from tokens and <code>eos</code> to their indices in <code>vocab_eos</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>class Potential(ABC, PotentialOps, PotentialTests):\n    \"\"\"Abstract base class for potentials.\n\n    A Potential is a function that maps sequences of tokens in a vocabulary to non-negative real numbers (weights).\n\n    Potentials assign weights to sequences of tokens based on whether they are complete sequences or prefixes of complete sequences.\n\n    - `complete`: Assess the log weight of a sequence of tokens in the vocabulary as a complete sequence.\n    - `prefix`: Assess the log weight of a sequence of tokens in the vocabulary as a prefix.\n\n    Potentials additionally implement a `logw_next` method:\n\n    - `logw_next`: Compute the next-token log weights of each token in the vocabulary and a special EOS (end-of-sequence) token given a context.\n\n    Subclasses must minimally implement `complete` and `prefix`. `logw_next` and batched versions of the above methods\n    come with default implementations, but may be overridden by subclasses for improved performance.\n\n    All Potentials must satisfy a set of properties which can be tested using [PotentialTests][genlm.control.potential.testing.PotentialTests].\n\n    Attributes:\n        token_type (TokenType): The type of tokens in the vocabulary.\n        vocab (list): List of tokens making up the vocabulary.\n        eos (EndOfSequence): Special token to use as end-of-sequence.\n        vocab_eos (list): List of tokens in `vocab` and `eos`. `eos` is assumed to be the last token in `vocab_eos`.\n        lookup (dict): Mapping from tokens and `eos` to their indices in `vocab_eos`.\n    \"\"\"\n\n    def __init__(self, vocabulary, token_type=None, eos=None):\n        \"\"\"\n        Initialize the potential.\n\n        Args:\n            vocabulary (list): List of tokens that make up the vocabulary.\n            token_type (TokenType, optional): Optional TokenType of all elements of the vocabulary.\n                If None, will be inferred from vocabulary.\n            eos (EndOfSequence, optional): Special token to use as end-of-sequence. Defaults to `EOS`.\n                In general, this should not be set by users.\n\n        Raises:\n            ValueError: If vocabulary is empty.\n            TypeError: If vocabulary contains tokens which are not of `token_type`.\n        \"\"\"\n        if not vocabulary:\n            raise ValueError(\"vocabulary cannot be empty\")\n\n        if token_type is None:\n            token_type = infer_vocabulary_type(vocabulary)\n        elif not isinstance(token_type, TokenType):\n            raise ValueError(f\"token_type must be a TokenType, got {token_type!r}.\")\n\n        if not all(token_type.check(x) for x in vocabulary):\n            raise TypeError(f\"Tokens in vocabulary must be of type {token_type}.\")\n\n        if eos and not isinstance(eos, EndOfSequence):\n            raise ValueError(f\"EOS must be an instance of EndOfSequence, got {eos!r}.\")\n\n        self.eos = eos or EOS\n\n        self.token_type = token_type\n        self.vocab = vocabulary\n        self.vocab_eos = self.vocab + [self.eos]\n        self.lookup = {}\n        for i, x in enumerate(vocabulary):\n            if x in self.lookup:\n                raise ValueError(f\"Duplicate token {x!r} found in vocabulary\")\n            self.lookup[x] = i\n        self.lookup[self.eos] = len(self.vocab)\n\n    ####################\n    # Instance methods #\n    ####################\n\n    @abstractmethod\n    async def complete(self, context):\n        \"\"\"Assess the weight of `context` as a complete sequence.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context under the language.\n        \"\"\"\n        pass  # pragma: no cover\n\n    @abstractmethod\n    async def prefix(self, context):\n        \"\"\"Assess the weight of `context` as a prefix.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context as a prefix.\n        \"\"\"\n        pass  # pragma: no cover\n\n    async def score(self, context):\n        \"\"\"Assess the weight of `context` based on EOS-termination.\n\n        This is a convenience method which dispatches to `complete` if `context` ends with `self.eos`, otherwise to `prefix`.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (float): Log weight of the context, either as a prefix or complete sequence.\n        \"\"\"\n        if context and context[-1] == self.eos:\n            return await self.complete(context[:-1])\n        else:\n            return await self.prefix(context)\n\n    async def logw_next(self, context):\n        \"\"\"Compute the next-token weights of each token in `self.vocab_eos` given `context`.\n\n        Args:\n            context (list): Sequence of tokens.\n\n        Returns:\n            (LazyWeights): Weights of each token in the vocabulary and EOS.\n        \"\"\"\n        ctx_log_w = await self.prefix(context)\n\n        if ctx_log_w == float(\"-inf\"):\n            raise ValueError(f\"Context {context!r} has weight zero under `prefix`.\")\n\n        scores = await self.batch_score([[*context, x] for x in self.vocab_eos])\n        logws = scores - ctx_log_w\n\n        return self.make_lazy_weights(logws)\n\n    ###################\n    # Batched methods #\n    ###################\n\n    async def batch_complete(self, contexts):\n        \"\"\"Batched equivalent to `complete`.\n\n        Assess the weight of each context as a complete sequence.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return np.array(\n            await asyncio.gather(*[self.complete(context) for context in contexts])\n        )\n\n    async def batch_prefix(self, contexts):\n        \"\"\"Batched equivalent to `prefix`.\n\n        Assess the weight of each context as a prefix.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return np.array(\n            await asyncio.gather(*[self.prefix(context) for context in contexts])\n        )\n\n    async def batch_score(self, contexts):\n        \"\"\"Batched equivalent to `score`.\n\n        Assess the weight of each context based on EOS-termination.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (np.array): Array of log weights for each context.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        complete, prefix = [], []\n        complete_indices, prefix_indices = [], []\n\n        for i, context in enumerate(contexts):\n            # We want == here instead of `is`.\n            if context and context[-1] == self.eos:\n                complete.append(context[:-1])\n                complete_indices.append(i)\n            else:\n                prefix.append(context)\n                prefix_indices.append(i)\n\n        complete_scores = (\n            await self.batch_complete(complete) if complete else np.array([])\n        )\n        prefix_scores = await self.batch_prefix(prefix) if prefix else np.array([])\n\n        results = np.empty(len(contexts))\n        if len(complete_scores) &gt; 0:\n            results[complete_indices] = complete_scores\n        if len(prefix_scores) &gt; 0:\n            results[prefix_indices] = prefix_scores\n\n        return results\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"Batched equivalent to `logw_next`.\n\n        Computes the next-token weights of each token in `self.vocab_eos` given each context in the batch.\n\n        Args:\n            contexts (list): List of sequences of tokens.\n\n        Returns:\n            (list): List of LazyWeights objects, one for each context.\n\n        Raises:\n            ValueError: If any context has zero weight (log weight of -inf) under `prefix`.\n        \"\"\"\n        if not contexts:\n            raise ValueError(\"Contexts must be non-empty.\")\n\n        return await asyncio.gather(*[self.logw_next(context) for context in contexts])\n\n    #############\n    # Utilities #\n    #############\n\n    def make_lazy_weights(self, weights, log=True):\n        \"\"\"Helper method to create a LazyWeights object over the potential's vocabulary and EOS.\n\n        Args:\n            weights (np.array): Array of weights.\n            log (bool, optional): Whether the weights are in log space. Defaults to True.\n\n        Returns:\n            (LazyWeights): LazyWeights object defined over `self.vocab_eos`.\n        \"\"\"\n        return LazyWeights(\n            weights=weights, encode=self.lookup, decode=self.vocab_eos, log=log\n        )\n\n    def alloc_logws(self, default=float(\"-inf\")):\n        \"\"\"Allocate a new array of log weights for the potential's vocabulary and EOS.\n\n        Args:\n            default (float, optional): Default log weight. Defaults to -inf.\n\n        Returns:\n            (np.array): Array of length `len(self.vocab_eos)` filled with `default`.\n        \"\"\"\n        return np.full((len(self.vocab_eos),), default)\n\n    def spawn(self):\n        \"\"\"\n        Spawn a fresh instance of the potential.\n\n        This method is not required by default, but may be implemented by subclasses\n        to support CPU-parallelism using (`MultiProcPotential`)[genlm.control.potential.multi_proc.MultiProcPotential].\n        \"\"\"\n        raise NotImplementedError(\n            \"Potential.spawn() must be implemented by subclasses.\"\n        )\n\n    async def cleanup(self):\n        \"\"\"\n        Cleanup the potential.\n\n        This method may be implemented by subclasses to release resources.\n        \"\"\"\n        pass\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.__init__","title":"<code>__init__(vocabulary, token_type=None, eos=None)</code>","text":"<p>Initialize the potential.</p> <p>Parameters:</p> Name Type Description Default <code>vocabulary</code> <code>list</code> <p>List of tokens that make up the vocabulary.</p> required <code>token_type</code> <code>TokenType</code> <p>Optional TokenType of all elements of the vocabulary. If None, will be inferred from vocabulary.</p> <code>None</code> <code>eos</code> <code>EndOfSequence</code> <p>Special token to use as end-of-sequence. Defaults to <code>EOS</code>. In general, this should not be set by users.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If vocabulary is empty.</p> <code>TypeError</code> <p>If vocabulary contains tokens which are not of <code>token_type</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def __init__(self, vocabulary, token_type=None, eos=None):\n    \"\"\"\n    Initialize the potential.\n\n    Args:\n        vocabulary (list): List of tokens that make up the vocabulary.\n        token_type (TokenType, optional): Optional TokenType of all elements of the vocabulary.\n            If None, will be inferred from vocabulary.\n        eos (EndOfSequence, optional): Special token to use as end-of-sequence. Defaults to `EOS`.\n            In general, this should not be set by users.\n\n    Raises:\n        ValueError: If vocabulary is empty.\n        TypeError: If vocabulary contains tokens which are not of `token_type`.\n    \"\"\"\n    if not vocabulary:\n        raise ValueError(\"vocabulary cannot be empty\")\n\n    if token_type is None:\n        token_type = infer_vocabulary_type(vocabulary)\n    elif not isinstance(token_type, TokenType):\n        raise ValueError(f\"token_type must be a TokenType, got {token_type!r}.\")\n\n    if not all(token_type.check(x) for x in vocabulary):\n        raise TypeError(f\"Tokens in vocabulary must be of type {token_type}.\")\n\n    if eos and not isinstance(eos, EndOfSequence):\n        raise ValueError(f\"EOS must be an instance of EndOfSequence, got {eos!r}.\")\n\n    self.eos = eos or EOS\n\n    self.token_type = token_type\n    self.vocab = vocabulary\n    self.vocab_eos = self.vocab + [self.eos]\n    self.lookup = {}\n    for i, x in enumerate(vocabulary):\n        if x in self.lookup:\n            raise ValueError(f\"Duplicate token {x!r} found in vocabulary\")\n        self.lookup[x] = i\n    self.lookup[self.eos] = len(self.vocab)\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.complete","title":"<code>complete(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Assess the weight of <code>context</code> as a complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context under the language.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>@abstractmethod\nasync def complete(self, context):\n    \"\"\"Assess the weight of `context` as a complete sequence.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context under the language.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.prefix","title":"<code>prefix(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Assess the weight of <code>context</code> as a prefix.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context as a prefix.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>@abstractmethod\nasync def prefix(self, context):\n    \"\"\"Assess the weight of `context` as a prefix.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context as a prefix.\n    \"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.score","title":"<code>score(context)</code>  <code>async</code>","text":"<p>Assess the weight of <code>context</code> based on EOS-termination.</p> <p>This is a convenience method which dispatches to <code>complete</code> if <code>context</code> ends with <code>self.eos</code>, otherwise to <code>prefix</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of the context, either as a prefix or complete sequence.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def score(self, context):\n    \"\"\"Assess the weight of `context` based on EOS-termination.\n\n    This is a convenience method which dispatches to `complete` if `context` ends with `self.eos`, otherwise to `prefix`.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (float): Log weight of the context, either as a prefix or complete sequence.\n    \"\"\"\n    if context and context[-1] == self.eos:\n        return await self.complete(context[:-1])\n    else:\n        return await self.prefix(context)\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next-token weights of each token in <code>self.vocab_eos</code> given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Weights of each token in the vocabulary and EOS.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Compute the next-token weights of each token in `self.vocab_eos` given `context`.\n\n    Args:\n        context (list): Sequence of tokens.\n\n    Returns:\n        (LazyWeights): Weights of each token in the vocabulary and EOS.\n    \"\"\"\n    ctx_log_w = await self.prefix(context)\n\n    if ctx_log_w == float(\"-inf\"):\n        raise ValueError(f\"Context {context!r} has weight zero under `prefix`.\")\n\n    scores = await self.batch_score([[*context, x] for x in self.vocab_eos])\n    logws = scores - ctx_log_w\n\n    return self.make_lazy_weights(logws)\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.batch_complete","title":"<code>batch_complete(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>complete</code>.</p> <p>Assess the weight of each context as a complete sequence.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_complete(self, contexts):\n    \"\"\"Batched equivalent to `complete`.\n\n    Assess the weight of each context as a complete sequence.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return np.array(\n        await asyncio.gather(*[self.complete(context) for context in contexts])\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.batch_prefix","title":"<code>batch_prefix(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>prefix</code>.</p> <p>Assess the weight of each context as a prefix.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_prefix(self, contexts):\n    \"\"\"Batched equivalent to `prefix`.\n\n    Assess the weight of each context as a prefix.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return np.array(\n        await asyncio.gather(*[self.prefix(context) for context in contexts])\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.batch_score","title":"<code>batch_score(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>score</code>.</p> <p>Assess the weight of each context based on EOS-termination.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>array</code> <p>Array of log weights for each context.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_score(self, contexts):\n    \"\"\"Batched equivalent to `score`.\n\n    Assess the weight of each context based on EOS-termination.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (np.array): Array of log weights for each context.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    complete, prefix = [], []\n    complete_indices, prefix_indices = [], []\n\n    for i, context in enumerate(contexts):\n        # We want == here instead of `is`.\n        if context and context[-1] == self.eos:\n            complete.append(context[:-1])\n            complete_indices.append(i)\n        else:\n            prefix.append(context)\n            prefix_indices.append(i)\n\n    complete_scores = (\n        await self.batch_complete(complete) if complete else np.array([])\n    )\n    prefix_scores = await self.batch_prefix(prefix) if prefix else np.array([])\n\n    results = np.empty(len(contexts))\n    if len(complete_scores) &gt; 0:\n        results[complete_indices] = complete_scores\n    if len(prefix_scores) &gt; 0:\n        results[prefix_indices] = prefix_scores\n\n    return results\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Batched equivalent to <code>logw_next</code>.</p> <p>Computes the next-token weights of each token in <code>self.vocab_eos</code> given each context in the batch.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of sequences of tokens.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of LazyWeights objects, one for each context.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any context has zero weight (log weight of -inf) under <code>prefix</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"Batched equivalent to `logw_next`.\n\n    Computes the next-token weights of each token in `self.vocab_eos` given each context in the batch.\n\n    Args:\n        contexts (list): List of sequences of tokens.\n\n    Returns:\n        (list): List of LazyWeights objects, one for each context.\n\n    Raises:\n        ValueError: If any context has zero weight (log weight of -inf) under `prefix`.\n    \"\"\"\n    if not contexts:\n        raise ValueError(\"Contexts must be non-empty.\")\n\n    return await asyncio.gather(*[self.logw_next(context) for context in contexts])\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.make_lazy_weights","title":"<code>make_lazy_weights(weights, log=True)</code>","text":"<p>Helper method to create a LazyWeights object over the potential's vocabulary and EOS.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>array</code> <p>Array of weights.</p> required <code>log</code> <code>bool</code> <p>Whether the weights are in log space. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>LazyWeights</code> <p>LazyWeights object defined over <code>self.vocab_eos</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def make_lazy_weights(self, weights, log=True):\n    \"\"\"Helper method to create a LazyWeights object over the potential's vocabulary and EOS.\n\n    Args:\n        weights (np.array): Array of weights.\n        log (bool, optional): Whether the weights are in log space. Defaults to True.\n\n    Returns:\n        (LazyWeights): LazyWeights object defined over `self.vocab_eos`.\n    \"\"\"\n    return LazyWeights(\n        weights=weights, encode=self.lookup, decode=self.vocab_eos, log=log\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.alloc_logws","title":"<code>alloc_logws(default=float('-inf'))</code>","text":"<p>Allocate a new array of log weights for the potential's vocabulary and EOS.</p> <p>Parameters:</p> Name Type Description Default <code>default</code> <code>float</code> <p>Default log weight. Defaults to -inf.</p> <code>float('-inf')</code> <p>Returns:</p> Type Description <code>array</code> <p>Array of length <code>len(self.vocab_eos)</code> filled with <code>default</code>.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def alloc_logws(self, default=float(\"-inf\")):\n    \"\"\"Allocate a new array of log weights for the potential's vocabulary and EOS.\n\n    Args:\n        default (float, optional): Default log weight. Defaults to -inf.\n\n    Returns:\n        (np.array): Array of length `len(self.vocab_eos)` filled with `default`.\n    \"\"\"\n    return np.full((len(self.vocab_eos),), default)\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a fresh instance of the potential.</p> <p>This method is not required by default, but may be implemented by subclasses to support CPU-parallelism using (<code>MultiProcPotential</code>)[genlm.control.potential.multi_proc.MultiProcPotential].</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>def spawn(self):\n    \"\"\"\n    Spawn a fresh instance of the potential.\n\n    This method is not required by default, but may be implemented by subclasses\n    to support CPU-parallelism using (`MultiProcPotential`)[genlm.control.potential.multi_proc.MultiProcPotential].\n    \"\"\"\n    raise NotImplementedError(\n        \"Potential.spawn() must be implemented by subclasses.\"\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/base/#genlm.control.potential.base.Potential.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleanup the potential.</p> <p>This method may be implemented by subclasses to release resources.</p> Source code in <code>genlm/control/potential/base.py</code> <pre><code>async def cleanup(self):\n    \"\"\"\n    Cleanup the potential.\n\n    This method may be implemented by subclasses to release resources.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"reference/genlm/control/potential/coerce/","title":"coerce","text":""},{"location":"reference/genlm/control/potential/coerce/#genlm.control.potential.coerce.Coerced","title":"<code>Coerced</code>","text":"<p>               Bases: <code>Potential</code></p> <p>Coerce a potential to operate on another vocabulary.</p> <p>This class allows a potential to be adapted to work with a different set of tokens, defined by a target vocabulary and coersion function.</p> <p>This class inherits all methods from <code>Potential</code>. Each method delegates to the corresponding method of the underlying potential, but first maps any input token sequences from the target vocabulary to the original potential's vocabulary using the coercion function.</p> <p>Formally, if \\(f\\) is the coercion function, then for any sequence \\(x_1, \\ldots, x_n\\) of tokens from the target vocabulary, $$ \\textsf{Coerced.prefix}(x_1, \\ldots, x_n) = \\textsf{Coerced.potential.prefix}(f(x_1, \\ldots, x_n)) $$</p> \\[ \\textsf{Coerced.complete}(x_1, \\ldots, x_n) = \\textsf{Coerced.potential.complete}(f(x_1, \\ldots, x_n)) \\] <p>Attributes:</p> Name Type Description <code>potential</code> <code>Potential</code> <p>The original potential instance that is being coerced.</p> <code>f</code> <code>callable</code> <p>A function that maps sequences of tokens from the target vocabulary to sequences of tokens from the original potential's vocabulary.</p> Note <p>The coerced potential's vocabulary will by default be pruned to only include tokens that can be mapped to the original potential's vocabulary via the coercion function (i.e. <code>set(f([x])) &lt;= set(potential.vocab)</code>). If no such tokens are found, a <code>ValueError</code> is raised. This behavior can be overridden by setting <code>prune=False</code>, in which case the coerced potential's vocabulary will include all tokens from the target vocabulary.</p> Source code in <code>genlm/control/potential/coerce.py</code> <pre><code>class Coerced(Potential):\n    \"\"\"\n    Coerce a potential to operate on another vocabulary.\n\n    This class allows a potential to be adapted to work with a different set of tokens,\n    defined by a target vocabulary and coersion function.\n\n    This class inherits all methods from [`Potential`][genlm.control.potential.base.Potential].\n    Each method delegates to the corresponding method of the underlying potential, but first\n    maps any input token sequences from the target vocabulary to the original potential's vocabulary\n    using the coercion function.\n\n    Formally, if $f$ is the coercion function, then for any sequence $x_1, \\\\ldots, x_n$ of tokens from the target vocabulary,\n    $$\n    \\\\textsf{Coerced.prefix}(x_1, \\\\ldots, x_n) = \\\\textsf{Coerced.potential.prefix}(f(x_1, \\\\ldots, x_n))\n    $$\n\n    $$\n    \\\\textsf{Coerced.complete}(x_1, \\\\ldots, x_n) = \\\\textsf{Coerced.potential.complete}(f(x_1, \\\\ldots, x_n))\n    $$\n\n    Attributes:\n        potential (Potential): The original potential instance that is being coerced.\n        f (callable): A function that maps sequences of tokens from the target vocabulary to sequences of tokens from\n            the original potential's vocabulary.\n\n    Note:\n        The coerced potential's vocabulary will by default be pruned to only include tokens that can be mapped to the original potential's vocabulary\n        via the coercion function (i.e. `set(f([x])) &lt;= set(potential.vocab)`). If no such tokens are found, a `ValueError` is raised.\n        This behavior can be overridden by setting `prune=False`, in which case the coerced potential's vocabulary will include all tokens from the target vocabulary.\n    \"\"\"\n\n    def __init__(self, potential, target_vocab, f, prune=True):\n        \"\"\"\n        Initialize a Coerced potential.\n\n        Args:\n            potential (Potential): The original potential instance that is being coerced.\n            target_vocab (list): The target vocabulary that the potential will operate on.\n                Each element of `target_vocab` must be hashable.\n            f (callable): A function that maps iterables of tokens from the target vocabulary\n                to the original potential's vocabulary.\n            prune (bool): Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary.\n                If `False`, the coerced potential's vocabulary will include all tokens from the target vocabulary.\n\n        Raises:\n            ValueError: If no valid tokens are found in the target vocabulary that can be mapped to the original potential's vocabulary.\n        \"\"\"\n        self.potential = potential\n        self.f = f\n\n        if prune:\n            tokens = []\n            for target_token in target_vocab:\n                base_token = f([target_token])\n                if set(base_token) &lt;= set(potential.vocab):\n                    tokens.append(target_token)\n        else:\n            tokens = target_vocab\n\n        if not tokens:\n            raise ValueError(\"No valid tokens found in target vocabulary\")\n\n        super().__init__(tokens)\n\n    def _batch_f(self, contexts):\n        return [self.f(context) for context in contexts]\n\n    async def complete(self, context):\n        return await self.potential.complete(context=self.f(context))\n\n    async def prefix(self, context):\n        return await self.potential.prefix(context=self.f(context))\n\n    async def logw_next(self, context):\n        Ws = self.alloc_logws()\n        ctx = self.f(context)\n        ctx_w = await self.potential.prefix(ctx)\n        Ws[-1] = await self.potential.complete(ctx) - ctx_w\n        exts = [self.f(chain(context, [x])) for x in self.vocab]  # slow!!\n        Ws[:-1] = await self.potential.batch_prefix(exts) - ctx_w\n        return self.make_lazy_weights(Ws)\n\n    async def batch_complete(self, contexts):\n        return await self.potential.batch_complete(contexts=self._batch_f(contexts))\n\n    async def batch_prefix(self, contexts):\n        return await self.potential.batch_prefix(contexts=self._batch_f(contexts))\n\n    async def batch_logw_next(self, contexts):\n        return await asyncio.gather(*[self.logw_next(context) for context in contexts])\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.potential!r})\"\n</code></pre>"},{"location":"reference/genlm/control/potential/coerce/#genlm.control.potential.coerce.Coerced.__init__","title":"<code>__init__(potential, target_vocab, f, prune=True)</code>","text":"<p>Initialize a Coerced potential.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The original potential instance that is being coerced.</p> required <code>target_vocab</code> <code>list</code> <p>The target vocabulary that the potential will operate on. Each element of <code>target_vocab</code> must be hashable.</p> required <code>f</code> <code>callable</code> <p>A function that maps iterables of tokens from the target vocabulary to the original potential's vocabulary.</p> required <code>prune</code> <code>bool</code> <p>Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary. If <code>False</code>, the coerced potential's vocabulary will include all tokens from the target vocabulary.</p> <code>True</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no valid tokens are found in the target vocabulary that can be mapped to the original potential's vocabulary.</p> Source code in <code>genlm/control/potential/coerce.py</code> <pre><code>def __init__(self, potential, target_vocab, f, prune=True):\n    \"\"\"\n    Initialize a Coerced potential.\n\n    Args:\n        potential (Potential): The original potential instance that is being coerced.\n        target_vocab (list): The target vocabulary that the potential will operate on.\n            Each element of `target_vocab` must be hashable.\n        f (callable): A function that maps iterables of tokens from the target vocabulary\n            to the original potential's vocabulary.\n        prune (bool): Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary.\n            If `False`, the coerced potential's vocabulary will include all tokens from the target vocabulary.\n\n    Raises:\n        ValueError: If no valid tokens are found in the target vocabulary that can be mapped to the original potential's vocabulary.\n    \"\"\"\n    self.potential = potential\n    self.f = f\n\n    if prune:\n        tokens = []\n        for target_token in target_vocab:\n            base_token = f([target_token])\n            if set(base_token) &lt;= set(potential.vocab):\n                tokens.append(target_token)\n    else:\n        tokens = target_vocab\n\n    if not tokens:\n        raise ValueError(\"No valid tokens found in target vocabulary\")\n\n    super().__init__(tokens)\n</code></pre>"},{"location":"reference/genlm/control/potential/multi_proc/","title":"multi_proc","text":""},{"location":"reference/genlm/control/potential/multi_proc/#genlm.control.potential.multi_proc.MultiProcPotential","title":"<code>MultiProcPotential</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A Potential that adds parallel processing capabilities to any base Potential implementation.</p> <p>Creates a process pool of worker processes, each containing an instance of the potential.</p> <p>This class inherits all methods from <code>Potential</code>. Each method delegates to a corresponding method of the potential instances running in the worker processes, distributing work across multiple processes for improved performance.</p> Source code in <code>genlm/control/potential/multi_proc.py</code> <pre><code>class MultiProcPotential(Potential):\n    \"\"\"A Potential that adds parallel processing capabilities to any base Potential implementation.\n\n    Creates a process pool of worker processes, each containing an instance of the potential.\n\n    This class inherits all methods from [`Potential`][genlm.control.potential.base.Potential].\n    Each method delegates to a corresponding method of the potential instances running in the\n    worker processes, distributing work across multiple processes for improved performance.\n    \"\"\"\n\n    def __init__(self, potential_factory, factory_args, num_workers=2):\n        \"\"\"\n        Initialize the MultiProcPotential.\n\n        Args:\n            potential_factory (callable): A factory function that creates a potential instance.\n            factory_args (tuple): Arguments to pass to the potential factory.\n            num_workers (int): The number of worker processes to spawn. Each will contain an instance of the potential.\n        \"\"\"\n        self.num_workers = num_workers\n        self.executor = ProcessPoolExecutor(\n            max_workers=num_workers,\n            initializer=self._init_worker,\n            initargs=(potential_factory, factory_args),\n        )\n        # Get vocab and eos from one of the workers\n        vocab, eos = self.executor.submit(self._get_vocab_and_eos).result()\n        super().__init__(vocab, eos=eos)\n\n    @staticmethod\n    def _init_worker(factory, args):\n        global _worker_potential, _worker_event_loop\n        _worker_potential = factory(*args)\n        _worker_event_loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(_worker_event_loop)\n\n    @staticmethod\n    def _get_vocab_and_eos():\n        return _worker_potential.vocab, _worker_potential.eos\n\n    @staticmethod\n    def _run_coroutine(coroutine):\n        global _worker_event_loop\n        return _worker_event_loop.run_until_complete(coroutine)\n\n    @staticmethod\n    def _worker_logw_next(context):\n        return MultiProcPotential._run_coroutine(\n            _worker_potential.logw_next(context)\n        ).weights\n\n    @staticmethod\n    def _worker_prefix(context):\n        return MultiProcPotential._run_coroutine(_worker_potential.prefix(context))\n\n    @staticmethod\n    def _worker_complete(context):\n        return MultiProcPotential._run_coroutine(_worker_potential.complete(context))\n\n    # @staticmethod\n    # def _worker_score(context):\n    #    return MultiProcPotential._run_coroutine(_worker_potential.score(context))\n\n    async def _run_in_executor(self, func, *args):\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(self.executor, func, *args)\n\n    async def logw_next(self, context):\n        result = await self._run_in_executor(self._worker_logw_next, context)\n        return self.make_lazy_weights(result)\n\n    async def prefix(self, context):\n        return await self._run_in_executor(self._worker_prefix, context)\n\n    async def complete(self, context):\n        return await self._run_in_executor(self._worker_complete, context)\n\n    async def batch_logw_next(self, contexts):\n        results = await asyncio.gather(\n            *(\n                self._run_in_executor(self._worker_logw_next, context)\n                for context in contexts\n            )\n        )\n        return [self.make_lazy_weights(result) for result in results]\n\n    async def batch_complete(self, contexts):\n        results = await asyncio.gather(\n            *(\n                self._run_in_executor(self._worker_complete, context)\n                for context in contexts\n            )\n        )\n        return np.array(results)\n\n    async def batch_prefix(self, contexts):\n        results = await asyncio.gather(\n            *(\n                self._run_in_executor(self._worker_prefix, context)\n                for context in contexts\n            )\n        )\n        return np.array(results)\n\n    def __del__(self):\n        if self.executor is not None:\n            self.executor.shutdown()\n            self.executor = None\n\n    def __repr__(self):\n        return f\"{self.__class__.__name__}({self.num_workers=})\"\n\n    def spawn(self):\n        raise ValueError(\"MultiProcPotentials are not spawnable.\")\n</code></pre>"},{"location":"reference/genlm/control/potential/multi_proc/#genlm.control.potential.multi_proc.MultiProcPotential.__init__","title":"<code>__init__(potential_factory, factory_args, num_workers=2)</code>","text":"<p>Initialize the MultiProcPotential.</p> <p>Parameters:</p> Name Type Description Default <code>potential_factory</code> <code>callable</code> <p>A factory function that creates a potential instance.</p> required <code>factory_args</code> <code>tuple</code> <p>Arguments to pass to the potential factory.</p> required <code>num_workers</code> <code>int</code> <p>The number of worker processes to spawn. Each will contain an instance of the potential.</p> <code>2</code> Source code in <code>genlm/control/potential/multi_proc.py</code> <pre><code>def __init__(self, potential_factory, factory_args, num_workers=2):\n    \"\"\"\n    Initialize the MultiProcPotential.\n\n    Args:\n        potential_factory (callable): A factory function that creates a potential instance.\n        factory_args (tuple): Arguments to pass to the potential factory.\n        num_workers (int): The number of worker processes to spawn. Each will contain an instance of the potential.\n    \"\"\"\n    self.num_workers = num_workers\n    self.executor = ProcessPoolExecutor(\n        max_workers=num_workers,\n        initializer=self._init_worker,\n        initargs=(potential_factory, factory_args),\n    )\n    # Get vocab and eos from one of the workers\n    vocab, eos = self.executor.submit(self._get_vocab_and_eos).result()\n    super().__init__(vocab, eos=eos)\n</code></pre>"},{"location":"reference/genlm/control/potential/operators/","title":"operators","text":""},{"location":"reference/genlm/control/potential/operators/#genlm.control.potential.operators.PotentialOps","title":"<code>PotentialOps</code>","text":"<p>Mixin providing operations for potential functions:</p> <ol> <li> <p>Product (<code>*</code>): Take the product of two potentials.</p> </li> <li> <p>Coercion (<code>coerce</code>): Coerce the potential to operate on another potential's vocabulary.</p> </li> <li> <p>Auto-batching (<code>to_autobatched</code>): Create a version that automatically batches concurrent requests to the instance methods.</p> </li> <li> <p>Parallelization (<code>to_multiprocess</code>): Create a version that parallelizes operations over multiple processes.</p> </li> </ol> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>class PotentialOps:\n    \"\"\"Mixin providing operations for potential functions:\n\n    1. Product (`*`): Take the product of two potentials.\\n\n    2. Coercion (`coerce`): Coerce the potential to operate on another potential's vocabulary.\\n\n    3. Auto-batching (`to_autobatched`): Create a version that automatically batches concurrent requests to the instance methods.\\n\n    4. Parallelization (`to_multiprocess`): Create a version that parallelizes operations over multiple processes.\\n\n    \"\"\"\n\n    def __mul__(self, other):\n        \"\"\"Take the product of two potentials.\n\n        See [`Product`][genlm.control.potential.product.Product] for more details.\n\n        Args:\n            other (Potential): Another potential instance to take the product with.\n\n        Returns:\n            (Product): A Product instance representing the unnormalized product of the two potentials.\n\n        Note:\n            Potentials must operate on the same token type and the intersection of their vocabularies must be non-empty.\n        \"\"\"\n        from genlm.control.potential.product import Product\n\n        return Product(self, other)\n\n    def coerce(self, other, f, prune=True):\n        \"\"\"Coerce the current potential to operate on the vocabulary of another potential.\n\n        See [`Coerced`][genlm.control.potential.coerce.Coerced] for more details.\n\n        Args:\n            other (Potential): The potential instance whose vocabulary will be used.\n            f (callable): A function mapping sequences of tokens from self's vocab to sequences of tokens from other's vocab.\n            prune (bool): Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary.\n                If `False`, the coerced potential's vocabulary will include all tokens from the target vocabulary.\n\n        Returns:\n            (Coerced): A Potential that operates on the vocabulary of `other`.\n        \"\"\"\n        from genlm.control.potential.coerce import Coerced\n\n        return Coerced(self, other.vocab, f=f, prune=prune)\n\n    def to_autobatched(self):\n        \"\"\"Create a new potential instance that automatically batches concurrent requests to the instance methods.\n\n        See [`AutoBatchedPotential`][genlm.control.potential.autobatch.AutoBatchedPotential] for more details.\n\n        Returns:\n            (AutoBatchedPotential): A new potential instance that wraps the current potential and automatically batches concurrent requests to the instance methods.\n        \"\"\"\n        from genlm.control.potential.autobatch import AutoBatchedPotential\n\n        return AutoBatchedPotential(self)\n\n    def to_multiprocess(self, num_workers=2, spawn_args=None):\n        \"\"\"Create a new potential instance that parallelizes operations using multiprocessing.\n\n        See [`MultiProcPotential`][genlm.control.potential.multi_proc.MultiProcPotential] for more details.\n\n        Args:\n            num_workers (int): The number of workers to use in the multiprocessing pool.\n            spawn_args (tuple): The positional arguments to pass to the potential's `spawn` method.\n\n        Returns:\n            (MultiProcPotential): A new potential instance that wraps the current potential and uses multiprocessing to parallelize operations.\n\n        Note:\n            For this method to be used, the potential must implement a picklable `spawn` method.\n        \"\"\"\n        from genlm.control.potential.multi_proc import MultiProcPotential\n\n        factory_args = spawn_args or ()\n        return MultiProcPotential(\n            potential_factory=self.spawn,\n            factory_args=factory_args,\n            num_workers=num_workers,\n        )\n</code></pre>"},{"location":"reference/genlm/control/potential/operators/#genlm.control.potential.operators.PotentialOps.__mul__","title":"<code>__mul__(other)</code>","text":"<p>Take the product of two potentials.</p> <p>See <code>Product</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Potential</code> <p>Another potential instance to take the product with.</p> required <p>Returns:</p> Type Description <code>Product</code> <p>A Product instance representing the unnormalized product of the two potentials.</p> Note <p>Potentials must operate on the same token type and the intersection of their vocabularies must be non-empty.</p> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>def __mul__(self, other):\n    \"\"\"Take the product of two potentials.\n\n    See [`Product`][genlm.control.potential.product.Product] for more details.\n\n    Args:\n        other (Potential): Another potential instance to take the product with.\n\n    Returns:\n        (Product): A Product instance representing the unnormalized product of the two potentials.\n\n    Note:\n        Potentials must operate on the same token type and the intersection of their vocabularies must be non-empty.\n    \"\"\"\n    from genlm.control.potential.product import Product\n\n    return Product(self, other)\n</code></pre>"},{"location":"reference/genlm/control/potential/operators/#genlm.control.potential.operators.PotentialOps.coerce","title":"<code>coerce(other, f, prune=True)</code>","text":"<p>Coerce the current potential to operate on the vocabulary of another potential.</p> <p>See <code>Coerced</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>other</code> <code>Potential</code> <p>The potential instance whose vocabulary will be used.</p> required <code>f</code> <code>callable</code> <p>A function mapping sequences of tokens from self's vocab to sequences of tokens from other's vocab.</p> required <code>prune</code> <code>bool</code> <p>Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary. If <code>False</code>, the coerced potential's vocabulary will include all tokens from the target vocabulary.</p> <code>True</code> <p>Returns:</p> Type Description <code>Coerced</code> <p>A Potential that operates on the vocabulary of <code>other</code>.</p> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>def coerce(self, other, f, prune=True):\n    \"\"\"Coerce the current potential to operate on the vocabulary of another potential.\n\n    See [`Coerced`][genlm.control.potential.coerce.Coerced] for more details.\n\n    Args:\n        other (Potential): The potential instance whose vocabulary will be used.\n        f (callable): A function mapping sequences of tokens from self's vocab to sequences of tokens from other's vocab.\n        prune (bool): Whether to prune the coerced potential's vocabulary to only include tokens that can be mapped to the original potential's vocabulary.\n            If `False`, the coerced potential's vocabulary will include all tokens from the target vocabulary.\n\n    Returns:\n        (Coerced): A Potential that operates on the vocabulary of `other`.\n    \"\"\"\n    from genlm.control.potential.coerce import Coerced\n\n    return Coerced(self, other.vocab, f=f, prune=prune)\n</code></pre>"},{"location":"reference/genlm/control/potential/operators/#genlm.control.potential.operators.PotentialOps.to_autobatched","title":"<code>to_autobatched()</code>","text":"<p>Create a new potential instance that automatically batches concurrent requests to the instance methods.</p> <p>See <code>AutoBatchedPotential</code> for more details.</p> <p>Returns:</p> Type Description <code>AutoBatchedPotential</code> <p>A new potential instance that wraps the current potential and automatically batches concurrent requests to the instance methods.</p> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>def to_autobatched(self):\n    \"\"\"Create a new potential instance that automatically batches concurrent requests to the instance methods.\n\n    See [`AutoBatchedPotential`][genlm.control.potential.autobatch.AutoBatchedPotential] for more details.\n\n    Returns:\n        (AutoBatchedPotential): A new potential instance that wraps the current potential and automatically batches concurrent requests to the instance methods.\n    \"\"\"\n    from genlm.control.potential.autobatch import AutoBatchedPotential\n\n    return AutoBatchedPotential(self)\n</code></pre>"},{"location":"reference/genlm/control/potential/operators/#genlm.control.potential.operators.PotentialOps.to_multiprocess","title":"<code>to_multiprocess(num_workers=2, spawn_args=None)</code>","text":"<p>Create a new potential instance that parallelizes operations using multiprocessing.</p> <p>See <code>MultiProcPotential</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>num_workers</code> <code>int</code> <p>The number of workers to use in the multiprocessing pool.</p> <code>2</code> <code>spawn_args</code> <code>tuple</code> <p>The positional arguments to pass to the potential's <code>spawn</code> method.</p> <code>None</code> <p>Returns:</p> Type Description <code>MultiProcPotential</code> <p>A new potential instance that wraps the current potential and uses multiprocessing to parallelize operations.</p> Note <p>For this method to be used, the potential must implement a picklable <code>spawn</code> method.</p> Source code in <code>genlm/control/potential/operators.py</code> <pre><code>def to_multiprocess(self, num_workers=2, spawn_args=None):\n    \"\"\"Create a new potential instance that parallelizes operations using multiprocessing.\n\n    See [`MultiProcPotential`][genlm.control.potential.multi_proc.MultiProcPotential] for more details.\n\n    Args:\n        num_workers (int): The number of workers to use in the multiprocessing pool.\n        spawn_args (tuple): The positional arguments to pass to the potential's `spawn` method.\n\n    Returns:\n        (MultiProcPotential): A new potential instance that wraps the current potential and uses multiprocessing to parallelize operations.\n\n    Note:\n        For this method to be used, the potential must implement a picklable `spawn` method.\n    \"\"\"\n    from genlm.control.potential.multi_proc import MultiProcPotential\n\n    factory_args = spawn_args or ()\n    return MultiProcPotential(\n        potential_factory=self.spawn,\n        factory_args=factory_args,\n        num_workers=num_workers,\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/product/","title":"product","text":""},{"location":"reference/genlm/control/potential/product/#genlm.control.potential.product.Product","title":"<code>Product</code>","text":"<p>               Bases: <code>Potential</code></p> <p>Combine two potential instances via element-wise multiplication (sum in log space).</p> <p>This class creates a new potential that is the element-wise product of two potentials: <pre><code>prefix(xs) = p1.prefix(xs) + p2.prefix(xs)\ncomplete(xs) = p1.complete(xs) + p2.complete(xs)\nlogw_next(x | xs) = p1.logw_next(x | xs) + p2.logw_next(x | xs)\n</code></pre></p> <p>The new potential's vocabulary is the intersection of the two potentials' vocabularies.</p> <p>This class inherits all methods from <code>Potential</code>, see there for method documentation.</p> <p>Attributes:</p> Name Type Description <code>p1</code> <code>Potential</code> <p>The first potential instance.</p> <code>p2</code> <code>Potential</code> <p>The second potential instance.</p> <code>token_type</code> <code>str</code> <p>The type of tokens that this product potential operates on.</p> <code>vocab</code> <code>list</code> <p>The common vocabulary shared between the two potentials.</p> Warning <p>Be careful when taking products of potentials with minimal vocabulary overlap. The resulting potential will only operate on tokens present in both vocabularies.</p> Source code in <code>genlm/control/potential/product.py</code> <pre><code>class Product(Potential):\n    \"\"\"\n    Combine two potential instances via element-wise multiplication (sum in log space).\n\n    This class creates a new potential that is the element-wise product of two potentials:\n    ```\n    prefix(xs) = p1.prefix(xs) + p2.prefix(xs)\n    complete(xs) = p1.complete(xs) + p2.complete(xs)\n    logw_next(x | xs) = p1.logw_next(x | xs) + p2.logw_next(x | xs)\n    ```\n\n    The new potential's vocabulary is the intersection of the two potentials' vocabularies.\n\n    This class inherits all methods from [`Potential`][genlm.control.potential.base.Potential],\n    see there for method documentation.\n\n    Attributes:\n        p1 (Potential): The first potential instance.\n        p2 (Potential): The second potential instance.\n        token_type (str): The type of tokens that this product potential operates on.\n        vocab (list): The common vocabulary shared between the two potentials.\n\n    Warning:\n        Be careful when taking products of potentials with minimal vocabulary overlap.\n        The resulting potential will only operate on tokens present in both vocabularies.\n    \"\"\"\n\n    def __init__(self, p1, p2):\n        \"\"\"Initialize a Product potential.\n\n        Args:\n            p1 (Potential): First potential\n            p2 (Potential): Second potential\n        \"\"\"\n        self.p1 = p1\n        self.p2 = p2\n\n        if self.p1.token_type == self.p2.token_type:\n            self.token_type = self.p1.token_type\n        else:\n            raise ValueError(\n                \"Potentials in product must have the same token type. \"\n                f\"Got {self.p1.token_type} and {self.p2.token_type}.\"\n                + (\n                    \"\\nMaybe you forgot to coerce the potentials to the same token type? See `Coerce`.\"\n                    if (\n                        self.p1.token_type.is_iterable_of(self.p2.token_type)\n                        or self.p2.token_type.is_iterable_of(self.p1.token_type)\n                    )\n                    else \"\"\n                )\n            )\n\n        common_vocab = list(set(p1.vocab) &amp; set(p2.vocab))\n        if not common_vocab:\n            raise ValueError(\"Potentials in product must share a common vocabulary\")\n\n        # Check for small vocabulary overlap\n        threshold = 0.1\n        for potential, name in [(p1, \"p1\"), (p2, \"p2\")]:\n            overlap_ratio = len(common_vocab) / len(potential.vocab)\n            if overlap_ratio &lt; threshold:\n                warnings.warn(\n                    f\"Common vocabulary ({len(common_vocab)} tokens) is less than {threshold * 100}% \"\n                    f\"of {name}'s ({potential!r}) vocabulary ({len(potential.vocab)} tokens). \"\n                    \"This Product potential only operates on this relatively small subset of tokens.\",\n                    RuntimeWarning,\n                )\n\n        super().__init__(common_vocab, token_type=self.token_type)\n\n        # For fast products of weights\n        self.v1_idxs = [p1.lookup[token] for token in self.vocab_eos]\n        self.v2_idxs = [p2.lookup[token] for token in self.vocab_eos]\n\n    async def prefix(self, context):\n        w1 = await self.p1.prefix(context)\n        if w1 == float(\"-inf\"):\n            return float(\"-inf\")\n        w2 = await self.p2.prefix(context)\n        return w1 + w2\n\n    async def complete(self, context):\n        w1 = await self.p1.complete(context)\n        if w1 == float(\"-inf\"):\n            return float(\"-inf\")\n        w2 = await self.p2.complete(context)\n        return w1 + w2\n\n    async def batch_complete(self, contexts):\n        W1, W2 = await asyncio.gather(\n            self.p1.batch_complete(contexts), self.p2.batch_complete(contexts)\n        )\n        return W1 + W2\n\n    async def batch_prefix(self, contexts):\n        W1, W2 = await asyncio.gather(\n            self.p1.batch_prefix(contexts), self.p2.batch_prefix(contexts)\n        )\n        return W1 + W2\n\n    async def logw_next(self, context):\n        W1, W2 = await asyncio.gather(\n            self.p1.logw_next(context), self.p2.logw_next(context)\n        )\n        return self.make_lazy_weights(\n            W1.weights[self.v1_idxs] + W2.weights[self.v2_idxs]\n        )\n\n    async def batch_logw_next(self, contexts):\n        Ws1, Ws2 = await asyncio.gather(\n            self.p1.batch_logw_next(contexts), self.p2.batch_logw_next(contexts)\n        )\n        return [\n            self.make_lazy_weights(\n                Ws1[n].weights[self.v1_idxs] + Ws2[n].weights[self.v2_idxs]\n            )\n            for n in range(len(contexts))\n        ]\n\n    def spawn(self, p1_opts=None, p2_opts=None):\n        return Product(\n            self.p1.spawn(**(p1_opts or {})),\n            self.p2.spawn(**(p2_opts or {})),\n        )\n\n    def __repr__(self):\n        return f\"Product({self.p1!r}, {self.p2!r})\"\n</code></pre>"},{"location":"reference/genlm/control/potential/product/#genlm.control.potential.product.Product.__init__","title":"<code>__init__(p1, p2)</code>","text":"<p>Initialize a Product potential.</p> <p>Parameters:</p> Name Type Description Default <code>p1</code> <code>Potential</code> <p>First potential</p> required <code>p2</code> <code>Potential</code> <p>Second potential</p> required Source code in <code>genlm/control/potential/product.py</code> <pre><code>def __init__(self, p1, p2):\n    \"\"\"Initialize a Product potential.\n\n    Args:\n        p1 (Potential): First potential\n        p2 (Potential): Second potential\n    \"\"\"\n    self.p1 = p1\n    self.p2 = p2\n\n    if self.p1.token_type == self.p2.token_type:\n        self.token_type = self.p1.token_type\n    else:\n        raise ValueError(\n            \"Potentials in product must have the same token type. \"\n            f\"Got {self.p1.token_type} and {self.p2.token_type}.\"\n            + (\n                \"\\nMaybe you forgot to coerce the potentials to the same token type? See `Coerce`.\"\n                if (\n                    self.p1.token_type.is_iterable_of(self.p2.token_type)\n                    or self.p2.token_type.is_iterable_of(self.p1.token_type)\n                )\n                else \"\"\n            )\n        )\n\n    common_vocab = list(set(p1.vocab) &amp; set(p2.vocab))\n    if not common_vocab:\n        raise ValueError(\"Potentials in product must share a common vocabulary\")\n\n    # Check for small vocabulary overlap\n    threshold = 0.1\n    for potential, name in [(p1, \"p1\"), (p2, \"p2\")]:\n        overlap_ratio = len(common_vocab) / len(potential.vocab)\n        if overlap_ratio &lt; threshold:\n            warnings.warn(\n                f\"Common vocabulary ({len(common_vocab)} tokens) is less than {threshold * 100}% \"\n                f\"of {name}'s ({potential!r}) vocabulary ({len(potential.vocab)} tokens). \"\n                \"This Product potential only operates on this relatively small subset of tokens.\",\n                RuntimeWarning,\n            )\n\n    super().__init__(common_vocab, token_type=self.token_type)\n\n    # For fast products of weights\n    self.v1_idxs = [p1.lookup[token] for token in self.vocab_eos]\n    self.v2_idxs = [p2.lookup[token] for token in self.vocab_eos]\n</code></pre>"},{"location":"reference/genlm/control/potential/testing/","title":"testing","text":""},{"location":"reference/genlm/control/potential/testing/#genlm.control.potential.testing.PotentialTests","title":"<code>PotentialTests</code>","text":"<p>A mixin class providing testing utilities for validating Potential implementations.</p> <p>This class provides methods to verify the mathematical consistency and correctness of Potential implementations through various assertions:</p> <ul> <li>logw_next consistency: Verifies that token-level log weights are consistent with   prefix and complete scores.</li> <li>Autoregressive factorization: Validates that complete scores factor correctly as   a sum of log token weights (with an additional correction term corresponding to the   prefix weight of the empty sequence).</li> <li>Batch consistency: Ensures batch operations produce identical results to   their non-batch counterparts.</li> </ul> <p>All Potential instances inherit from this class and thus automatically gain access to these testing utilities.</p> Source code in <code>genlm/control/potential/testing.py</code> <pre><code>class PotentialTests:\n    \"\"\"A mixin class providing testing utilities for validating Potential implementations.\n\n    This class provides methods to verify the mathematical consistency and correctness\n    of Potential implementations through various assertions:\n\n    - logw_next consistency: Verifies that token-level log weights are consistent with\n      prefix and complete scores.\n    - Autoregressive factorization: Validates that complete scores factor correctly as\n      a sum of log token weights (with an additional correction term corresponding to the\n      prefix weight of the empty sequence).\n    - Batch consistency: Ensures batch operations produce identical results to\n      their non-batch counterparts.\n\n    All Potential instances inherit from this class and thus automatically gain access to these\n    testing utilities.\n    \"\"\"\n\n    colors = {\n        \"green\": \"\\033[92m\",\n        \"yellow\": \"\\033[93m\",\n        \"blue\": \"\\033[94m\",\n        \"magenta\": \"\\033[95m\",\n        \"cyan\": \"\\033[96m\",\n        \"red\": \"\\033[91m\",\n        \"reset\": \"\\033[0m\",\n    }\n\n    async def assert_logw_next_consistency(\n        self, context, rtol=1e-3, atol=1e-5, top=None, verbosity=0, method_args=()\n    ):\n        \"\"\"\n        Assert that `logw_next` is consistent with `prefix` and `complete`.\n\n        For a `context` of tokens $x_1, \\\\ldots, x_{n-1}$, this checks (in log space) whether:\n\n        $$\n        \\\\textsf{logw\\\\_next}(x_n | x_1, \\\\ldots, x_{n-1}) = \\\\textsf{score}(x_1, \\\\ldots, x_n) - \\\\textsf{prefix}(x_1, \\\\ldots, x_{n-1})\n        $$\n        for $x_n \\\\in \\\\textsf{vocab_eos}$, i.e., the potential's vocabulary and end-of-sequence token.\n\n        Args:\n            context (list): Context to test.\n            rtol (float): Relative tolerance for floating point comparison.\n            atol (float): Absolute tolerance for floating point comparison.\n            top (int):If specified, only test the top-k tokens by log weight. If None, test all tokens.\n            verbosity (int): Verbosity level.\n            method_args (tuple): Positional arguments to pass to `logw_next`, `prefix`, and `batch_score`.\n                Defaults to empty tuple.\n\n        Raises:\n            AssertionError: If `logw_next` is not consistent with `prefix` and `complete`.\n        \"\"\"\n        top_logw_next = (await self.logw_next(context, *method_args)).materialize(\n            top=top\n        )\n        tokens = list(top_logw_next.keys())\n        extended = [[*context, x] for x in tokens]\n\n        context_w = await self.prefix(context, *method_args)\n        extended_ws = np.array(\n            await asyncio.gather(*[self.score(e, *method_args) for e in extended])\n        )\n\n        wants = np.array([top_logw_next[x] for x in tokens])\n        haves = extended_ws - context_w\n\n        errors, valids = [], []\n        for i, (want, have) in enumerate(zip(wants, haves)):\n            abs_diff, rel_diff = self._compute_diff(want, have)\n            info = (want, have, abs_diff, rel_diff, tokens[i])\n            (valids if abs_diff &lt;= atol and rel_diff &lt;= rtol else errors).append(info)\n\n        if valids and verbosity &gt; 0:\n            print(\n                f\"{self.colors['green']}logw_next consistency with context={context!r} satisfied for tokens:{self.colors['reset']}\\n\"\n            )\n            for valid in valids:\n                want, have, abs_diff, rel_diff, token = valid\n                print(\n                    self._format_diff(want, have, abs_diff, rel_diff, atol, rtol, token)\n                )\n\n        if errors:\n            error_msg = f\"{self.colors['red']}logw_next consistency with context={context!r} not satisfied for tokens:{self.colors['reset']}\\n\\n\"\n            for error in errors:\n                want, have, abs_diff, rel_diff, token = error\n                error_msg += self._format_diff(\n                    want, have, abs_diff, rel_diff, atol, rtol, token\n                )\n            raise AssertionError(error_msg)\n\n    async def assert_autoreg_fact(\n        self, context, rtol=1e-3, atol=1e-5, verbosity=0, method_args=()\n    ):\n        \"\"\"\n        Assert that `complete` factors as an autoregressive sum of `logw_next`s.\n\n        For a `context` of tokens $x_1, \\\\ldots, x_n$, this checks (in log space) whether:\n\n        $$\n        \\\\textsf{complete}(x_1, \\\\ldots, x_n) - \\\\textsf{prefix}(\\\\epsilon) = \\\\textsf{logw\\\\_next}(\\\\textsf{eos} \\\\mid x_1, \\\\ldots, x_{n}) + \\\\sum_{i=1}^{n} \\\\textsf{logw_next}(x_i \\\\mid x_1, \\\\ldots, x_{i-1})\n        $$\n        where $\\\\epsilon$ is the empty sequence.\n\n        Args:\n            context (list): Context to test.\n            rtol (float): Relative tolerance for floating point comparison.\n            atol (float): Absolute tolerance for floating point comparison.\n            verbosity (int): Verbosity level.\n            method_args (tuple): Positional arguments to pass to `complete`, `prefix`, and `logw_next`.\n                Defaults to empty tuple.\n\n        Raises:\n            AssertionError: If the autoregressive factorization is not satisfied.\n        \"\"\"\n        want = (await self.complete(context, *method_args)) - (\n            await self.prefix([], *method_args)\n        )\n\n        logw_next_results = await asyncio.gather(\n            *[self.logw_next(context[:i], *method_args) for i in range(len(context))],\n            self.logw_next(context, *method_args),\n        )\n\n        have = (\n            sum(logw_next_results[i][context[i]] for i in range(len(context)))\n            + logw_next_results[-1][self.eos]\n        )\n\n        abs_diff, rel_diff = self._compute_diff(want, have)\n        if abs_diff &gt; atol or rel_diff &gt; rtol:\n            error_msg = (\n                f\"{self.colors['red']}Factorization not satisfied for context {context!r}:{self.colors['reset']}\\n\"\n                + self._format_diff(want, have, abs_diff, rel_diff, atol, rtol)\n            )\n            raise AssertionError(error_msg)\n\n        if verbosity &gt; 0:\n            print(\n                f\"{self.colors['green']}Factorization property satisfied for context {context}:{self.colors['reset']}\\n\"\n            )\n            print(self._format_diff(want, have, abs_diff, rel_diff, atol, rtol))\n\n    async def assert_batch_consistency(\n        self,\n        contexts,\n        rtol=1e-3,\n        atol=1e-5,\n        verbosity=0,\n        batch_method_args=(),\n        method_args=(),\n    ):\n        \"\"\"\n        Assert that batch results are equal to non-batch results.\n\n        Args:\n            contexts (list[list]): Contexts to test.\n            rtol (float): Relative tolerance for floating point comparison.\n            atol (float): Absolute tolerance for floating point comparison.\n            verbosity (int): Verbosity level.\n            batch_method_args (tuple): Positional arguments to pass to batch methods.\n                Defaults to empty tuple.\n            method_args (tuple): Positional arguments to pass to underlying potential methods.\n                Defaults to empty tuple.\n\n        Raises:\n            AssertionError: If the batch results are not equal to the non-batch results.\n        \"\"\"\n        batch_logw_nexts = await self.batch_logw_next(contexts, *batch_method_args)\n        batch_scores = await self.batch_score(contexts, *batch_method_args)\n\n        for i, context in enumerate(contexts):\n            logw_next = await self.logw_next(context, *method_args)\n            try:\n                np.testing.assert_allclose(\n                    batch_logw_nexts[i].weights, logw_next.weights, rtol=rtol, atol=atol\n                )\n                if verbosity &gt; 0:\n                    print(\n                        f\"{self.colors['green']}Batch logw_next consistency satisfied for context {context}:{self.colors['reset']}\"\n                    )\n                    print(\n                        f\"{self.colors['green']}Non-batched: {logw_next.weights}\\n\"\n                        + f\"{self.colors['green']}Batched:     {batch_logw_nexts[i].weights}{self.colors['reset']}\\n\"\n                    )\n            except AssertionError:\n                raise AssertionError(\n                    f\"{self.colors['red']}Batch logw_next mismatch for context {context}:{self.colors['reset']}\\n\"\n                    + f\"{self.colors['green']}Non-batched: {logw_next.weights}\\n\"\n                    + f\"{self.colors['red']}Batched:     {batch_logw_nexts[i].weights}{self.colors['reset']}\"\n                )\n\n            score = await self.score(context, *method_args)\n            abs_diff, rel_diff = self._compute_diff(score, batch_scores[i])\n            if abs_diff &gt; atol or rel_diff &gt; rtol:\n                raise AssertionError(\n                    f\"{self.colors['red']}Batch score mismatch for context {context}:{self.colors['reset']}\\n\"\n                    + f\"{self.colors['green']}Non-batched: {score}\\n\"\n                    + f\"{self.colors['red']}Batched:     {batch_scores[i]}{self.colors['reset']}\"\n                )\n            elif verbosity &gt; 0:\n                print(\n                    f\"{self.colors['green']}Batch score consistency satisfied for context {context}:{self.colors['reset']}\"\n                )\n                print(\n                    f\"{self.colors['green']}Non-batched: {score}\\n\"\n                    + f\"{self.colors['green']}Batched:     {batch_scores[i]}{self.colors['reset']}\\n\"\n                )\n\n    def _compute_diff(self, want, have):\n        is_inf = want == float(\"-inf\") and have == float(\"-inf\")\n        abs_diff = 0 if is_inf else abs(want - have)\n        if want == 0:\n            rel_diff = 0 if have == 0 else float(\"inf\")\n        else:\n            rel_diff = 0 if is_inf else abs((want - have) / want)\n        return abs_diff, rel_diff\n\n    def _format_diff(self, want, have, abs_diff, rel_diff, atol, rtol, token=None):\n        abs_diff_str = (\n            f\"{self.colors['cyan']}Abs Diff: {abs_diff:.6f} &lt;= {atol=}\\033[0m\"\n        )\n        rel_diff_str = (\n            f\"{self.colors['magenta']}Rel Diff: {rel_diff:.6f} &lt;= {rtol=}\\033[0m\"\n        )\n\n        want_str = f\"{self.colors['green']}Expected: {want:.6f}{self.colors['reset']}\"\n        have_clr = (\n            self.colors[\"yellow\"]\n            if abs_diff &lt;= atol and rel_diff &lt;= rtol\n            else self.colors[\"red\"]\n        )\n        have_str = f\"{have_clr}Actual:   {have:.6f}{self.colors['reset']}\"\n\n        if abs_diff &lt;= atol:\n            abs_diff_str = f\"{self.colors['green']}Abs Diff: {abs_diff:.6f} &lt;= {atol=}{self.colors['reset']}\"\n        else:\n            abs_diff_str = f\"{self.colors['red']}Abs Diff: {abs_diff:.6f} &gt; {atol=}{self.colors['reset']}\"\n\n        if rel_diff &lt;= rtol:\n            rel_diff_str = f\"{self.colors['green']}Rel Diff: {rel_diff:.6f} &lt;= {rtol=}{self.colors['reset']}\"\n        else:\n            rel_diff_str = f\"{self.colors['red']}Rel Diff: {rel_diff:.6f} &gt; {rtol=}{self.colors['reset']}\"\n\n        token_str = (\n            f\"{self.colors['blue']}Token:    {token}{self.colors['reset']}\\n\"\n            if token\n            else \"\"\n        )\n        return f\"{token_str}{want_str}\\n{have_str}\\n{abs_diff_str}\\n{rel_diff_str}\\n\\n\"\n</code></pre>"},{"location":"reference/genlm/control/potential/testing/#genlm.control.potential.testing.PotentialTests.assert_logw_next_consistency","title":"<code>assert_logw_next_consistency(context, rtol=0.001, atol=1e-05, top=None, verbosity=0, method_args=())</code>  <code>async</code>","text":"<p>Assert that <code>logw_next</code> is consistent with <code>prefix</code> and <code>complete</code>.</p> <p>For a <code>context</code> of tokens \\(x_1, \\ldots, x_{n-1}\\), this checks (in log space) whether:</p> <p>$$ \\textsf{logw_next}(x_n | x_1, \\ldots, x_{n-1}) = \\textsf{score}(x_1, \\ldots, x_n) - \\textsf{prefix}(x_1, \\ldots, x_{n-1}) $$ for \\(x_n \\in \\textsf{vocab_eos}\\), i.e., the potential's vocabulary and end-of-sequence token.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Context to test.</p> required <code>rtol</code> <code>float</code> <p>Relative tolerance for floating point comparison.</p> <code>0.001</code> <code>atol</code> <code>float</code> <p>Absolute tolerance for floating point comparison.</p> <code>1e-05</code> <code>top</code> <code>int</code> <p>If specified, only test the top-k tokens by log weight. If None, test all tokens.</p> <code>None</code> <code>verbosity</code> <code>int</code> <p>Verbosity level.</p> <code>0</code> <code>method_args</code> <code>tuple</code> <p>Positional arguments to pass to <code>logw_next</code>, <code>prefix</code>, and <code>batch_score</code>. Defaults to empty tuple.</p> <code>()</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If <code>logw_next</code> is not consistent with <code>prefix</code> and <code>complete</code>.</p> Source code in <code>genlm/control/potential/testing.py</code> <pre><code>async def assert_logw_next_consistency(\n    self, context, rtol=1e-3, atol=1e-5, top=None, verbosity=0, method_args=()\n):\n    \"\"\"\n    Assert that `logw_next` is consistent with `prefix` and `complete`.\n\n    For a `context` of tokens $x_1, \\\\ldots, x_{n-1}$, this checks (in log space) whether:\n\n    $$\n    \\\\textsf{logw\\\\_next}(x_n | x_1, \\\\ldots, x_{n-1}) = \\\\textsf{score}(x_1, \\\\ldots, x_n) - \\\\textsf{prefix}(x_1, \\\\ldots, x_{n-1})\n    $$\n    for $x_n \\\\in \\\\textsf{vocab_eos}$, i.e., the potential's vocabulary and end-of-sequence token.\n\n    Args:\n        context (list): Context to test.\n        rtol (float): Relative tolerance for floating point comparison.\n        atol (float): Absolute tolerance for floating point comparison.\n        top (int):If specified, only test the top-k tokens by log weight. If None, test all tokens.\n        verbosity (int): Verbosity level.\n        method_args (tuple): Positional arguments to pass to `logw_next`, `prefix`, and `batch_score`.\n            Defaults to empty tuple.\n\n    Raises:\n        AssertionError: If `logw_next` is not consistent with `prefix` and `complete`.\n    \"\"\"\n    top_logw_next = (await self.logw_next(context, *method_args)).materialize(\n        top=top\n    )\n    tokens = list(top_logw_next.keys())\n    extended = [[*context, x] for x in tokens]\n\n    context_w = await self.prefix(context, *method_args)\n    extended_ws = np.array(\n        await asyncio.gather(*[self.score(e, *method_args) for e in extended])\n    )\n\n    wants = np.array([top_logw_next[x] for x in tokens])\n    haves = extended_ws - context_w\n\n    errors, valids = [], []\n    for i, (want, have) in enumerate(zip(wants, haves)):\n        abs_diff, rel_diff = self._compute_diff(want, have)\n        info = (want, have, abs_diff, rel_diff, tokens[i])\n        (valids if abs_diff &lt;= atol and rel_diff &lt;= rtol else errors).append(info)\n\n    if valids and verbosity &gt; 0:\n        print(\n            f\"{self.colors['green']}logw_next consistency with context={context!r} satisfied for tokens:{self.colors['reset']}\\n\"\n        )\n        for valid in valids:\n            want, have, abs_diff, rel_diff, token = valid\n            print(\n                self._format_diff(want, have, abs_diff, rel_diff, atol, rtol, token)\n            )\n\n    if errors:\n        error_msg = f\"{self.colors['red']}logw_next consistency with context={context!r} not satisfied for tokens:{self.colors['reset']}\\n\\n\"\n        for error in errors:\n            want, have, abs_diff, rel_diff, token = error\n            error_msg += self._format_diff(\n                want, have, abs_diff, rel_diff, atol, rtol, token\n            )\n        raise AssertionError(error_msg)\n</code></pre>"},{"location":"reference/genlm/control/potential/testing/#genlm.control.potential.testing.PotentialTests.assert_autoreg_fact","title":"<code>assert_autoreg_fact(context, rtol=0.001, atol=1e-05, verbosity=0, method_args=())</code>  <code>async</code>","text":"<p>Assert that <code>complete</code> factors as an autoregressive sum of <code>logw_next</code>s.</p> <p>For a <code>context</code> of tokens \\(x_1, \\ldots, x_n\\), this checks (in log space) whether:</p> <p>$$ \\textsf{complete}(x_1, \\ldots, x_n) - \\textsf{prefix}(\\epsilon) = \\textsf{logw_next}(\\textsf{eos} \\mid x_1, \\ldots, x_{n}) + \\sum_{i=1}^{n} \\textsf{logw_next}(x_i \\mid x_1, \\ldots, x_{i-1}) $$ where \\(\\epsilon\\) is the empty sequence.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Context to test.</p> required <code>rtol</code> <code>float</code> <p>Relative tolerance for floating point comparison.</p> <code>0.001</code> <code>atol</code> <code>float</code> <p>Absolute tolerance for floating point comparison.</p> <code>1e-05</code> <code>verbosity</code> <code>int</code> <p>Verbosity level.</p> <code>0</code> <code>method_args</code> <code>tuple</code> <p>Positional arguments to pass to <code>complete</code>, <code>prefix</code>, and <code>logw_next</code>. Defaults to empty tuple.</p> <code>()</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the autoregressive factorization is not satisfied.</p> Source code in <code>genlm/control/potential/testing.py</code> <pre><code>async def assert_autoreg_fact(\n    self, context, rtol=1e-3, atol=1e-5, verbosity=0, method_args=()\n):\n    \"\"\"\n    Assert that `complete` factors as an autoregressive sum of `logw_next`s.\n\n    For a `context` of tokens $x_1, \\\\ldots, x_n$, this checks (in log space) whether:\n\n    $$\n    \\\\textsf{complete}(x_1, \\\\ldots, x_n) - \\\\textsf{prefix}(\\\\epsilon) = \\\\textsf{logw\\\\_next}(\\\\textsf{eos} \\\\mid x_1, \\\\ldots, x_{n}) + \\\\sum_{i=1}^{n} \\\\textsf{logw_next}(x_i \\\\mid x_1, \\\\ldots, x_{i-1})\n    $$\n    where $\\\\epsilon$ is the empty sequence.\n\n    Args:\n        context (list): Context to test.\n        rtol (float): Relative tolerance for floating point comparison.\n        atol (float): Absolute tolerance for floating point comparison.\n        verbosity (int): Verbosity level.\n        method_args (tuple): Positional arguments to pass to `complete`, `prefix`, and `logw_next`.\n            Defaults to empty tuple.\n\n    Raises:\n        AssertionError: If the autoregressive factorization is not satisfied.\n    \"\"\"\n    want = (await self.complete(context, *method_args)) - (\n        await self.prefix([], *method_args)\n    )\n\n    logw_next_results = await asyncio.gather(\n        *[self.logw_next(context[:i], *method_args) for i in range(len(context))],\n        self.logw_next(context, *method_args),\n    )\n\n    have = (\n        sum(logw_next_results[i][context[i]] for i in range(len(context)))\n        + logw_next_results[-1][self.eos]\n    )\n\n    abs_diff, rel_diff = self._compute_diff(want, have)\n    if abs_diff &gt; atol or rel_diff &gt; rtol:\n        error_msg = (\n            f\"{self.colors['red']}Factorization not satisfied for context {context!r}:{self.colors['reset']}\\n\"\n            + self._format_diff(want, have, abs_diff, rel_diff, atol, rtol)\n        )\n        raise AssertionError(error_msg)\n\n    if verbosity &gt; 0:\n        print(\n            f\"{self.colors['green']}Factorization property satisfied for context {context}:{self.colors['reset']}\\n\"\n        )\n        print(self._format_diff(want, have, abs_diff, rel_diff, atol, rtol))\n</code></pre>"},{"location":"reference/genlm/control/potential/testing/#genlm.control.potential.testing.PotentialTests.assert_batch_consistency","title":"<code>assert_batch_consistency(contexts, rtol=0.001, atol=1e-05, verbosity=0, batch_method_args=(), method_args=())</code>  <code>async</code>","text":"<p>Assert that batch results are equal to non-batch results.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list[list]</code> <p>Contexts to test.</p> required <code>rtol</code> <code>float</code> <p>Relative tolerance for floating point comparison.</p> <code>0.001</code> <code>atol</code> <code>float</code> <p>Absolute tolerance for floating point comparison.</p> <code>1e-05</code> <code>verbosity</code> <code>int</code> <p>Verbosity level.</p> <code>0</code> <code>batch_method_args</code> <code>tuple</code> <p>Positional arguments to pass to batch methods. Defaults to empty tuple.</p> <code>()</code> <code>method_args</code> <code>tuple</code> <p>Positional arguments to pass to underlying potential methods. Defaults to empty tuple.</p> <code>()</code> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the batch results are not equal to the non-batch results.</p> Source code in <code>genlm/control/potential/testing.py</code> <pre><code>async def assert_batch_consistency(\n    self,\n    contexts,\n    rtol=1e-3,\n    atol=1e-5,\n    verbosity=0,\n    batch_method_args=(),\n    method_args=(),\n):\n    \"\"\"\n    Assert that batch results are equal to non-batch results.\n\n    Args:\n        contexts (list[list]): Contexts to test.\n        rtol (float): Relative tolerance for floating point comparison.\n        atol (float): Absolute tolerance for floating point comparison.\n        verbosity (int): Verbosity level.\n        batch_method_args (tuple): Positional arguments to pass to batch methods.\n            Defaults to empty tuple.\n        method_args (tuple): Positional arguments to pass to underlying potential methods.\n            Defaults to empty tuple.\n\n    Raises:\n        AssertionError: If the batch results are not equal to the non-batch results.\n    \"\"\"\n    batch_logw_nexts = await self.batch_logw_next(contexts, *batch_method_args)\n    batch_scores = await self.batch_score(contexts, *batch_method_args)\n\n    for i, context in enumerate(contexts):\n        logw_next = await self.logw_next(context, *method_args)\n        try:\n            np.testing.assert_allclose(\n                batch_logw_nexts[i].weights, logw_next.weights, rtol=rtol, atol=atol\n            )\n            if verbosity &gt; 0:\n                print(\n                    f\"{self.colors['green']}Batch logw_next consistency satisfied for context {context}:{self.colors['reset']}\"\n                )\n                print(\n                    f\"{self.colors['green']}Non-batched: {logw_next.weights}\\n\"\n                    + f\"{self.colors['green']}Batched:     {batch_logw_nexts[i].weights}{self.colors['reset']}\\n\"\n                )\n        except AssertionError:\n            raise AssertionError(\n                f\"{self.colors['red']}Batch logw_next mismatch for context {context}:{self.colors['reset']}\\n\"\n                + f\"{self.colors['green']}Non-batched: {logw_next.weights}\\n\"\n                + f\"{self.colors['red']}Batched:     {batch_logw_nexts[i].weights}{self.colors['reset']}\"\n            )\n\n        score = await self.score(context, *method_args)\n        abs_diff, rel_diff = self._compute_diff(score, batch_scores[i])\n        if abs_diff &gt; atol or rel_diff &gt; rtol:\n            raise AssertionError(\n                f\"{self.colors['red']}Batch score mismatch for context {context}:{self.colors['reset']}\\n\"\n                + f\"{self.colors['green']}Non-batched: {score}\\n\"\n                + f\"{self.colors['red']}Batched:     {batch_scores[i]}{self.colors['reset']}\"\n            )\n        elif verbosity &gt; 0:\n            print(\n                f\"{self.colors['green']}Batch score consistency satisfied for context {context}:{self.colors['reset']}\"\n            )\n            print(\n                f\"{self.colors['green']}Non-batched: {score}\\n\"\n                + f\"{self.colors['green']}Batched:     {batch_scores[i]}{self.colors['reset']}\\n\"\n            )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/","title":"built_in","text":""},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM","title":"<code>PromptedLLM</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A potential representing a language model conditioned on a fixed prompt prefix.</p> <p><code>PromptedLLM</code>s operate on byte sequences.</p> <p>Notes on EOS Token Handling:</p> <ul> <li> <p>Tokens to treat as end-of-sequence tokens are specified via the <code>eos_tokens</code> argument.</p> </li> <li> <p>These tokens are excluded from the potential's vocabulary and as such do not appear in the <code>vocab</code> attribute.</p> <p>This means they cannot appear in any input contexts to the potential nor in the output of <code>logw_next</code>. They can be used in the prompt however.</p> </li> <li> <p>The log probability assigned to the <code>genlm.control</code>'s reserved <code>EOS</code> token is the sum of the log probabilities of all the specified EOS tokens.</p> </li> </ul> <p>This class wraps an <code>AsyncLM</code> instance.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>class PromptedLLM(Potential):\n    \"\"\"A potential representing a language model conditioned on a fixed prompt prefix.\n\n    `PromptedLLM`s operate on byte sequences.\n\n    Notes on EOS Token Handling:\\n\n    - Tokens to treat as end-of-sequence tokens are specified via the `eos_tokens` argument.\\n\n    - These tokens are excluded from the potential's vocabulary and as such do not appear in the `vocab` attribute.\\n\n        This means they cannot appear in any input contexts to the potential nor in the output of `logw_next`. They can be used in the prompt however.\\n\n    - The log probability assigned to the `genlm.control`'s reserved `EOS` token is the sum of the log probabilities of all the specified EOS tokens.\\n\n\n    This class wraps an `AsyncLM` instance.\n    \"\"\"\n\n    def __init__(self, llm, prompt_ids=None, eos_tokens=None, temperature=1):\n        \"\"\"`\n        Initializes the PromptedLLM potential.\n\n        Args:\n            llm (AsyncLM): The language model to use.\n            prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n                Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `prompt` or `prompt_ids`.\n            eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n                Defaults to the EOS token of the language model's tokenizer.\n            temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n\n        Raises:\n            ValueError: If any EOS token is not in the language model vocabulary.\n        \"\"\"\n        self.model = llm\n        self.prompt_ids = prompt_ids or []\n\n        if not eos_tokens:\n            self._eos_tokens = [llm.byte_vocab[self.model.tokenizer.eos_token_id]]\n        else:\n            self._eos_tokens = eos_tokens\n\n        assert len(set(self._eos_tokens)) == len(self._eos_tokens), (\n            \"duplicate eos tokens\"\n        )\n\n        self.token_maps = TokenMappings.create(\n            decode=llm.byte_vocab, eos_tokens=self._eos_tokens\n        )\n\n        self.temperature = temperature\n\n        V = [x for x in self.token_maps.decode if x not in self._eos_tokens]\n\n        super().__init__(vocabulary=V)\n\n    @classmethod\n    def from_name(\n        cls,\n        name,\n        backend=None,\n        eos_tokens=None,\n        prompt_ids=None,\n        temperature=1.0,\n        **kwargs,\n    ):\n        \"\"\"Create a `PromptedLLM` from a HugginFace model name.\n\n        Args:\n            name (str): Name of the model to load\n            backend (str, optional): `AsyncLM` backend to use:\\n\n                * 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\\n\n                * 'hf' for an `AsyncTransformer`; ideal for CPU usage\\n\n                * 'mock' for a `MockAsyncLM`; ideal for testing.\\n\n                Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n            eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n                Defaults to the EOS token of the language model's tokenizer.\n            prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n                Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `set_prompt_from_str` or `prompt_ids`.\n            temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n            **kwargs (dict): Additional arguments passed to AsyncLM constructor\n\n        Returns:\n            (PromptedLLM): An instance of PromptedLLM\n        \"\"\"\n        backend = backend or (\"vllm\" if torch.cuda.is_available() else \"hf\")\n        model = load_model_by_name(name, backend=backend, **kwargs)\n        return cls(\n            model, prompt_ids=prompt_ids, eos_tokens=eos_tokens, temperature=temperature\n        )\n\n    @property\n    def eos_tokens(self):\n        return self._eos_tokens\n\n    @eos_tokens.setter\n    def eos_tokens(self, value):\n        raise ValueError(\n            \"Cannot reset eos_tokens after initialization. \"\n            \"Use spawn_new_eos(new_eos_tokens) instead.\"\n        )\n\n    @property\n    def prompt(self):\n        \"\"\"\n        Get the current prompt as a list of byte sequences corresponding to the prompt token IDs.\n\n        Returns:\n            (list[bytes]|None): The current prompt as a list of bytes sequences or None if no prompt_ids are set.\n        \"\"\"\n        if not self.prompt_ids:\n            return  # pragma: no cover\n        return [self.token_maps.decode[x] for x in self.prompt_ids]\n\n    def set_prompt_from_str(self, prompt_str):\n        \"\"\"Set the fixed prompt from a string.\n\n        Modifies `prompt_ids` to be the token IDs of the input prompt according to the language model's tokenizer.\n\n        Args:\n            prompt_str (str): The prompt to set.\n        \"\"\"\n        # TODO: Handle race condition where prompt_ids reset concurrently.\n        if not isinstance(prompt_str, str):\n            raise ValueError(\n                f\"Prompt must a string got {type(prompt_str)}. \"\n                f\"To set the prompt from a list of token IDs, use prompt_ids.\"\n            )\n\n        if prompt_str.endswith(\" \"):\n            warnings.warn(\n                \"Prompt ends with whitespace, which may affect tokenization. \"\n                \"Consider removing trailing whitespace.\",\n                stacklevel=2,\n            )\n\n        self.prompt_ids = self.model.tokenizer.encode(prompt_str)\n\n    def encode_tokens(self, tokens):\n        \"\"\"Encode a list of byte tokens to a list of token IDs in\n        the underlying language model's vocabulary.\n\n        Args:\n            tokens (list[bytes]): List of byte tokens to encode\n\n        Returns:\n            (list[int]): A list of token IDs corresponding to the input tokens.\n\n        Raises:\n            ValueError: If any token is not in the vocabulary\n        \"\"\"\n        try:\n            return [self.token_maps.encode[x] for x in tokens]\n        except KeyError as e:\n            raise ValueError(f\"Token {e.args[0]} not in vocabulary\") from e\n\n    def decode_tokens(self, ids):\n        \"\"\"\n        Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.\n\n        Args:\n            ids (list[int]): A list of token IDs in the language model's vocabulary.\n\n        Returns:\n            (list[bytes]): A list of byte tokens corresponding to the input token IDs.\n        \"\"\"\n        return [self.token_maps.decode[x] for x in ids]\n\n    def tokenize(self, context_str):\n        \"\"\"Tokenize a string to a list of `bytes` objects, each corresponding to a token in the vocabulary.\n\n        Uses the language model's tokenizer to map `context_str` to a list of token IDs, and then decodes the token IDs to bytes.\n\n        Args:\n            context_str (str): A string to encode\n\n        Returns:\n            (List[bytes]): A list of byte tokens corresponding to the input string.\n        \"\"\"\n        return self.decode_tokens(self.model.tokenizer.encode(context_str))\n\n    async def log_probability(self, context):\n        \"\"\"\n        Compute the log probability of `context` given the prompt.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of `context`.\n        \"\"\"\n        if not context:\n            return 0\n\n        context_ids = self.encode_tokens(context)\n        return await self._log_probability(context_ids)\n\n    async def _log_probability(self, context_ids):\n        prefixes = [self.prompt_ids + context_ids[:i] for i in range(len(context_ids))]\n        log_ps = self._maybe_temper(\n            await self.model.batch_next_token_logprobs(prefixes)\n        )\n        target_ids = torch.tensor(context_ids, device=log_ps.device)\n        with torch.no_grad():\n            token_logprobs = torch.gather(log_ps, 1, target_ids.unsqueeze(1))\n            total_logprob = token_logprobs.sum().item()\n\n        return total_logprob\n\n    def _maybe_temper(self, logps):\n        if self.temperature == 1:\n            return logps\n        return torch.log_softmax(logps / self.temperature, dim=-1)\n\n    async def prefix(self, context):\n        \"\"\"\n        Compute the log probability of `context` given the prompt.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of `context`.\n        \"\"\"\n        return await self.log_probability(context)\n\n    async def complete(self, context):\n        \"\"\"\n        Compute the log probability of `context` and the eos tokens given the prompt.\n\n        If the model has multiple eos tokens, their probabilities will be summed.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of the context.\n        \"\"\"\n        context_ids = self.encode_tokens(context)\n        logp_context = await self._log_probability(context_ids)\n        logp_next = self._maybe_temper(\n            await self.model.next_token_logprobs(self.prompt_ids + context_ids)\n        )\n        logp_eos = torch.logsumexp(logp_next[self.token_maps.eos_idxs], dim=0).item()\n        return logp_context + logp_eos\n\n    def _process_logw_next(self, logw_next):\n        \"\"\"Process the log probabilities for the next tokens.\n\n        This function rearranges the log probabilities such that the end-of-sequence (EOS) token's log probability\n        is the sum of the log probabilities of `self.eos_tokens`.\n\n        Args:\n            logw_next (torch.tensor): The log probabilities for the next tokens.\n\n        Returns:\n            (LazyWeights): Processed log probabilities for the next tokens.\n        \"\"\"\n        # This is ugly, but it's useful for all potentials to adhere to the convention\n        # of keeping the EOS token at the end of the weights array.\n        logw_next = logw_next[: len(self.token_maps.decode)]\n        logw_next = logw_next.log_softmax(dim=0)\n        _logw_next = torch.full((len(self.vocab) + 1,), float('-inf'), dtype=logw_next.dtype, device=logw_next.device)\n        _logw_next[: len(self.vocab)] = logw_next[\n            ~torch.isin(torch.arange(len(logw_next)), torch.tensor(self.token_maps.eos_idxs))\n        ]\n        _logw_next[-1] = torch.logsumexp(logw_next[self.token_maps.eos_idxs], dim=0).item()\n        return self.make_lazy_weights(_logw_next.float().cpu().numpy())\n\n    async def logw_next(self, context):\n        \"\"\"Get log probabilities for next tokens given the prompt and `context`.\n\n        Args:\n            context (List[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (LazyWeights): Log probabilities for next tokens and EOS.\n        \"\"\"\n        logw_next = self._maybe_temper(\n            await self.model.next_token_logprobs(\n                self.prompt_ids + self.encode_tokens(context)\n            )\n        )\n        return self._process_logw_next(logw_next)\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"Get log probabilities for next tokens given the prompt and `context`, for a batch of contexts.\n\n        Args:\n            contexts (list[list[bytes]]): A list of sequences of bytes tokens.\n\n        Returns:\n            (List[LazyWeights]): Log probabilities for next tokens and EOS for each context.\n        \"\"\"\n        logw_nexts = self._maybe_temper(\n            await self.model.batch_next_token_logprobs(\n                [self.prompt_ids + self.encode_tokens(context) for context in contexts]\n            )\n        )\n        return [\n            self._process_logw_next(logw_next)\n            for logw_next in logw_nexts\n        ]\n\n    def __repr__(self):\n        return f\"PromptedLLM(prompt={self.prompt!r})\"\n\n    def spawn(self):\n        \"\"\"\n        Spawn a new PromptedLLM with the same prompt and eos tokens.\n\n        Returns:\n            (PromptedLLM): A new PromptedLLM with the same prompt and eos tokens.\n\n        Note:\n            This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.\n        \"\"\"\n        return PromptedLLM(\n            self.model,\n            prompt_ids=self.prompt_ids.copy(),\n            eos_tokens=self._eos_tokens.copy(),\n            temperature=self.temperature,\n        )\n\n    def spawn_new_eos(self, eos_tokens):\n        \"\"\"\n        Create a new PromptedLLM with a different set of end-of-sequence tokens.\n\n        Args:\n            eos_tokens (list[bytes]): A list of tokens to treat as end-of-sequence tokens.\n\n        Returns:\n            (PromptedLLM): A new PromptedLLM with the specified end-of-sequence tokens.\n                The new model will have the same prompt_ids as `self`.\n        \"\"\"\n        return PromptedLLM(\n            self.model,\n            prompt_ids=self.prompt_ids.copy(),\n            eos_tokens=eos_tokens.copy(),\n            temperature=self.temperature,\n        )\n\n    def to_autobatched(self):\n        raise ValueError(\"PromptedLLMs are autobatched by default.\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.__init__","title":"<code>__init__(llm, prompt_ids=None, eos_tokens=None, temperature=1)</code>","text":"<p>` Initializes the PromptedLLM potential.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>AsyncLM</code> <p>The language model to use.</p> required <code>prompt_ids</code> <code>list[int]</code> <p>Optional prompt to use as a prompt prefix for all input contexts. Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via <code>prompt</code> or <code>prompt_ids</code>.</p> <code>None</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens to treat as end-of-sequence tokens. Defaults to the EOS token of the language model's tokenizer.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to apply to the language model's logits. Defaults to 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any EOS token is not in the language model vocabulary.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def __init__(self, llm, prompt_ids=None, eos_tokens=None, temperature=1):\n    \"\"\"`\n    Initializes the PromptedLLM potential.\n\n    Args:\n        llm (AsyncLM): The language model to use.\n        prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n            Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `prompt` or `prompt_ids`.\n        eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n            Defaults to the EOS token of the language model's tokenizer.\n        temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n\n    Raises:\n        ValueError: If any EOS token is not in the language model vocabulary.\n    \"\"\"\n    self.model = llm\n    self.prompt_ids = prompt_ids or []\n\n    if not eos_tokens:\n        self._eos_tokens = [llm.byte_vocab[self.model.tokenizer.eos_token_id]]\n    else:\n        self._eos_tokens = eos_tokens\n\n    assert len(set(self._eos_tokens)) == len(self._eos_tokens), (\n        \"duplicate eos tokens\"\n    )\n\n    self.token_maps = TokenMappings.create(\n        decode=llm.byte_vocab, eos_tokens=self._eos_tokens\n    )\n\n    self.temperature = temperature\n\n    V = [x for x in self.token_maps.decode if x not in self._eos_tokens]\n\n    super().__init__(vocabulary=V)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.from_name","title":"<code>from_name(name, backend=None, eos_tokens=None, prompt_ids=None, temperature=1.0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>PromptedLLM</code> from a HugginFace model name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model to load</p> required <code>backend</code> <code>str</code> <p><code>AsyncLM</code> backend to use:</p> <ul> <li> <p>'vllm' to instantiate an <code>AsyncVirtualLM</code>; ideal for GPU usage</p> </li> <li> <p>'hf' for an <code>AsyncTransformer</code>; ideal for CPU usage</p> </li> <li> <p>'mock' for a <code>MockAsyncLM</code>; ideal for testing.</p> </li> </ul> <p>Defaults to 'vllm' if CUDA is available, otherwise 'hf'.</p> <code>None</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens to treat as end-of-sequence tokens. Defaults to the EOS token of the language model's tokenizer.</p> <code>None</code> <code>prompt_ids</code> <code>list[int]</code> <p>Optional prompt to use as a prompt prefix for all input contexts. Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via <code>set_prompt_from_str</code> or <code>prompt_ids</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to apply to the language model's logits. Defaults to 1.</p> <code>1.0</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to AsyncLM constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>An instance of PromptedLLM</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>@classmethod\ndef from_name(\n    cls,\n    name,\n    backend=None,\n    eos_tokens=None,\n    prompt_ids=None,\n    temperature=1.0,\n    **kwargs,\n):\n    \"\"\"Create a `PromptedLLM` from a HugginFace model name.\n\n    Args:\n        name (str): Name of the model to load\n        backend (str, optional): `AsyncLM` backend to use:\\n\n            * 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\\n\n            * 'hf' for an `AsyncTransformer`; ideal for CPU usage\\n\n            * 'mock' for a `MockAsyncLM`; ideal for testing.\\n\n            Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n        eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n            Defaults to the EOS token of the language model's tokenizer.\n        prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n            Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `set_prompt_from_str` or `prompt_ids`.\n        temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n        **kwargs (dict): Additional arguments passed to AsyncLM constructor\n\n    Returns:\n        (PromptedLLM): An instance of PromptedLLM\n    \"\"\"\n    backend = backend or (\"vllm\" if torch.cuda.is_available() else \"hf\")\n    model = load_model_by_name(name, backend=backend, **kwargs)\n    return cls(\n        model, prompt_ids=prompt_ids, eos_tokens=eos_tokens, temperature=temperature\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.prompt","title":"<code>prompt</code>  <code>property</code>","text":"<p>Get the current prompt as a list of byte sequences corresponding to the prompt token IDs.</p> <p>Returns:</p> Type Description <code>list[bytes] | None</code> <p>The current prompt as a list of bytes sequences or None if no prompt_ids are set.</p>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.set_prompt_from_str","title":"<code>set_prompt_from_str(prompt_str)</code>","text":"<p>Set the fixed prompt from a string.</p> <p>Modifies <code>prompt_ids</code> to be the token IDs of the input prompt according to the language model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_str</code> <code>str</code> <p>The prompt to set.</p> required Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def set_prompt_from_str(self, prompt_str):\n    \"\"\"Set the fixed prompt from a string.\n\n    Modifies `prompt_ids` to be the token IDs of the input prompt according to the language model's tokenizer.\n\n    Args:\n        prompt_str (str): The prompt to set.\n    \"\"\"\n    # TODO: Handle race condition where prompt_ids reset concurrently.\n    if not isinstance(prompt_str, str):\n        raise ValueError(\n            f\"Prompt must a string got {type(prompt_str)}. \"\n            f\"To set the prompt from a list of token IDs, use prompt_ids.\"\n        )\n\n    if prompt_str.endswith(\" \"):\n        warnings.warn(\n            \"Prompt ends with whitespace, which may affect tokenization. \"\n            \"Consider removing trailing whitespace.\",\n            stacklevel=2,\n        )\n\n    self.prompt_ids = self.model.tokenizer.encode(prompt_str)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.encode_tokens","title":"<code>encode_tokens(tokens)</code>","text":"<p>Encode a list of byte tokens to a list of token IDs in the underlying language model's vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[bytes]</code> <p>List of byte tokens to encode</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of token IDs corresponding to the input tokens.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any token is not in the vocabulary</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def encode_tokens(self, tokens):\n    \"\"\"Encode a list of byte tokens to a list of token IDs in\n    the underlying language model's vocabulary.\n\n    Args:\n        tokens (list[bytes]): List of byte tokens to encode\n\n    Returns:\n        (list[int]): A list of token IDs corresponding to the input tokens.\n\n    Raises:\n        ValueError: If any token is not in the vocabulary\n    \"\"\"\n    try:\n        return [self.token_maps.encode[x] for x in tokens]\n    except KeyError as e:\n        raise ValueError(f\"Token {e.args[0]} not in vocabulary\") from e\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.decode_tokens","title":"<code>decode_tokens(ids)</code>","text":"<p>Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[int]</code> <p>A list of token IDs in the language model's vocabulary.</p> required <p>Returns:</p> Type Description <code>list[bytes]</code> <p>A list of byte tokens corresponding to the input token IDs.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def decode_tokens(self, ids):\n    \"\"\"\n    Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.\n\n    Args:\n        ids (list[int]): A list of token IDs in the language model's vocabulary.\n\n    Returns:\n        (list[bytes]): A list of byte tokens corresponding to the input token IDs.\n    \"\"\"\n    return [self.token_maps.decode[x] for x in ids]\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.tokenize","title":"<code>tokenize(context_str)</code>","text":"<p>Tokenize a string to a list of <code>bytes</code> objects, each corresponding to a token in the vocabulary.</p> <p>Uses the language model's tokenizer to map <code>context_str</code> to a list of token IDs, and then decodes the token IDs to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>context_str</code> <code>str</code> <p>A string to encode</p> required <p>Returns:</p> Type Description <code>List[bytes]</code> <p>A list of byte tokens corresponding to the input string.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def tokenize(self, context_str):\n    \"\"\"Tokenize a string to a list of `bytes` objects, each corresponding to a token in the vocabulary.\n\n    Uses the language model's tokenizer to map `context_str` to a list of token IDs, and then decodes the token IDs to bytes.\n\n    Args:\n        context_str (str): A string to encode\n\n    Returns:\n        (List[bytes]): A list of byte tokens corresponding to the input string.\n    \"\"\"\n    return self.decode_tokens(self.model.tokenizer.encode(context_str))\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.log_probability","title":"<code>log_probability(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> given the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def log_probability(self, context):\n    \"\"\"\n    Compute the log probability of `context` given the prompt.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of `context`.\n    \"\"\"\n    if not context:\n        return 0\n\n    context_ids = self.encode_tokens(context)\n    return await self._log_probability(context_ids)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> given the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Compute the log probability of `context` given the prompt.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of `context`.\n    \"\"\"\n    return await self.log_probability(context)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> and the eos tokens given the prompt.</p> <p>If the model has multiple eos tokens, their probabilities will be summed.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of the context.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Compute the log probability of `context` and the eos tokens given the prompt.\n\n    If the model has multiple eos tokens, their probabilities will be summed.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of the context.\n    \"\"\"\n    context_ids = self.encode_tokens(context)\n    logp_context = await self._log_probability(context_ids)\n    logp_next = self._maybe_temper(\n        await self.model.next_token_logprobs(self.prompt_ids + context_ids)\n    )\n    logp_eos = torch.logsumexp(logp_next[self.token_maps.eos_idxs], dim=0).item()\n    return logp_context + logp_eos\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Get log probabilities for next tokens given the prompt and <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>List[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Log probabilities for next tokens and EOS.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Get log probabilities for next tokens given the prompt and `context`.\n\n    Args:\n        context (List[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (LazyWeights): Log probabilities for next tokens and EOS.\n    \"\"\"\n    logw_next = self._maybe_temper(\n        await self.model.next_token_logprobs(\n            self.prompt_ids + self.encode_tokens(context)\n        )\n    )\n    return self._process_logw_next(logw_next)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Get log probabilities for next tokens given the prompt and <code>context</code>, for a batch of contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list[list[bytes]]</code> <p>A list of sequences of bytes tokens.</p> required <p>Returns:</p> Type Description <code>List[LazyWeights]</code> <p>Log probabilities for next tokens and EOS for each context.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"Get log probabilities for next tokens given the prompt and `context`, for a batch of contexts.\n\n    Args:\n        contexts (list[list[bytes]]): A list of sequences of bytes tokens.\n\n    Returns:\n        (List[LazyWeights]): Log probabilities for next tokens and EOS for each context.\n    \"\"\"\n    logw_nexts = self._maybe_temper(\n        await self.model.batch_next_token_logprobs(\n            [self.prompt_ids + self.encode_tokens(context) for context in contexts]\n        )\n    )\n    return [\n        self._process_logw_next(logw_next)\n        for logw_next in logw_nexts\n    ]\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new PromptedLLM with the same prompt and eos tokens.</p> <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>A new PromptedLLM with the same prompt and eos tokens.</p> Note <p>This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def spawn(self):\n    \"\"\"\n    Spawn a new PromptedLLM with the same prompt and eos tokens.\n\n    Returns:\n        (PromptedLLM): A new PromptedLLM with the same prompt and eos tokens.\n\n    Note:\n        This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.\n    \"\"\"\n    return PromptedLLM(\n        self.model,\n        prompt_ids=self.prompt_ids.copy(),\n        eos_tokens=self._eos_tokens.copy(),\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.PromptedLLM.spawn_new_eos","title":"<code>spawn_new_eos(eos_tokens)</code>","text":"<p>Create a new PromptedLLM with a different set of end-of-sequence tokens.</p> <p>Parameters:</p> Name Type Description Default <code>eos_tokens</code> <code>list[bytes]</code> <p>A list of tokens to treat as end-of-sequence tokens.</p> required <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>A new PromptedLLM with the specified end-of-sequence tokens. The new model will have the same prompt_ids as <code>self</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def spawn_new_eos(self, eos_tokens):\n    \"\"\"\n    Create a new PromptedLLM with a different set of end-of-sequence tokens.\n\n    Args:\n        eos_tokens (list[bytes]): A list of tokens to treat as end-of-sequence tokens.\n\n    Returns:\n        (PromptedLLM): A new PromptedLLM with the specified end-of-sequence tokens.\n            The new model will have the same prompt_ids as `self`.\n    \"\"\"\n    return PromptedLLM(\n        self.model,\n        prompt_ids=self.prompt_ids.copy(),\n        eos_tokens=eos_tokens.copy(),\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WCFG","title":"<code>WCFG</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A weighted context-free grammar potential.</p> <p>This class wraps a <code>genlm_grammar.CFG</code> and provides methods for computing the log-weight of a sequence, the prefix log-weight of a sequence, and the log-weights of the next token given a sequence.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>class WCFG(Potential):\n    \"\"\"\n    A weighted context-free grammar potential.\n\n    This class wraps a `genlm_grammar.CFG` and provides methods for computing the log-weight of a sequence,\n    the prefix log-weight of a sequence, and the log-weights of the next token given a sequence.\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Initialize the WCFG potential.\n\n        Args:\n            cfg (genlm_grammar.CFG): The context-free grammar configuration to use.\n                The CFG must in the Float semiring.\n        \"\"\"\n        # TODO: convert to LogSemiring to handle underflow\n        if cfg.R is not Float:\n            raise ValueError(\"cfg semiring must be Float\")\n        self.cfg = cfg  # cfg before prefix transform\n        self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n        self.model = Earley(self.cfg_eos.prefix_grammar)\n        super().__init__(vocabulary=list(cfg.V))\n\n    @classmethod\n    def from_string(cls, grammar, to_bytes=True, **kwargs):\n        \"\"\"Create a WCFG from a string.\n\n        Args:\n            grammar (str): The string grammar specification to create the WCFG from.\n            to_bytes (bool, optional): Whether to convert the WCFG terminals to indivudual bytes.\n                Defaults to True.\n            **kwargs (dict): Additional arguments passed to the WCFG constructor.\n\n        Returns:\n            (WCFG): The created WCFG.\n        \"\"\"\n        cfg = CFG.from_string(grammar, Float)\n        if to_bytes:\n            cfg = cfg.to_bytes()\n        return cls(cfg, **kwargs)\n\n    async def complete(self, context):\n        \"\"\"\n        Compute the log weight of `context` under the WCFG.\n\n        For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\\n\n        - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (float): The log weight of `context` under the WCFG.\n        \"\"\"\n        w = self.model([*context, EOS])\n        return np.log(w) if w &gt; 0 else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Compute the log prefix weight of `context` under the WCFG.\n\n        This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n        For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n        - `prefix(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `prefix(\"d\")` returns $-\\\\infty$ since the WCFG does not accept any sequences with prefix \"d\"\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (float): The log prefix weight of `context` under the WCFG.\n        \"\"\"\n        w = self.model(context)\n        return np.log(w) if w &gt; 0 else float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute the next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (LazyWeights): The log weights for the next tokens and EOS given `context`.\n        \"\"\"\n        ws = self.model.next_token_weights(self.model.chart(context))\n        ws = ws.trim().normalize()\n\n        ws_array = np.array([ws[x] for x in self.vocab_eos])\n        mask = ws_array &gt; 0\n        log_ws = np.full_like(ws_array, float(\"-inf\"), dtype=np.float64)\n        log_ws[mask] = np.log(ws_array[mask])\n\n        return self.make_lazy_weights(log_ws)\n\n    def clear_cache(self):\n        \"\"\"Clear the internal cache of the parser.\"\"\"\n        self.model.clear_cache()\n\n    def __repr__(self):\n        return f\"WCFG(cfg={self.cfg!r})\"\n\n    def _repr_html_(self):\n        return self.cfg._repr_html_()\n\n    def spawn(self):\n        \"\"\"Spawn a new WCFG.\"\"\"\n        return WCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WCFG.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Initialize the WCFG potential.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CFG</code> <p>The context-free grammar configuration to use. The CFG must in the Float semiring.</p> required Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def __init__(self, cfg):\n    \"\"\"\n    Initialize the WCFG potential.\n\n    Args:\n        cfg (genlm_grammar.CFG): The context-free grammar configuration to use.\n            The CFG must in the Float semiring.\n    \"\"\"\n    # TODO: convert to LogSemiring to handle underflow\n    if cfg.R is not Float:\n        raise ValueError(\"cfg semiring must be Float\")\n    self.cfg = cfg  # cfg before prefix transform\n    self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n    self.model = Earley(self.cfg_eos.prefix_grammar)\n    super().__init__(vocabulary=list(cfg.V))\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WCFG.from_string","title":"<code>from_string(grammar, to_bytes=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a WCFG from a string.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The string grammar specification to create the WCFG from.</p> required <code>to_bytes</code> <code>bool</code> <p>Whether to convert the WCFG terminals to indivudual bytes. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the WCFG constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>WCFG</code> <p>The created WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>@classmethod\ndef from_string(cls, grammar, to_bytes=True, **kwargs):\n    \"\"\"Create a WCFG from a string.\n\n    Args:\n        grammar (str): The string grammar specification to create the WCFG from.\n        to_bytes (bool, optional): Whether to convert the WCFG terminals to indivudual bytes.\n            Defaults to True.\n        **kwargs (dict): Additional arguments passed to the WCFG constructor.\n\n    Returns:\n        (WCFG): The created WCFG.\n    \"\"\"\n    cfg = CFG.from_string(grammar, Float)\n    if to_bytes:\n        cfg = cfg.to_bytes()\n    return cls(cfg, **kwargs)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WCFG.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Compute the log weight of <code>context</code> under the WCFG.</p> <p>For example, if the WCFG accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>complete(\"c\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WCFG</p> </li> <li> <p><code>complete(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>complete(\"d\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WCFG</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log weight of <code>context</code> under the WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Compute the log weight of `context` under the WCFG.\n\n    For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\\n\n    - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (float): The log weight of `context` under the WCFG.\n    \"\"\"\n    w = self.model([*context, EOS])\n    return np.log(w) if w &gt; 0 else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WCFG.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Compute the log prefix weight of <code>context</code> under the WCFG.</p> <p>This corresponds to the log of the sum of the weights of all sequences with prefix <code>context</code>.</p> <p>For example, if the WCFG accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>prefix(\"c\")</code> returns \\(\\log(w_{cat} + w_{car})\\)</p> </li> <li> <p><code>prefix(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>prefix(\"d\")</code> returns \\(-\\infty\\) since the WCFG does not accept any sequences with prefix \"d\"</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log prefix weight of <code>context</code> under the WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Compute the log prefix weight of `context` under the WCFG.\n\n    This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n    For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n    - `prefix(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `prefix(\"d\")` returns $-\\\\infty$ since the WCFG does not accept any sequences with prefix \"d\"\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (float): The log prefix weight of `context` under the WCFG.\n    \"\"\"\n    w = self.model(context)\n    return np.log(w) if w &gt; 0 else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WCFG.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>The log weights for the next tokens and EOS given <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute the next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (LazyWeights): The log weights for the next tokens and EOS given `context`.\n    \"\"\"\n    ws = self.model.next_token_weights(self.model.chart(context))\n    ws = ws.trim().normalize()\n\n    ws_array = np.array([ws[x] for x in self.vocab_eos])\n    mask = ws_array &gt; 0\n    log_ws = np.full_like(ws_array, float(\"-inf\"), dtype=np.float64)\n    log_ws[mask] = np.log(ws_array[mask])\n\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WCFG.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the internal cache of the parser.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the internal cache of the parser.\"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WCFG.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def spawn(self):\n    \"\"\"Spawn a new WCFG.\"\"\"\n    return WCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolCFG","title":"<code>BoolCFG</code>","text":"<p>               Bases: <code>Potential</code></p> <p>BoolCFG represents a boolean context-free grammar.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>class BoolCFG(Potential):\n    \"\"\"BoolCFG represents a boolean context-free grammar.\"\"\"\n\n    def __init__(self, cfg):\n        if cfg.R != Boolean:\n            cfg = cfg.map_values(lambda x: Boolean(x &gt; 0), Boolean)\n        self.cfg = cfg  # cfg before prefix transform\n        self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n        self.model = Earley(self.cfg_eos.prefix_grammar)\n        super().__init__(vocabulary=list(cfg.V))\n\n    @classmethod\n    def from_lark(cls, lark_string, charset=\"core\"):\n        \"\"\"\n        Create a BoolCFG instance from a Lark grammar string.\n\n        The output grammar will be defined at the byte-level.\n\n        Args:\n            lark_string (str): The Lark grammar string to parse. See Lark documentation for correct syntax.\n            charset (str): The character set to use. Defaults to \"core\".\n                See `genlm-grammar` documentation for more details.\n\n        Returns:\n            (BoolCFG): An instance of BoolCFG created from the provided Lark grammar.\n        \"\"\"\n        byte_cfg = LarkStuff(lark_string).byte_cfg(charset=charset)\n        return cls(byte_cfg)\n\n    async def complete(self, context):\n        \"\"\"\n        Checks whether the context is accepted by the CFG.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (float): Log weight for whether `context` is accepted by the CFG.\n        \"\"\"\n        w = self.model([*context, EOS])\n        return 0 if w.score else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Checks whether `context` is accepted as a prefix by the CFG, i.e.,\n        whether there exists a completion to `context` that is accepted by the CFG.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (float): Log weight for whether `context` is accepted as a prefix by the CFG.\n        \"\"\"\n        if not context:  # FIX: this is a hack to handle the empty string because genlm-grammar doesn't support it\n            return 0\n        w = self.model(context)\n        return 0 if w.score else float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute the next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (LazyWeights): The log weights for the next tokens and EOS given `context`.\n        \"\"\"\n        ws = self.model.next_token_weights(self.model.chart(context))\n        log_ws = np.array([0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos])\n        return self.make_lazy_weights(log_ws)\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"\n        Batch version of `logw_next`.\n\n        Args:\n            contexts (list): A list of sequences of tokens in the CFG's alphabet.\n\n        Returns:\n            (list): A list of log-weights for next token, one per context.\n        \"\"\"\n        Ws = []\n        for context in contexts:\n            ws = self.model.next_token_weights(self.model.chart(context))\n            log_ws = np.array(\n                [0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos]\n            )\n            Ws.append(self.make_lazy_weights(log_ws))\n        return Ws\n\n    def spawn(self):\n        \"\"\"Spawn a new BoolCFG.\"\"\"\n        return BoolCFG(self.cfg)\n\n    def clear_cache(self):\n        \"\"\"Clear the internal cache of the parser.\"\"\"\n        self.model.clear_cache()\n\n    def __repr__(self):\n        return f\"BoolCFG(cfg={self.cfg!r})\"\n\n    def _repr_html_(self):\n        return self.cfg._repr_html_()\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolCFG.from_lark","title":"<code>from_lark(lark_string, charset='core')</code>  <code>classmethod</code>","text":"<p>Create a BoolCFG instance from a Lark grammar string.</p> <p>The output grammar will be defined at the byte-level.</p> <p>Parameters:</p> Name Type Description Default <code>lark_string</code> <code>str</code> <p>The Lark grammar string to parse. See Lark documentation for correct syntax.</p> required <code>charset</code> <code>str</code> <p>The character set to use. Defaults to \"core\". See <code>genlm-grammar</code> documentation for more details.</p> <code>'core'</code> <p>Returns:</p> Type Description <code>BoolCFG</code> <p>An instance of BoolCFG created from the provided Lark grammar.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>@classmethod\ndef from_lark(cls, lark_string, charset=\"core\"):\n    \"\"\"\n    Create a BoolCFG instance from a Lark grammar string.\n\n    The output grammar will be defined at the byte-level.\n\n    Args:\n        lark_string (str): The Lark grammar string to parse. See Lark documentation for correct syntax.\n        charset (str): The character set to use. Defaults to \"core\".\n            See `genlm-grammar` documentation for more details.\n\n    Returns:\n        (BoolCFG): An instance of BoolCFG created from the provided Lark grammar.\n    \"\"\"\n    byte_cfg = LarkStuff(lark_string).byte_cfg(charset=charset)\n    return cls(byte_cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolCFG.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Checks whether the context is accepted by the CFG.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight for whether <code>context</code> is accepted by the CFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Checks whether the context is accepted by the CFG.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (float): Log weight for whether `context` is accepted by the CFG.\n    \"\"\"\n    w = self.model([*context, EOS])\n    return 0 if w.score else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolCFG.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Checks whether <code>context</code> is accepted as a prefix by the CFG, i.e., whether there exists a completion to <code>context</code> that is accepted by the CFG.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight for whether <code>context</code> is accepted as a prefix by the CFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Checks whether `context` is accepted as a prefix by the CFG, i.e.,\n    whether there exists a completion to `context` that is accepted by the CFG.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (float): Log weight for whether `context` is accepted as a prefix by the CFG.\n    \"\"\"\n    if not context:  # FIX: this is a hack to handle the empty string because genlm-grammar doesn't support it\n        return 0\n    w = self.model(context)\n    return 0 if w.score else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolCFG.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>The log weights for the next tokens and EOS given <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute the next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (LazyWeights): The log weights for the next tokens and EOS given `context`.\n    \"\"\"\n    ws = self.model.next_token_weights(self.model.chart(context))\n    log_ws = np.array([0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos])\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolCFG.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Batch version of <code>logw_next</code>.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>A list of sequences of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of log-weights for next token, one per context.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"\n    Batch version of `logw_next`.\n\n    Args:\n        contexts (list): A list of sequences of tokens in the CFG's alphabet.\n\n    Returns:\n        (list): A list of log-weights for next token, one per context.\n    \"\"\"\n    Ws = []\n    for context in contexts:\n        ws = self.model.next_token_weights(self.model.chart(context))\n        log_ws = np.array(\n            [0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos]\n        )\n        Ws.append(self.make_lazy_weights(log_ws))\n    return Ws\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolCFG.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new BoolCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def spawn(self):\n    \"\"\"Spawn a new BoolCFG.\"\"\"\n    return BoolCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolCFG.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the internal cache of the parser.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the internal cache of the parser.\"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WFSA","title":"<code>WFSA</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A weighted finite state automaton (WFSA) potential.</p> <p>This class wraps a <code>genlm_grammar.WFSA</code> and provides methods for computing the log-weight of a context, the prefix log-weight of a context, and the log-weights of the next token given a context.</p> <p>Attributes:</p> Name Type Description <code>wfsa</code> <code>WFSA</code> <p>The weighted finite state automaton used for potential calculations.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>class WFSA(Potential):\n    \"\"\"\n    A weighted finite state automaton (WFSA) potential.\n\n    This class wraps a `genlm_grammar.WFSA` and provides methods for computing the log-weight of a context,\n    the prefix log-weight of a context, and the log-weights of the next token given a context.\n\n    Attributes:\n        wfsa (genlm_grammar.WFSA): The weighted finite state automaton used for potential calculations.\n    \"\"\"\n\n    def __init__(self, wfsa):\n        \"\"\"\n        Initializes the WFSA potential.\n\n        Args:\n            wfsa (genlm_grammar.WFSA): The weighted finite state automaton.\n\n        Raises:\n            ValueError: If the semiring of the provided WFSA is not Float or Log.\n\n        Note:\n            The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.\n        \"\"\"\n        if wfsa.R not in (Float, Log):\n            raise ValueError(f\"Unsupported semiring: {wfsa.R}\")\n\n        if wfsa.R is Float:\n            self.wfsa = self._convert_to_log(wfsa)\n        else:\n            self.wfsa = wfsa\n\n        self.cache = {(): self.wfsa.epsremove.start}\n        super().__init__(vocabulary=list(self.wfsa.alphabet))\n\n    @classmethod\n    def from_regex(cls, pattern, charset=None, to_bytes=True):\n        \"\"\"\n        Create a WFSA from a regex pattern.\n\n        Args:\n            pattern (str): The regex pattern to convert into a WFSA.\n            charset (set): The character set to use for negative character classes.\n                Defaults to characters in string.printable.\n            to_bytes (bool): Whether to convert the WFSA transitions to bytes.\n                Defaults to True. When set to False, the WFSA transitions will be strings.\n\n        Returns:\n            (WFSA): An instance of the WFSA class.\n\n        Note:\n            The transition weights are automatically normalized to form a probability distribution.\n            For each state, the weights of all outgoing transitions (including final state transitions)\n            sum to 1.0. This means if a state has n possible transitions, each transition will have\n            weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use `BoolFSA`.\n        \"\"\"\n        charset = charset or set(string.printable)\n        wfsa = interegular_to_wfsa(pattern, charset=charset)\n        if to_bytes:\n            wfsa = wfsa.to_bytes()\n        return cls(wfsa=wfsa)\n\n    @staticmethod\n    def _convert_to_log(wfsa):\n        \"\"\"Convert a WFSA from the Float semiring to the Log semiring.\"\"\"\n        assert wfsa.R is Float\n        assert isinstance(wfsa, BaseWFSA)\n        new = BaseWFSA(Log)\n\n        for i, w in wfsa.I:\n            new.add_I(i, Log(np.log(w)))\n\n        for i, w in wfsa.F:\n            new.add_F(i, Log(np.log(w)))\n\n        for i, a, j, w in wfsa.arcs():\n            new.add_arc(i, a, j, Log(np.log(w)))\n\n        return new\n\n    def _consume(self, bs):\n        # XXX implement cache eviction\n        bs = tuple(bs)\n\n        try:\n            return self.cache[bs]\n        except KeyError:\n            pass\n\n        wfsa = self.wfsa.epsremove\n        curr = wfsa.R.chart()\n        prev = self._consume(bs[:-1])\n        for i in prev:\n            for j, w in wfsa.arcs(i, bs[-1]):\n                curr[j] += prev[i] * w\n\n        self.cache[bs] = curr\n\n        return curr\n\n    async def complete(self, context):\n        \"\"\"\n        Computes the log weight of the context under the weighted language represented by the WFSA.\n\n        For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\\n\n        - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): Log weight of context under the WFSA.\n        \"\"\"\n        # TODO: optimize to use _consume cache\n        return self.wfsa(context).score\n\n    def _prefix(self, context):\n        curr = self._consume(context)\n\n        if not curr:\n            return float(\"-inf\"), curr\n\n        bkwd = self.wfsa.epsremove.backward\n        log_ctx_w = logsumexp([(curr[i] * bkwd[i]).score for i in curr])\n\n        if np.isnan(log_ctx_w):\n            return float(\"-inf\"), curr\n\n        return log_ctx_w, curr\n\n    async def prefix(self, context):\n        \"\"\"\n        Computes the prefix log weight of `context` under the WFSA.\n\n        This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n        For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n        - `prefix(\"ca\")` returns $\\\\log(w_{cat})$\\n\n        - `prefix(\"d\")` returns $-\\\\infty$ since the WFSA does not accept any sequences with prefix \"d\"\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): Log weight of `context` as a prefix under the WFSA.\n        \"\"\"\n        return self._prefix(context)[0]\n\n    async def logw_next(self, context):\n        \"\"\"Returns next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (LazyWeights): Log-weights for next token and EOS.\n        \"\"\"\n        log_ctx_w, curr = self._prefix(context)\n\n        if log_ctx_w == float(\"-inf\"):\n            raise ValueError(f\"Context {context!r} has zero weight.\")\n\n        bkwd = self.wfsa.epsremove.backward\n\n        ws = self.wfsa.R.chart()\n        for i in curr:\n            for b, j, w in self.wfsa.epsremove.arcs(i=i):\n                ws[b] += curr[i] * w * bkwd[j]\n\n        ws[self.eos] = self.wfsa.R.zero\n        for j, w in self.wfsa.epsremove.F:\n            ws[self.eos] += curr[j] * w\n\n        log_ws = np.array([ws[b].score for b in self.vocab_eos]) - log_ctx_w\n\n        return self.make_lazy_weights(log_ws)\n\n    def _repr_svg_(self):\n        return self.wfsa._repr_svg_()\n\n    def __repr__(self):\n        return f\"WFSA(wfsa={self.wfsa!r})\"\n\n    def spawn(self):\n        cls = type(self)\n        return cls(wfsa=self.wfsa)\n\n    def clear_cache(self):\n        self.cache = {(): self.wfsa.epsremove.start}\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WFSA.__init__","title":"<code>__init__(wfsa)</code>","text":"<p>Initializes the WFSA potential.</p> <p>Parameters:</p> Name Type Description Default <code>wfsa</code> <code>WFSA</code> <p>The weighted finite state automaton.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the semiring of the provided WFSA is not Float or Log.</p> Note <p>The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>def __init__(self, wfsa):\n    \"\"\"\n    Initializes the WFSA potential.\n\n    Args:\n        wfsa (genlm_grammar.WFSA): The weighted finite state automaton.\n\n    Raises:\n        ValueError: If the semiring of the provided WFSA is not Float or Log.\n\n    Note:\n        The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.\n    \"\"\"\n    if wfsa.R not in (Float, Log):\n        raise ValueError(f\"Unsupported semiring: {wfsa.R}\")\n\n    if wfsa.R is Float:\n        self.wfsa = self._convert_to_log(wfsa)\n    else:\n        self.wfsa = wfsa\n\n    self.cache = {(): self.wfsa.epsremove.start}\n    super().__init__(vocabulary=list(self.wfsa.alphabet))\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WFSA.from_regex","title":"<code>from_regex(pattern, charset=None, to_bytes=True)</code>  <code>classmethod</code>","text":"<p>Create a WFSA from a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regex pattern to convert into a WFSA.</p> required <code>charset</code> <code>set</code> <p>The character set to use for negative character classes. Defaults to characters in string.printable.</p> <code>None</code> <code>to_bytes</code> <code>bool</code> <p>Whether to convert the WFSA transitions to bytes. Defaults to True. When set to False, the WFSA transitions will be strings.</p> <code>True</code> <p>Returns:</p> Type Description <code>WFSA</code> <p>An instance of the WFSA class.</p> Note <p>The transition weights are automatically normalized to form a probability distribution. For each state, the weights of all outgoing transitions (including final state transitions) sum to 1.0. This means if a state has n possible transitions, each transition will have weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use <code>BoolFSA</code>.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>@classmethod\ndef from_regex(cls, pattern, charset=None, to_bytes=True):\n    \"\"\"\n    Create a WFSA from a regex pattern.\n\n    Args:\n        pattern (str): The regex pattern to convert into a WFSA.\n        charset (set): The character set to use for negative character classes.\n            Defaults to characters in string.printable.\n        to_bytes (bool): Whether to convert the WFSA transitions to bytes.\n            Defaults to True. When set to False, the WFSA transitions will be strings.\n\n    Returns:\n        (WFSA): An instance of the WFSA class.\n\n    Note:\n        The transition weights are automatically normalized to form a probability distribution.\n        For each state, the weights of all outgoing transitions (including final state transitions)\n        sum to 1.0. This means if a state has n possible transitions, each transition will have\n        weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use `BoolFSA`.\n    \"\"\"\n    charset = charset or set(string.printable)\n    wfsa = interegular_to_wfsa(pattern, charset=charset)\n    if to_bytes:\n        wfsa = wfsa.to_bytes()\n    return cls(wfsa=wfsa)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WFSA.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Computes the log weight of the context under the weighted language represented by the WFSA.</p> <p>For example, if the WFSA accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>complete(\"c\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WFSA</p> </li> <li> <p><code>complete(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>complete(\"d\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WFSA</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of context under the WFSA.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Computes the log weight of the context under the weighted language represented by the WFSA.\n\n    For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\\n\n    - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): Log weight of context under the WFSA.\n    \"\"\"\n    # TODO: optimize to use _consume cache\n    return self.wfsa(context).score\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WFSA.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Computes the prefix log weight of <code>context</code> under the WFSA.</p> <p>This corresponds to the log of the sum of the weights of all sequences with prefix <code>context</code>.</p> <p>For example, if the WFSA accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>prefix(\"c\")</code> returns \\(\\log(w_{cat} + w_{car})\\)</p> </li> <li> <p><code>prefix(\"ca\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>prefix(\"d\")</code> returns \\(-\\infty\\) since the WFSA does not accept any sequences with prefix \"d\"</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of <code>context</code> as a prefix under the WFSA.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Computes the prefix log weight of `context` under the WFSA.\n\n    This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n    For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n    - `prefix(\"ca\")` returns $\\\\log(w_{cat})$\\n\n    - `prefix(\"d\")` returns $-\\\\infty$ since the WFSA does not accept any sequences with prefix \"d\"\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): Log weight of `context` as a prefix under the WFSA.\n    \"\"\"\n    return self._prefix(context)[0]\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.WFSA.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Returns next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Log-weights for next token and EOS.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Returns next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (LazyWeights): Log-weights for next token and EOS.\n    \"\"\"\n    log_ctx_w, curr = self._prefix(context)\n\n    if log_ctx_w == float(\"-inf\"):\n        raise ValueError(f\"Context {context!r} has zero weight.\")\n\n    bkwd = self.wfsa.epsremove.backward\n\n    ws = self.wfsa.R.chart()\n    for i in curr:\n        for b, j, w in self.wfsa.epsremove.arcs(i=i):\n            ws[b] += curr[i] * w * bkwd[j]\n\n    ws[self.eos] = self.wfsa.R.zero\n    for j, w in self.wfsa.epsremove.F:\n        ws[self.eos] += curr[j] * w\n\n    log_ws = np.array([ws[b].score for b in self.vocab_eos]) - log_ctx_w\n\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolFSA","title":"<code>BoolFSA</code>","text":"<p>               Bases: <code>WFSA</code></p> <p>Boolean FSA potential.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>class BoolFSA(WFSA):\n    \"\"\"Boolean FSA potential.\"\"\"\n\n    async def prefix(self, context):\n        \"\"\"\n        Computes whether the context is accepted as a prefix by the FSA.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): `0` if the context is accepted as a prefix, `-inf` otherwise.\n        \"\"\"\n        prefix_w = await super().prefix(context)\n        if prefix_w &gt; float(\"-inf\"):\n            return 0\n        return float(\"-inf\")\n\n    async def complete(self, context):\n        \"\"\"\n        Computes whether the context is accepted by the FSA.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): `0` if the context is accepted, `-inf` otherwise.\n        \"\"\"\n        complete_w = await super().complete(context)\n        if complete_w &gt; float(\"-inf\"):\n            return 0\n        return float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Returns next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (LazyWeights): Boolean log-weights for next token.\n        \"\"\"\n        logw_next = await super().logw_next(context)\n        return logw_next.spawn(\n            new_weights=np.where(\n                logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n            )\n        )\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"\n        Returns next token log weights for a batch of contexts.\n\n        Args:\n            contexts (list): The list of contexts.\n\n        Returns:\n            (list): List of log-weights for next token, one per context.\n        \"\"\"\n        logw_nexts = await super().batch_logw_next(contexts)\n        return [\n            logw_next.spawn(\n                new_weights=np.where(\n                    logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n                )\n            )\n            for logw_next in logw_nexts\n        ]\n\n    def __repr__(self):\n        return f\"BoolFSA(wfsa={self.wfsa!r})\"\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolFSA.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Computes whether the context is accepted as a prefix by the FSA.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p><code>0</code> if the context is accepted as a prefix, <code>-inf</code> otherwise.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Computes whether the context is accepted as a prefix by the FSA.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): `0` if the context is accepted as a prefix, `-inf` otherwise.\n    \"\"\"\n    prefix_w = await super().prefix(context)\n    if prefix_w &gt; float(\"-inf\"):\n        return 0\n    return float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolFSA.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Computes whether the context is accepted by the FSA.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p><code>0</code> if the context is accepted, <code>-inf</code> otherwise.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Computes whether the context is accepted by the FSA.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): `0` if the context is accepted, `-inf` otherwise.\n    \"\"\"\n    complete_w = await super().complete(context)\n    if complete_w &gt; float(\"-inf\"):\n        return 0\n    return float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolFSA.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Returns next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Boolean log-weights for next token.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Returns next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (LazyWeights): Boolean log-weights for next token.\n    \"\"\"\n    logw_next = await super().logw_next(context)\n    return logw_next.spawn(\n        new_weights=np.where(\n            logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n        )\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.BoolFSA.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Returns next token log weights for a batch of contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>The list of contexts.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of log-weights for next token, one per context.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"\n    Returns next token log weights for a batch of contexts.\n\n    Args:\n        contexts (list): The list of contexts.\n\n    Returns:\n        (list): List of log-weights for next token, one per context.\n    \"\"\"\n    logw_nexts = await super().batch_logw_next(contexts)\n    return [\n        logw_next.spawn(\n            new_weights=np.where(\n                logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n            )\n        )\n        for logw_next in logw_nexts\n    ]\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.CanonicalTokenization","title":"<code>CanonicalTokenization</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A custom potential that enforces canonical BPE tokenization.</p> <p>This potential ensures that tokens follow the canonical tokenization rules by using the FastCanonicalityFilterBPE under the hood.</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>class CanonicalTokenization(Potential):\n    \"\"\"\n    A custom potential that enforces canonical BPE tokenization.\n\n    This potential ensures that tokens follow the canonical tokenization rules\n    by using the FastCanonicalityFilterBPE under the hood.\n    \"\"\"\n\n    def __init__(self, canonicality_filter):\n        \"\"\"\n        Initialize the Canonical Potential\n\n        Args:\n            canonicality_filter (FastCanonicalityFilterBPE): An initialized FastCanonicalityFilterBPE instance.\n        \"\"\"\n        # Store the pre-initialized filter and tokenizer\n        self.canonicality_filter = canonicality_filter\n\n        # IMPORTANT: In the base Potential class, EOS will be added to vocab automatically\n        # So we should NOT add it ourselves to the vocabulary we pass to super().__init__\n        vocabulary = self.canonicality_filter._decode\n        super().__init__(vocabulary)\n\n    @classmethod\n    def from_llm(cls, llm):\n        \"\"\"\n        Factory method to create CanonicalTokenization from a PromptedLLM instance.\n\n        Args:\n            llm (PromptedLLM): An instance of PromptedLLM containing the model and tokenizer.\n\n        Returns:\n            (CanonicalTokenization): An initialized CanonicalTokenization instance.\n        \"\"\"\n        if not isinstance(llm, PromptedLLM):\n            raise TypeError(\n                f\"Expected llm to be an instance of PromptedLLM, got {type(llm)}\"\n            )\n\n        # Extract necessary components from llm\n        tokenizer = llm.model.tokenizer\n        eos_token_ids = llm.token_maps.eos_idxs\n        model_name = tokenizer.name_or_path\n\n        # Create the filter using its factory method\n        canonicality_filter = FastCanonicalityFilterBPE.from_tokenizer(\n            tokenizer, eos_token_ids\n        )\n\n        # Set overrides on the filter\n        canonicality_filter.set_overrides(model_name)\n\n        # Call __init__ with the created filter and tokenizer\n        return cls(canonicality_filter)\n\n    async def complete(self, context):\n        \"\"\"\n        Assess if a complete sequence follows canonical tokenization.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (float): 0.0 if canonical, float('-inf') otherwise\n        \"\"\"\n        # Empty sequences are considered canonical\n        if not context:\n            return 0.0\n\n        # Check if the sequence is canonical\n        is_canonical = self._check_canonicality(context)\n        return 0.0 if is_canonical else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Assess if a prefix sequence could potentially extend to a canonical sequence.\n        For canonicality, this is the same as complete.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (float): 0.0 if potentially canonical, float('-inf') otherwise\n        \"\"\"\n        return await self.complete(context)\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute weights for each possible next token given the context.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (LazyWeights): Weights for each token in the vocabulary and EOS\n        \"\"\"\n        # Get the prefix weight (to check if context itself is canonical)\n        ctx_log_w = await self.prefix(context)\n\n        if ctx_log_w == float(\"-inf\"):\n            raise ValueError(\"Context is non-canonical\")\n        else:\n            if context:\n                t = (None, context[-1])\n                filter_mask = self.canonicality_filter(t)\n            else:\n                filter_mask = np.ones(len(self.canonicality_filter._decode), dtype=bool)\n\n            # Create log weights directly instead of using np.log(filter_mask)\n            # This is more efficient, avoids torch (with torch can't combine with other potentials!)\n            logws_no_eos = np.where(filter_mask, 0.0, float(\"-inf\")).astype(np.float32)\n\n            # append eos to the logws, always allow eos.\n            # NOTE: concat is because ._decode does not include eos while .vocab_eos does\n            logws = np.concatenate([logws_no_eos, np.array([0.0], dtype=np.float32)])\n\n        return self.make_lazy_weights(logws)\n\n    def _check_canonicality(self, context):\n        \"\"\"\n        Check if a sequence follows canonical tokenization.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (bool): True if the sequence is canonical, False otherwise\n        \"\"\"\n        # If we're checking a single token, it's always canonical\n        if len(context) &lt;= 1:\n            return True\n\n        # Check all adjacent token pairs for canonicality\n        for i in range(1, len(context)):\n            prev_token = context[i - 1]\n            current_token = context[i]\n\n            # Format expected by the filter: (None, previous_token)\n            t = (None, prev_token)\n            mask = self.canonicality_filter(t)\n            # print(\"percent of mask: \", np.sum(mask)*100 / len(mask))\n\n            # Find token_id in the canonicality filter's vocabulary\n            token_id = self.canonicality_filter._encode[current_token]\n            if not mask[token_id]:\n                return False\n\n        return True\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.CanonicalTokenization.__init__","title":"<code>__init__(canonicality_filter)</code>","text":"<p>Initialize the Canonical Potential</p> <p>Parameters:</p> Name Type Description Default <code>canonicality_filter</code> <code>FastCanonicalityFilterBPE</code> <p>An initialized FastCanonicalityFilterBPE instance.</p> required Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>def __init__(self, canonicality_filter):\n    \"\"\"\n    Initialize the Canonical Potential\n\n    Args:\n        canonicality_filter (FastCanonicalityFilterBPE): An initialized FastCanonicalityFilterBPE instance.\n    \"\"\"\n    # Store the pre-initialized filter and tokenizer\n    self.canonicality_filter = canonicality_filter\n\n    # IMPORTANT: In the base Potential class, EOS will be added to vocab automatically\n    # So we should NOT add it ourselves to the vocabulary we pass to super().__init__\n    vocabulary = self.canonicality_filter._decode\n    super().__init__(vocabulary)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.CanonicalTokenization.from_llm","title":"<code>from_llm(llm)</code>  <code>classmethod</code>","text":"<p>Factory method to create CanonicalTokenization from a PromptedLLM instance.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>PromptedLLM</code> <p>An instance of PromptedLLM containing the model and tokenizer.</p> required <p>Returns:</p> Type Description <code>CanonicalTokenization</code> <p>An initialized CanonicalTokenization instance.</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>@classmethod\ndef from_llm(cls, llm):\n    \"\"\"\n    Factory method to create CanonicalTokenization from a PromptedLLM instance.\n\n    Args:\n        llm (PromptedLLM): An instance of PromptedLLM containing the model and tokenizer.\n\n    Returns:\n        (CanonicalTokenization): An initialized CanonicalTokenization instance.\n    \"\"\"\n    if not isinstance(llm, PromptedLLM):\n        raise TypeError(\n            f\"Expected llm to be an instance of PromptedLLM, got {type(llm)}\"\n        )\n\n    # Extract necessary components from llm\n    tokenizer = llm.model.tokenizer\n    eos_token_ids = llm.token_maps.eos_idxs\n    model_name = tokenizer.name_or_path\n\n    # Create the filter using its factory method\n    canonicality_filter = FastCanonicalityFilterBPE.from_tokenizer(\n        tokenizer, eos_token_ids\n    )\n\n    # Set overrides on the filter\n    canonicality_filter.set_overrides(model_name)\n\n    # Call __init__ with the created filter and tokenizer\n    return cls(canonicality_filter)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.CanonicalTokenization.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Assess if a complete sequence follows canonical tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>float</code> <p>0.0 if canonical, float('-inf') otherwise</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Assess if a complete sequence follows canonical tokenization.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (float): 0.0 if canonical, float('-inf') otherwise\n    \"\"\"\n    # Empty sequences are considered canonical\n    if not context:\n        return 0.0\n\n    # Check if the sequence is canonical\n    is_canonical = self._check_canonicality(context)\n    return 0.0 if is_canonical else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.CanonicalTokenization.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Assess if a prefix sequence could potentially extend to a canonical sequence. For canonicality, this is the same as complete.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>float</code> <p>0.0 if potentially canonical, float('-inf') otherwise</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Assess if a prefix sequence could potentially extend to a canonical sequence.\n    For canonicality, this is the same as complete.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (float): 0.0 if potentially canonical, float('-inf') otherwise\n    \"\"\"\n    return await self.complete(context)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/__init__/#genlm.control.potential.built_in.CanonicalTokenization.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute weights for each possible next token given the context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Weights for each token in the vocabulary and EOS</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute weights for each possible next token given the context.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (LazyWeights): Weights for each token in the vocabulary and EOS\n    \"\"\"\n    # Get the prefix weight (to check if context itself is canonical)\n    ctx_log_w = await self.prefix(context)\n\n    if ctx_log_w == float(\"-inf\"):\n        raise ValueError(\"Context is non-canonical\")\n    else:\n        if context:\n            t = (None, context[-1])\n            filter_mask = self.canonicality_filter(t)\n        else:\n            filter_mask = np.ones(len(self.canonicality_filter._decode), dtype=bool)\n\n        # Create log weights directly instead of using np.log(filter_mask)\n        # This is more efficient, avoids torch (with torch can't combine with other potentials!)\n        logws_no_eos = np.where(filter_mask, 0.0, float(\"-inf\")).astype(np.float32)\n\n        # append eos to the logws, always allow eos.\n        # NOTE: concat is because ._decode does not include eos while .vocab_eos does\n        logws = np.concatenate([logws_no_eos, np.array([0.0], dtype=np.float32)])\n\n    return self.make_lazy_weights(logws)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/canonical/","title":"canonical","text":""},{"location":"reference/genlm/control/potential/built_in/canonical/#genlm.control.potential.built_in.canonical.FastCanonicalityFilterBPE","title":"<code>FastCanonicalityFilterBPE</code>","text":"Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>class FastCanonicalityFilterBPE:\n    def __init__(self, _merges, _encode, _decode, _encode_byte, eos_token_ids):\n        self._encode_byte = _encode_byte\n        self._merges = _merges\n        self._encode = _encode\n        self._decode = _decode\n        self.V = len(_decode)  # token vocabulary size\n\n        # priority dict might still be useful if merges aren't strictly ordered\n        # or for potential future optimizations, keep it for now.\n        # self.priority = {(u, v): -i for i, (u, v, _) in enumerate(self._merges)}\n        self.make_derivation_table()  # Call the rewritten method\n\n        self.__left_spine, max_left_spine_width = self._left_spine_table()\n        self.__right_spine, max_right_spine_width = self._right_spine_table()\n\n        self.left_spine_vector = self.vectorize_spine(\n            self.__left_spine, max_left_spine_width\n        )\n        self.right_spine_vector = self.vectorize_spine(\n            self.__right_spine, max_right_spine_width\n        )\n\n        self.indices = np.array(\n            [\n                (index, j)\n                for index in range(self.V)\n                for j in range(len(self.__left_spine[index]) - 1)\n            ]\n        )\n\n        self.vector_r = self.left_spine_vector[self.indices[:, 0], self.indices[:, 1]]\n        self.vector_rp = self.left_spine_vector[\n            self.indices[:, 0], self.indices[:, 1] + 1\n        ]\n\n        tmp = sp.dok_matrix((self.V, self.V), dtype=np.int32)\n        for u, v, uv in _merges:\n            tmp[u, v] = uv + 1  # +1 to avoid zero-indexing\n\n        self.parent_l_matrix = tmp.tocsr()\n        self.parent_l_matrix = self.parent_l_matrix[:, self.vector_r]\n\n        self.eos_token_ids = set(eos_token_ids)\n        self.overrides = defaultdict(lambda: set())\n\n    def __call__(self, context):\n        if context == ():\n            mask = np.ones(self.V, dtype=bool)\n        else:\n            (_, last_token) = context\n            try:\n                left_id = self._encode[last_token]  # Get the ID of the last token\n            except KeyError as e:\n                raise KeyError(\n                    f\"Last token {last_token!r} not found in encode map.\"\n                ) from e\n\n            mask = self._vectorized_conflicting_next_tokens(\n                left_id\n            )  # Get base mask from BPE rules\n\n            # Apply overrides: Ensure overridden tokens are allowed (True)\n            if left_id in self.overrides:\n                override_ids = [oid for oid in self.overrides[left_id] if oid &lt; self.V]\n                mask[override_ids] = True\n\n            eos_indices = [e for e in self.eos_token_ids if e &lt; self.V]\n            mask[eos_indices] = True\n        return mask\n\n    def make_derivation_table(self):\n        # Initialize left and right child lookup tables\n        self._left = [None] * self.V\n        self._right = [None] * self.V\n\n        # Populate _left and _right based on the ordered merges\n        # Assumes self._merges is ordered by priority (highest priority first) because of the way we build it in extract_bpe_merges\n        for u, v, uv in self._merges:\n            # Only record the first (highest priority) merge that forms uv\n            if self._left[uv] is None and self._right[uv] is None:\n                self._left[uv] = u\n                self._right[uv] = v\n\n    def vectorize_spine(self, spine, max_spine_width):\n        new_spine = [\n            [s[i] if i &lt; len(s) else -VERYLARGE for i in range(max_spine_width)]\n            for s in spine\n        ]\n        return np.array(new_spine, dtype=np.int32)\n\n    def _left_spine_table(self):\n        \"Closure of the left tables.\"\n        max_width = 0\n        left_spine = [None] * self.V\n        left = self._left\n        for i in range(self.V):\n            spine = [VERYLARGE, i]\n            x = i\n            while True:\n                x = left[x]\n                if x is None:\n                    break\n                spine.append(x)\n            spine.reverse()\n            left_spine[i] = spine\n            max_width = max(max_width, len(spine))\n        return left_spine, max_width\n\n    def _right_spine_table(self):\n        \"Closure of the right tables.\"\n        max_width = 0\n        right_spine = [None] * self.V\n        right = self._right\n        for i in range(self.V):\n            spine = [VERYLARGE, i]\n            x = i\n            while True:\n                x = right[x]\n                if x is None:\n                    break\n                spine.append(x)\n            spine.reverse()\n            right_spine[i] = spine\n            max_width = max(max_width, len(spine))\n        return right_spine, max_width\n\n    def set_overrides(self, model_name):\n        if \"gpt2\" in model_name:\n            for left, right in [(198, 198), (2637, 82)]:\n                self.overrides[left].add(right)\n\n    def _vectorized_conflicting_next_tokens(self, left: int):\n        spine_left = self.__right_spine[left]\n\n        L = len(spine_left) - 1  # inf padding\n\n        mask = np.ones(self.V, dtype=bool)\n\n        np_matrix = self.parent_l_matrix[spine_left[:L]].toarray()\n\n        for i in range(L):\n            lp = spine_left[i + 1]\n\n            vector_k = np_matrix[i]\n            # convert 0 in vector_k to VERYLARGE\n            vector_k = np.where(vector_k != 0, vector_k - 1, VERYLARGE)\n\n            conflict_mask = vector_k &lt; VERYLARGE\n            conflict_mask &amp;= vector_k &lt;= self.vector_rp\n            conflict_mask &amp;= vector_k &lt; lp\n            mask[self.indices[conflict_mask][:, 0]] = False\n\n        return mask\n\n    @classmethod\n    def from_tokenizer(cls, tokenizer, eos_token_ids=None):\n        _decode, _ = decode_vocab(tokenizer)\n        if len(_decode) != len(set(_decode)):\n            raise ValueError(\n                \"Duplicate byte sequences found in vocabulary. Cannot create unique byte-&gt;ID mapping (_encode).\"\n            )\n\n        _merges = _extract_bpe_merges(tokenizer)\n\n        # Build _encode (bytes -&gt; token_id map) from _decode\n        _encode = {b: i for i, b in enumerate(_decode) if b is not None}\n\n        # Build _encode_byte (single byte -&gt; token_id map)\n        _encode_byte = [None] * 256\n        for i in range(256):\n            byte_val = bytes([i])\n            if byte_val in _encode:\n                _encode_byte[i] = _encode[byte_val]\n\n        if not eos_token_ids:\n            eos_token_ids = [tokenizer.eos_token_id]\n\n        return cls(_merges, _encode, _decode, _encode_byte, eos_token_ids)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/canonical/#genlm.control.potential.built_in.canonical.CanonicalTokenization","title":"<code>CanonicalTokenization</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A custom potential that enforces canonical BPE tokenization.</p> <p>This potential ensures that tokens follow the canonical tokenization rules by using the FastCanonicalityFilterBPE under the hood.</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>class CanonicalTokenization(Potential):\n    \"\"\"\n    A custom potential that enforces canonical BPE tokenization.\n\n    This potential ensures that tokens follow the canonical tokenization rules\n    by using the FastCanonicalityFilterBPE under the hood.\n    \"\"\"\n\n    def __init__(self, canonicality_filter):\n        \"\"\"\n        Initialize the Canonical Potential\n\n        Args:\n            canonicality_filter (FastCanonicalityFilterBPE): An initialized FastCanonicalityFilterBPE instance.\n        \"\"\"\n        # Store the pre-initialized filter and tokenizer\n        self.canonicality_filter = canonicality_filter\n\n        # IMPORTANT: In the base Potential class, EOS will be added to vocab automatically\n        # So we should NOT add it ourselves to the vocabulary we pass to super().__init__\n        vocabulary = self.canonicality_filter._decode\n        super().__init__(vocabulary)\n\n    @classmethod\n    def from_llm(cls, llm):\n        \"\"\"\n        Factory method to create CanonicalTokenization from a PromptedLLM instance.\n\n        Args:\n            llm (PromptedLLM): An instance of PromptedLLM containing the model and tokenizer.\n\n        Returns:\n            (CanonicalTokenization): An initialized CanonicalTokenization instance.\n        \"\"\"\n        if not isinstance(llm, PromptedLLM):\n            raise TypeError(\n                f\"Expected llm to be an instance of PromptedLLM, got {type(llm)}\"\n            )\n\n        # Extract necessary components from llm\n        tokenizer = llm.model.tokenizer\n        eos_token_ids = llm.token_maps.eos_idxs\n        model_name = tokenizer.name_or_path\n\n        # Create the filter using its factory method\n        canonicality_filter = FastCanonicalityFilterBPE.from_tokenizer(\n            tokenizer, eos_token_ids\n        )\n\n        # Set overrides on the filter\n        canonicality_filter.set_overrides(model_name)\n\n        # Call __init__ with the created filter and tokenizer\n        return cls(canonicality_filter)\n\n    async def complete(self, context):\n        \"\"\"\n        Assess if a complete sequence follows canonical tokenization.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (float): 0.0 if canonical, float('-inf') otherwise\n        \"\"\"\n        # Empty sequences are considered canonical\n        if not context:\n            return 0.0\n\n        # Check if the sequence is canonical\n        is_canonical = self._check_canonicality(context)\n        return 0.0 if is_canonical else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Assess if a prefix sequence could potentially extend to a canonical sequence.\n        For canonicality, this is the same as complete.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (float): 0.0 if potentially canonical, float('-inf') otherwise\n        \"\"\"\n        return await self.complete(context)\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute weights for each possible next token given the context.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (LazyWeights): Weights for each token in the vocabulary and EOS\n        \"\"\"\n        # Get the prefix weight (to check if context itself is canonical)\n        ctx_log_w = await self.prefix(context)\n\n        if ctx_log_w == float(\"-inf\"):\n            raise ValueError(\"Context is non-canonical\")\n        else:\n            if context:\n                t = (None, context[-1])\n                filter_mask = self.canonicality_filter(t)\n            else:\n                filter_mask = np.ones(len(self.canonicality_filter._decode), dtype=bool)\n\n            # Create log weights directly instead of using np.log(filter_mask)\n            # This is more efficient, avoids torch (with torch can't combine with other potentials!)\n            logws_no_eos = np.where(filter_mask, 0.0, float(\"-inf\")).astype(np.float32)\n\n            # append eos to the logws, always allow eos.\n            # NOTE: concat is because ._decode does not include eos while .vocab_eos does\n            logws = np.concatenate([logws_no_eos, np.array([0.0], dtype=np.float32)])\n\n        return self.make_lazy_weights(logws)\n\n    def _check_canonicality(self, context):\n        \"\"\"\n        Check if a sequence follows canonical tokenization.\n\n        Args:\n            context (list): Sequence of tokens\n\n        Returns:\n            (bool): True if the sequence is canonical, False otherwise\n        \"\"\"\n        # If we're checking a single token, it's always canonical\n        if len(context) &lt;= 1:\n            return True\n\n        # Check all adjacent token pairs for canonicality\n        for i in range(1, len(context)):\n            prev_token = context[i - 1]\n            current_token = context[i]\n\n            # Format expected by the filter: (None, previous_token)\n            t = (None, prev_token)\n            mask = self.canonicality_filter(t)\n            # print(\"percent of mask: \", np.sum(mask)*100 / len(mask))\n\n            # Find token_id in the canonicality filter's vocabulary\n            token_id = self.canonicality_filter._encode[current_token]\n            if not mask[token_id]:\n                return False\n\n        return True\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/canonical/#genlm.control.potential.built_in.canonical.CanonicalTokenization.__init__","title":"<code>__init__(canonicality_filter)</code>","text":"<p>Initialize the Canonical Potential</p> <p>Parameters:</p> Name Type Description Default <code>canonicality_filter</code> <code>FastCanonicalityFilterBPE</code> <p>An initialized FastCanonicalityFilterBPE instance.</p> required Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>def __init__(self, canonicality_filter):\n    \"\"\"\n    Initialize the Canonical Potential\n\n    Args:\n        canonicality_filter (FastCanonicalityFilterBPE): An initialized FastCanonicalityFilterBPE instance.\n    \"\"\"\n    # Store the pre-initialized filter and tokenizer\n    self.canonicality_filter = canonicality_filter\n\n    # IMPORTANT: In the base Potential class, EOS will be added to vocab automatically\n    # So we should NOT add it ourselves to the vocabulary we pass to super().__init__\n    vocabulary = self.canonicality_filter._decode\n    super().__init__(vocabulary)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/canonical/#genlm.control.potential.built_in.canonical.CanonicalTokenization.from_llm","title":"<code>from_llm(llm)</code>  <code>classmethod</code>","text":"<p>Factory method to create CanonicalTokenization from a PromptedLLM instance.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>PromptedLLM</code> <p>An instance of PromptedLLM containing the model and tokenizer.</p> required <p>Returns:</p> Type Description <code>CanonicalTokenization</code> <p>An initialized CanonicalTokenization instance.</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>@classmethod\ndef from_llm(cls, llm):\n    \"\"\"\n    Factory method to create CanonicalTokenization from a PromptedLLM instance.\n\n    Args:\n        llm (PromptedLLM): An instance of PromptedLLM containing the model and tokenizer.\n\n    Returns:\n        (CanonicalTokenization): An initialized CanonicalTokenization instance.\n    \"\"\"\n    if not isinstance(llm, PromptedLLM):\n        raise TypeError(\n            f\"Expected llm to be an instance of PromptedLLM, got {type(llm)}\"\n        )\n\n    # Extract necessary components from llm\n    tokenizer = llm.model.tokenizer\n    eos_token_ids = llm.token_maps.eos_idxs\n    model_name = tokenizer.name_or_path\n\n    # Create the filter using its factory method\n    canonicality_filter = FastCanonicalityFilterBPE.from_tokenizer(\n        tokenizer, eos_token_ids\n    )\n\n    # Set overrides on the filter\n    canonicality_filter.set_overrides(model_name)\n\n    # Call __init__ with the created filter and tokenizer\n    return cls(canonicality_filter)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/canonical/#genlm.control.potential.built_in.canonical.CanonicalTokenization.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Assess if a complete sequence follows canonical tokenization.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>float</code> <p>0.0 if canonical, float('-inf') otherwise</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Assess if a complete sequence follows canonical tokenization.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (float): 0.0 if canonical, float('-inf') otherwise\n    \"\"\"\n    # Empty sequences are considered canonical\n    if not context:\n        return 0.0\n\n    # Check if the sequence is canonical\n    is_canonical = self._check_canonicality(context)\n    return 0.0 if is_canonical else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/canonical/#genlm.control.potential.built_in.canonical.CanonicalTokenization.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Assess if a prefix sequence could potentially extend to a canonical sequence. For canonicality, this is the same as complete.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>float</code> <p>0.0 if potentially canonical, float('-inf') otherwise</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Assess if a prefix sequence could potentially extend to a canonical sequence.\n    For canonicality, this is the same as complete.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (float): 0.0 if potentially canonical, float('-inf') otherwise\n    \"\"\"\n    return await self.complete(context)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/canonical/#genlm.control.potential.built_in.canonical.CanonicalTokenization.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute weights for each possible next token given the context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>Sequence of tokens</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Weights for each token in the vocabulary and EOS</p> Source code in <code>genlm/control/potential/built_in/canonical.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute weights for each possible next token given the context.\n\n    Args:\n        context (list): Sequence of tokens\n\n    Returns:\n        (LazyWeights): Weights for each token in the vocabulary and EOS\n    \"\"\"\n    # Get the prefix weight (to check if context itself is canonical)\n    ctx_log_w = await self.prefix(context)\n\n    if ctx_log_w == float(\"-inf\"):\n        raise ValueError(\"Context is non-canonical\")\n    else:\n        if context:\n            t = (None, context[-1])\n            filter_mask = self.canonicality_filter(t)\n        else:\n            filter_mask = np.ones(len(self.canonicality_filter._decode), dtype=bool)\n\n        # Create log weights directly instead of using np.log(filter_mask)\n        # This is more efficient, avoids torch (with torch can't combine with other potentials!)\n        logws_no_eos = np.where(filter_mask, 0.0, float(\"-inf\")).astype(np.float32)\n\n        # append eos to the logws, always allow eos.\n        # NOTE: concat is because ._decode does not include eos while .vocab_eos does\n        logws = np.concatenate([logws_no_eos, np.array([0.0], dtype=np.float32)])\n\n    return self.make_lazy_weights(logws)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/json/","title":"json","text":""},{"location":"reference/genlm/control/potential/built_in/json/#genlm.control.potential.built_in.json.JustOneBlockIterable","title":"<code>JustOneBlockIterable</code>","text":"<p>Provides a single value (intended to be bytes from a context) and then signals if the reader tried to read past it. This allows us to distinguish invalid JSON from incomplete JSON by seeing if the reader tried to read more than it had or failed early.</p> Source code in <code>genlm/control/potential/built_in/json.py</code> <pre><code>class JustOneBlockIterable:\n    \"\"\"Provides a single value (intended to be bytes from a context)\n    and then signals if the reader tried to read past it. This allows\n    us to distinguish invalid JSON from incomplete JSON by seeing if\n    the reader tried to read more than it had or failed early.\"\"\"\n\n    def __init__(self, block):\n        self.block = block\n        self.read_past_first_block = False\n\n    def __iter__(self):\n        yield self.block\n        self.read_past_first_block = True\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/json/#genlm.control.potential.built_in.json.is_utf8_start_byte","title":"<code>is_utf8_start_byte(n)</code>","text":"<p>Checks if this is a byte that can appear at the start of a UTF-8 character.</p> Source code in <code>genlm/control/potential/built_in/json.py</code> <pre><code>def is_utf8_start_byte(n: int) -&gt; bool:\n    \"\"\"Checks if this is a byte that can appear at the\n    start of a UTF-8 character.\"\"\"\n    assert 0 &lt;= n &lt; 256\n    for prefix, mask in UTF8_START_BYTE_MASKS:\n        if n &amp; mask == prefix:\n            return True\n    return False\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/json/#genlm.control.potential.built_in.json.Parser","title":"<code>Parser</code>","text":"<p>               Bases: <code>Generic[T]</code></p> <p>Very basic parser combinators for mostly unambiguous grammars.</p> Source code in <code>genlm/control/potential/built_in/json.py</code> <pre><code>class Parser(Generic[T]):\n    \"\"\"Very basic parser combinators for mostly unambiguous grammars.\"\"\"\n\n    def parse(self, buffer: str, start: int) -&gt; tuple[int, T]: ...\n\n    def __floordiv__(self, other: Generic[S]) -&gt; \"Parser[Union[T, S]]\":\n        return AltParser(self, other)\n\n    def drop_result(self) -&gt; \"Parser[None]\":\n        return self.map(lambda x: None)\n\n    def map(self, apply: Callable[[T], S]) -&gt; \"Parser[S]\":\n        return MapParser(self, apply)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/json/#genlm.control.potential.built_in.json.Input","title":"<code>Input</code>","text":"<p>Convenience wrapper to provide a stateful stream-like interface that makes it easier to write parsers.</p> Source code in <code>genlm/control/potential/built_in/json.py</code> <pre><code>class Input:\n    \"\"\"Convenience wrapper to provide a stateful stream-like interface\n    that makes it easier to write parsers.\"\"\"\n\n    def __init__(self, buffer, index):\n        self.buffer = buffer\n        self.index = index\n\n    def current_char(self):\n        if self.index &gt;= len(self.buffer):\n            raise Incomplete()\n        else:\n            return self.buffer[self.index]\n\n    def read(self, n) -&gt; str:\n        result = self.buffer[self.index : self.index + n]\n        if len(result) &lt; n:\n            raise Incomplete()\n        else:\n            self.index += n\n            return result\n\n    def expect(self, expected: str):\n        actual = self.read(len(expected))\n        if actual != expected:\n            raise ParseError(\n                f\"Expected: {expected} but got {actual} at index {self.index}\"\n            )\n\n    def parse(self, parser: Parser[T]) -&gt; T:\n        try:\n            self.index, result = parser.parse(self.buffer, self.index)\n            return result\n        except Incomplete:\n            self.index = len(self.buffer)\n            raise\n\n    def skip_whitespace(self):\n        self.parse(WHITESPACE_PARSER)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/","title":"llm","text":""},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.TokenMappings","title":"<code>TokenMappings</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Container for token mappings between bytes and tokens IDs in a language model.</p> <p>This mapping is generally different from the <code>decode</code> and <code>encode</code> mappings in the <code>PromptedLLM</code> class (see notes on EOS token handling).</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>class TokenMappings(NamedTuple):\n    \"\"\"\n    Container for token mappings between bytes and tokens IDs in a language model.\n\n    This mapping is generally different from the `decode` and `encode` mappings in the `PromptedLLM` class (see notes on EOS token handling).\n    \"\"\"\n\n    decode: list[bytes]  # token_id -&gt; bytes\n    encode: dict[bytes, int]  # bytes -&gt; token_id\n    eos_idxs: list[int]  # IDs of EOS tokens\n\n    @classmethod\n    def create(cls, decode, eos_tokens):\n        encode = {x: i for i, x in enumerate(decode)}\n        if not all(eos in encode for eos in eos_tokens):\n            raise ValueError(\"EOS token not in language model vocabulary\")\n        eos_idxs = [encode[eos] for eos in eos_tokens]\n        return cls(decode=decode, encode=encode, eos_idxs=eos_idxs)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM","title":"<code>PromptedLLM</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A potential representing a language model conditioned on a fixed prompt prefix.</p> <p><code>PromptedLLM</code>s operate on byte sequences.</p> <p>Notes on EOS Token Handling:</p> <ul> <li> <p>Tokens to treat as end-of-sequence tokens are specified via the <code>eos_tokens</code> argument.</p> </li> <li> <p>These tokens are excluded from the potential's vocabulary and as such do not appear in the <code>vocab</code> attribute.</p> <p>This means they cannot appear in any input contexts to the potential nor in the output of <code>logw_next</code>. They can be used in the prompt however.</p> </li> <li> <p>The log probability assigned to the <code>genlm.control</code>'s reserved <code>EOS</code> token is the sum of the log probabilities of all the specified EOS tokens.</p> </li> </ul> <p>This class wraps an <code>AsyncLM</code> instance.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>class PromptedLLM(Potential):\n    \"\"\"A potential representing a language model conditioned on a fixed prompt prefix.\n\n    `PromptedLLM`s operate on byte sequences.\n\n    Notes on EOS Token Handling:\\n\n    - Tokens to treat as end-of-sequence tokens are specified via the `eos_tokens` argument.\\n\n    - These tokens are excluded from the potential's vocabulary and as such do not appear in the `vocab` attribute.\\n\n        This means they cannot appear in any input contexts to the potential nor in the output of `logw_next`. They can be used in the prompt however.\\n\n    - The log probability assigned to the `genlm.control`'s reserved `EOS` token is the sum of the log probabilities of all the specified EOS tokens.\\n\n\n    This class wraps an `AsyncLM` instance.\n    \"\"\"\n\n    def __init__(self, llm, prompt_ids=None, eos_tokens=None, temperature=1):\n        \"\"\"`\n        Initializes the PromptedLLM potential.\n\n        Args:\n            llm (AsyncLM): The language model to use.\n            prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n                Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `prompt` or `prompt_ids`.\n            eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n                Defaults to the EOS token of the language model's tokenizer.\n            temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n\n        Raises:\n            ValueError: If any EOS token is not in the language model vocabulary.\n        \"\"\"\n        self.model = llm\n        self.prompt_ids = prompt_ids or []\n\n        if not eos_tokens:\n            self._eos_tokens = [llm.byte_vocab[self.model.tokenizer.eos_token_id]]\n        else:\n            self._eos_tokens = eos_tokens\n\n        assert len(set(self._eos_tokens)) == len(self._eos_tokens), (\n            \"duplicate eos tokens\"\n        )\n\n        self.token_maps = TokenMappings.create(\n            decode=llm.byte_vocab, eos_tokens=self._eos_tokens\n        )\n\n        self.temperature = temperature\n\n        V = [x for x in self.token_maps.decode if x not in self._eos_tokens]\n\n        super().__init__(vocabulary=V)\n\n    @classmethod\n    def from_name(\n        cls,\n        name,\n        backend=None,\n        eos_tokens=None,\n        prompt_ids=None,\n        temperature=1.0,\n        **kwargs,\n    ):\n        \"\"\"Create a `PromptedLLM` from a HugginFace model name.\n\n        Args:\n            name (str): Name of the model to load\n            backend (str, optional): `AsyncLM` backend to use:\\n\n                * 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\\n\n                * 'hf' for an `AsyncTransformer`; ideal for CPU usage\\n\n                * 'mock' for a `MockAsyncLM`; ideal for testing.\\n\n                Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n            eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n                Defaults to the EOS token of the language model's tokenizer.\n            prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n                Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `set_prompt_from_str` or `prompt_ids`.\n            temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n            **kwargs (dict): Additional arguments passed to AsyncLM constructor\n\n        Returns:\n            (PromptedLLM): An instance of PromptedLLM\n        \"\"\"\n        backend = backend or (\"vllm\" if torch.cuda.is_available() else \"hf\")\n        model = load_model_by_name(name, backend=backend, **kwargs)\n        return cls(\n            model, prompt_ids=prompt_ids, eos_tokens=eos_tokens, temperature=temperature\n        )\n\n    @property\n    def eos_tokens(self):\n        return self._eos_tokens\n\n    @eos_tokens.setter\n    def eos_tokens(self, value):\n        raise ValueError(\n            \"Cannot reset eos_tokens after initialization. \"\n            \"Use spawn_new_eos(new_eos_tokens) instead.\"\n        )\n\n    @property\n    def prompt(self):\n        \"\"\"\n        Get the current prompt as a list of byte sequences corresponding to the prompt token IDs.\n\n        Returns:\n            (list[bytes]|None): The current prompt as a list of bytes sequences or None if no prompt_ids are set.\n        \"\"\"\n        if not self.prompt_ids:\n            return  # pragma: no cover\n        return [self.token_maps.decode[x] for x in self.prompt_ids]\n\n    def set_prompt_from_str(self, prompt_str):\n        \"\"\"Set the fixed prompt from a string.\n\n        Modifies `prompt_ids` to be the token IDs of the input prompt according to the language model's tokenizer.\n\n        Args:\n            prompt_str (str): The prompt to set.\n        \"\"\"\n        # TODO: Handle race condition where prompt_ids reset concurrently.\n        if not isinstance(prompt_str, str):\n            raise ValueError(\n                f\"Prompt must a string got {type(prompt_str)}. \"\n                f\"To set the prompt from a list of token IDs, use prompt_ids.\"\n            )\n\n        if prompt_str.endswith(\" \"):\n            warnings.warn(\n                \"Prompt ends with whitespace, which may affect tokenization. \"\n                \"Consider removing trailing whitespace.\",\n                stacklevel=2,\n            )\n\n        self.prompt_ids = self.model.tokenizer.encode(prompt_str)\n\n    def encode_tokens(self, tokens):\n        \"\"\"Encode a list of byte tokens to a list of token IDs in\n        the underlying language model's vocabulary.\n\n        Args:\n            tokens (list[bytes]): List of byte tokens to encode\n\n        Returns:\n            (list[int]): A list of token IDs corresponding to the input tokens.\n\n        Raises:\n            ValueError: If any token is not in the vocabulary\n        \"\"\"\n        try:\n            return [self.token_maps.encode[x] for x in tokens]\n        except KeyError as e:\n            raise ValueError(f\"Token {e.args[0]} not in vocabulary\") from e\n\n    def decode_tokens(self, ids):\n        \"\"\"\n        Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.\n\n        Args:\n            ids (list[int]): A list of token IDs in the language model's vocabulary.\n\n        Returns:\n            (list[bytes]): A list of byte tokens corresponding to the input token IDs.\n        \"\"\"\n        return [self.token_maps.decode[x] for x in ids]\n\n    def tokenize(self, context_str):\n        \"\"\"Tokenize a string to a list of `bytes` objects, each corresponding to a token in the vocabulary.\n\n        Uses the language model's tokenizer to map `context_str` to a list of token IDs, and then decodes the token IDs to bytes.\n\n        Args:\n            context_str (str): A string to encode\n\n        Returns:\n            (List[bytes]): A list of byte tokens corresponding to the input string.\n        \"\"\"\n        return self.decode_tokens(self.model.tokenizer.encode(context_str))\n\n    async def log_probability(self, context):\n        \"\"\"\n        Compute the log probability of `context` given the prompt.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of `context`.\n        \"\"\"\n        if not context:\n            return 0\n\n        context_ids = self.encode_tokens(context)\n        return await self._log_probability(context_ids)\n\n    async def _log_probability(self, context_ids):\n        prefixes = [self.prompt_ids + context_ids[:i] for i in range(len(context_ids))]\n        log_ps = self._maybe_temper(\n            await self.model.batch_next_token_logprobs(prefixes)\n        )\n        target_ids = torch.tensor(context_ids, device=log_ps.device)\n        with torch.no_grad():\n            token_logprobs = torch.gather(log_ps, 1, target_ids.unsqueeze(1))\n            total_logprob = token_logprobs.sum().item()\n\n        return total_logprob\n\n    def _maybe_temper(self, logps):\n        if self.temperature == 1:\n            return logps\n        return torch.log_softmax(logps / self.temperature, dim=-1)\n\n    async def prefix(self, context):\n        \"\"\"\n        Compute the log probability of `context` given the prompt.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of `context`.\n        \"\"\"\n        return await self.log_probability(context)\n\n    async def complete(self, context):\n        \"\"\"\n        Compute the log probability of `context` and the eos tokens given the prompt.\n\n        If the model has multiple eos tokens, their probabilities will be summed.\n\n        Args:\n            context (list[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (float): The log probability of the context.\n        \"\"\"\n        context_ids = self.encode_tokens(context)\n        logp_context = await self._log_probability(context_ids)\n        logp_next = self._maybe_temper(\n            await self.model.next_token_logprobs(self.prompt_ids + context_ids)\n        )\n        logp_eos = torch.logsumexp(logp_next[self.token_maps.eos_idxs], dim=0).item()\n        return logp_context + logp_eos\n\n    def _process_logw_next(self, logw_next):\n        \"\"\"Process the log probabilities for the next tokens.\n\n        This function rearranges the log probabilities such that the end-of-sequence (EOS) token's log probability\n        is the sum of the log probabilities of `self.eos_tokens`.\n\n        Args:\n            logw_next (torch.tensor): The log probabilities for the next tokens.\n\n        Returns:\n            (LazyWeights): Processed log probabilities for the next tokens.\n        \"\"\"\n        # This is ugly, but it's useful for all potentials to adhere to the convention\n        # of keeping the EOS token at the end of the weights array.\n        logw_next = logw_next[: len(self.token_maps.decode)]\n        logw_next = logw_next.log_softmax(dim=0)\n        _logw_next = torch.full((len(self.vocab) + 1,), float('-inf'), dtype=logw_next.dtype, device=logw_next.device)\n        _logw_next[: len(self.vocab)] = logw_next[\n            ~torch.isin(torch.arange(len(logw_next)), torch.tensor(self.token_maps.eos_idxs))\n        ]\n        _logw_next[-1] = torch.logsumexp(logw_next[self.token_maps.eos_idxs], dim=0).item()\n        return self.make_lazy_weights(_logw_next.float().cpu().numpy())\n\n    async def logw_next(self, context):\n        \"\"\"Get log probabilities for next tokens given the prompt and `context`.\n\n        Args:\n            context (List[bytes]): A sequence of bytes tokens.\n\n        Returns:\n            (LazyWeights): Log probabilities for next tokens and EOS.\n        \"\"\"\n        logw_next = self._maybe_temper(\n            await self.model.next_token_logprobs(\n                self.prompt_ids + self.encode_tokens(context)\n            )\n        )\n        return self._process_logw_next(logw_next)\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"Get log probabilities for next tokens given the prompt and `context`, for a batch of contexts.\n\n        Args:\n            contexts (list[list[bytes]]): A list of sequences of bytes tokens.\n\n        Returns:\n            (List[LazyWeights]): Log probabilities for next tokens and EOS for each context.\n        \"\"\"\n        logw_nexts = self._maybe_temper(\n            await self.model.batch_next_token_logprobs(\n                [self.prompt_ids + self.encode_tokens(context) for context in contexts]\n            )\n        )\n        return [\n            self._process_logw_next(logw_next)\n            for logw_next in logw_nexts\n        ]\n\n    def __repr__(self):\n        return f\"PromptedLLM(prompt={self.prompt!r})\"\n\n    def spawn(self):\n        \"\"\"\n        Spawn a new PromptedLLM with the same prompt and eos tokens.\n\n        Returns:\n            (PromptedLLM): A new PromptedLLM with the same prompt and eos tokens.\n\n        Note:\n            This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.\n        \"\"\"\n        return PromptedLLM(\n            self.model,\n            prompt_ids=self.prompt_ids.copy(),\n            eos_tokens=self._eos_tokens.copy(),\n            temperature=self.temperature,\n        )\n\n    def spawn_new_eos(self, eos_tokens):\n        \"\"\"\n        Create a new PromptedLLM with a different set of end-of-sequence tokens.\n\n        Args:\n            eos_tokens (list[bytes]): A list of tokens to treat as end-of-sequence tokens.\n\n        Returns:\n            (PromptedLLM): A new PromptedLLM with the specified end-of-sequence tokens.\n                The new model will have the same prompt_ids as `self`.\n        \"\"\"\n        return PromptedLLM(\n            self.model,\n            prompt_ids=self.prompt_ids.copy(),\n            eos_tokens=eos_tokens.copy(),\n            temperature=self.temperature,\n        )\n\n    def to_autobatched(self):\n        raise ValueError(\"PromptedLLMs are autobatched by default.\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.__init__","title":"<code>__init__(llm, prompt_ids=None, eos_tokens=None, temperature=1)</code>","text":"<p>` Initializes the PromptedLLM potential.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>AsyncLM</code> <p>The language model to use.</p> required <code>prompt_ids</code> <code>list[int]</code> <p>Optional prompt to use as a prompt prefix for all input contexts. Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via <code>prompt</code> or <code>prompt_ids</code>.</p> <code>None</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens to treat as end-of-sequence tokens. Defaults to the EOS token of the language model's tokenizer.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to apply to the language model's logits. Defaults to 1.</p> <code>1</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any EOS token is not in the language model vocabulary.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def __init__(self, llm, prompt_ids=None, eos_tokens=None, temperature=1):\n    \"\"\"`\n    Initializes the PromptedLLM potential.\n\n    Args:\n        llm (AsyncLM): The language model to use.\n        prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n            Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `prompt` or `prompt_ids`.\n        eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n            Defaults to the EOS token of the language model's tokenizer.\n        temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n\n    Raises:\n        ValueError: If any EOS token is not in the language model vocabulary.\n    \"\"\"\n    self.model = llm\n    self.prompt_ids = prompt_ids or []\n\n    if not eos_tokens:\n        self._eos_tokens = [llm.byte_vocab[self.model.tokenizer.eos_token_id]]\n    else:\n        self._eos_tokens = eos_tokens\n\n    assert len(set(self._eos_tokens)) == len(self._eos_tokens), (\n        \"duplicate eos tokens\"\n    )\n\n    self.token_maps = TokenMappings.create(\n        decode=llm.byte_vocab, eos_tokens=self._eos_tokens\n    )\n\n    self.temperature = temperature\n\n    V = [x for x in self.token_maps.decode if x not in self._eos_tokens]\n\n    super().__init__(vocabulary=V)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.from_name","title":"<code>from_name(name, backend=None, eos_tokens=None, prompt_ids=None, temperature=1.0, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a <code>PromptedLLM</code> from a HugginFace model name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the model to load</p> required <code>backend</code> <code>str</code> <p><code>AsyncLM</code> backend to use:</p> <ul> <li> <p>'vllm' to instantiate an <code>AsyncVirtualLM</code>; ideal for GPU usage</p> </li> <li> <p>'hf' for an <code>AsyncTransformer</code>; ideal for CPU usage</p> </li> <li> <p>'mock' for a <code>MockAsyncLM</code>; ideal for testing.</p> </li> </ul> <p>Defaults to 'vllm' if CUDA is available, otherwise 'hf'.</p> <code>None</code> <code>eos_tokens</code> <code>list[bytes]</code> <p>List of tokens to treat as end-of-sequence tokens. Defaults to the EOS token of the language model's tokenizer.</p> <code>None</code> <code>prompt_ids</code> <code>list[int]</code> <p>Optional prompt to use as a prompt prefix for all input contexts. Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via <code>set_prompt_from_str</code> or <code>prompt_ids</code>.</p> <code>None</code> <code>temperature</code> <code>float</code> <p>The temperature to apply to the language model's logits. Defaults to 1.</p> <code>1.0</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to AsyncLM constructor</p> <code>{}</code> <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>An instance of PromptedLLM</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>@classmethod\ndef from_name(\n    cls,\n    name,\n    backend=None,\n    eos_tokens=None,\n    prompt_ids=None,\n    temperature=1.0,\n    **kwargs,\n):\n    \"\"\"Create a `PromptedLLM` from a HugginFace model name.\n\n    Args:\n        name (str): Name of the model to load\n        backend (str, optional): `AsyncLM` backend to use:\\n\n            * 'vllm' to instantiate an `AsyncVirtualLM`; ideal for GPU usage\\n\n            * 'hf' for an `AsyncTransformer`; ideal for CPU usage\\n\n            * 'mock' for a `MockAsyncLM`; ideal for testing.\\n\n            Defaults to 'vllm' if CUDA is available, otherwise 'hf'.\n        eos_tokens (list[bytes], optional): List of tokens to treat as end-of-sequence tokens.\n            Defaults to the EOS token of the language model's tokenizer.\n        prompt_ids (list[int], optional): Optional prompt to use as a prompt prefix for all input contexts.\n            Must be a list of token IDs. Defaults to None. The prompt ids can be set post-init via `set_prompt_from_str` or `prompt_ids`.\n        temperature (float, optional): The temperature to apply to the language model's logits. Defaults to 1.\n        **kwargs (dict): Additional arguments passed to AsyncLM constructor\n\n    Returns:\n        (PromptedLLM): An instance of PromptedLLM\n    \"\"\"\n    backend = backend or (\"vllm\" if torch.cuda.is_available() else \"hf\")\n    model = load_model_by_name(name, backend=backend, **kwargs)\n    return cls(\n        model, prompt_ids=prompt_ids, eos_tokens=eos_tokens, temperature=temperature\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.prompt","title":"<code>prompt</code>  <code>property</code>","text":"<p>Get the current prompt as a list of byte sequences corresponding to the prompt token IDs.</p> <p>Returns:</p> Type Description <code>list[bytes] | None</code> <p>The current prompt as a list of bytes sequences or None if no prompt_ids are set.</p>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.set_prompt_from_str","title":"<code>set_prompt_from_str(prompt_str)</code>","text":"<p>Set the fixed prompt from a string.</p> <p>Modifies <code>prompt_ids</code> to be the token IDs of the input prompt according to the language model's tokenizer.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_str</code> <code>str</code> <p>The prompt to set.</p> required Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def set_prompt_from_str(self, prompt_str):\n    \"\"\"Set the fixed prompt from a string.\n\n    Modifies `prompt_ids` to be the token IDs of the input prompt according to the language model's tokenizer.\n\n    Args:\n        prompt_str (str): The prompt to set.\n    \"\"\"\n    # TODO: Handle race condition where prompt_ids reset concurrently.\n    if not isinstance(prompt_str, str):\n        raise ValueError(\n            f\"Prompt must a string got {type(prompt_str)}. \"\n            f\"To set the prompt from a list of token IDs, use prompt_ids.\"\n        )\n\n    if prompt_str.endswith(\" \"):\n        warnings.warn(\n            \"Prompt ends with whitespace, which may affect tokenization. \"\n            \"Consider removing trailing whitespace.\",\n            stacklevel=2,\n        )\n\n    self.prompt_ids = self.model.tokenizer.encode(prompt_str)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.encode_tokens","title":"<code>encode_tokens(tokens)</code>","text":"<p>Encode a list of byte tokens to a list of token IDs in the underlying language model's vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>list[bytes]</code> <p>List of byte tokens to encode</p> required <p>Returns:</p> Type Description <code>list[int]</code> <p>A list of token IDs corresponding to the input tokens.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If any token is not in the vocabulary</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def encode_tokens(self, tokens):\n    \"\"\"Encode a list of byte tokens to a list of token IDs in\n    the underlying language model's vocabulary.\n\n    Args:\n        tokens (list[bytes]): List of byte tokens to encode\n\n    Returns:\n        (list[int]): A list of token IDs corresponding to the input tokens.\n\n    Raises:\n        ValueError: If any token is not in the vocabulary\n    \"\"\"\n    try:\n        return [self.token_maps.encode[x] for x in tokens]\n    except KeyError as e:\n        raise ValueError(f\"Token {e.args[0]} not in vocabulary\") from e\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.decode_tokens","title":"<code>decode_tokens(ids)</code>","text":"<p>Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.</p> <p>Parameters:</p> Name Type Description Default <code>ids</code> <code>list[int]</code> <p>A list of token IDs in the language model's vocabulary.</p> required <p>Returns:</p> Type Description <code>list[bytes]</code> <p>A list of byte tokens corresponding to the input token IDs.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def decode_tokens(self, ids):\n    \"\"\"\n    Decode a list of token IDs in the language model's vocabulary to a list of byte tokens.\n\n    Args:\n        ids (list[int]): A list of token IDs in the language model's vocabulary.\n\n    Returns:\n        (list[bytes]): A list of byte tokens corresponding to the input token IDs.\n    \"\"\"\n    return [self.token_maps.decode[x] for x in ids]\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.tokenize","title":"<code>tokenize(context_str)</code>","text":"<p>Tokenize a string to a list of <code>bytes</code> objects, each corresponding to a token in the vocabulary.</p> <p>Uses the language model's tokenizer to map <code>context_str</code> to a list of token IDs, and then decodes the token IDs to bytes.</p> <p>Parameters:</p> Name Type Description Default <code>context_str</code> <code>str</code> <p>A string to encode</p> required <p>Returns:</p> Type Description <code>List[bytes]</code> <p>A list of byte tokens corresponding to the input string.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def tokenize(self, context_str):\n    \"\"\"Tokenize a string to a list of `bytes` objects, each corresponding to a token in the vocabulary.\n\n    Uses the language model's tokenizer to map `context_str` to a list of token IDs, and then decodes the token IDs to bytes.\n\n    Args:\n        context_str (str): A string to encode\n\n    Returns:\n        (List[bytes]): A list of byte tokens corresponding to the input string.\n    \"\"\"\n    return self.decode_tokens(self.model.tokenizer.encode(context_str))\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.log_probability","title":"<code>log_probability(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> given the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def log_probability(self, context):\n    \"\"\"\n    Compute the log probability of `context` given the prompt.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of `context`.\n    \"\"\"\n    if not context:\n        return 0\n\n    context_ids = self.encode_tokens(context)\n    return await self._log_probability(context_ids)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> given the prompt.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Compute the log probability of `context` given the prompt.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of `context`.\n    \"\"\"\n    return await self.log_probability(context)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Compute the log probability of <code>context</code> and the eos tokens given the prompt.</p> <p>If the model has multiple eos tokens, their probabilities will be summed.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log probability of the context.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Compute the log probability of `context` and the eos tokens given the prompt.\n\n    If the model has multiple eos tokens, their probabilities will be summed.\n\n    Args:\n        context (list[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (float): The log probability of the context.\n    \"\"\"\n    context_ids = self.encode_tokens(context)\n    logp_context = await self._log_probability(context_ids)\n    logp_next = self._maybe_temper(\n        await self.model.next_token_logprobs(self.prompt_ids + context_ids)\n    )\n    logp_eos = torch.logsumexp(logp_next[self.token_maps.eos_idxs], dim=0).item()\n    return logp_context + logp_eos\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Get log probabilities for next tokens given the prompt and <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>List[bytes]</code> <p>A sequence of bytes tokens.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Log probabilities for next tokens and EOS.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Get log probabilities for next tokens given the prompt and `context`.\n\n    Args:\n        context (List[bytes]): A sequence of bytes tokens.\n\n    Returns:\n        (LazyWeights): Log probabilities for next tokens and EOS.\n    \"\"\"\n    logw_next = self._maybe_temper(\n        await self.model.next_token_logprobs(\n            self.prompt_ids + self.encode_tokens(context)\n        )\n    )\n    return self._process_logw_next(logw_next)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Get log probabilities for next tokens given the prompt and <code>context</code>, for a batch of contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list[list[bytes]]</code> <p>A list of sequences of bytes tokens.</p> required <p>Returns:</p> Type Description <code>List[LazyWeights]</code> <p>Log probabilities for next tokens and EOS for each context.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"Get log probabilities for next tokens given the prompt and `context`, for a batch of contexts.\n\n    Args:\n        contexts (list[list[bytes]]): A list of sequences of bytes tokens.\n\n    Returns:\n        (List[LazyWeights]): Log probabilities for next tokens and EOS for each context.\n    \"\"\"\n    logw_nexts = self._maybe_temper(\n        await self.model.batch_next_token_logprobs(\n            [self.prompt_ids + self.encode_tokens(context) for context in contexts]\n        )\n    )\n    return [\n        self._process_logw_next(logw_next)\n        for logw_next in logw_nexts\n    ]\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new PromptedLLM with the same prompt and eos tokens.</p> <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>A new PromptedLLM with the same prompt and eos tokens.</p> Note <p>This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def spawn(self):\n    \"\"\"\n    Spawn a new PromptedLLM with the same prompt and eos tokens.\n\n    Returns:\n        (PromptedLLM): A new PromptedLLM with the same prompt and eos tokens.\n\n    Note:\n        This is a shallow copy. The new PromptedLLM will share the underlying AsyncLM instance.\n    \"\"\"\n    return PromptedLLM(\n        self.model,\n        prompt_ids=self.prompt_ids.copy(),\n        eos_tokens=self._eos_tokens.copy(),\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/llm/#genlm.control.potential.built_in.llm.PromptedLLM.spawn_new_eos","title":"<code>spawn_new_eos(eos_tokens)</code>","text":"<p>Create a new PromptedLLM with a different set of end-of-sequence tokens.</p> <p>Parameters:</p> Name Type Description Default <code>eos_tokens</code> <code>list[bytes]</code> <p>A list of tokens to treat as end-of-sequence tokens.</p> required <p>Returns:</p> Type Description <code>PromptedLLM</code> <p>A new PromptedLLM with the specified end-of-sequence tokens. The new model will have the same prompt_ids as <code>self</code>.</p> Source code in <code>genlm/control/potential/built_in/llm.py</code> <pre><code>def spawn_new_eos(self, eos_tokens):\n    \"\"\"\n    Create a new PromptedLLM with a different set of end-of-sequence tokens.\n\n    Args:\n        eos_tokens (list[bytes]): A list of tokens to treat as end-of-sequence tokens.\n\n    Returns:\n        (PromptedLLM): A new PromptedLLM with the specified end-of-sequence tokens.\n            The new model will have the same prompt_ids as `self`.\n    \"\"\"\n    return PromptedLLM(\n        self.model,\n        prompt_ids=self.prompt_ids.copy(),\n        eos_tokens=eos_tokens.copy(),\n        temperature=self.temperature,\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/","title":"wcfg","text":""},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.WCFG","title":"<code>WCFG</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A weighted context-free grammar potential.</p> <p>This class wraps a <code>genlm_grammar.CFG</code> and provides methods for computing the log-weight of a sequence, the prefix log-weight of a sequence, and the log-weights of the next token given a sequence.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>class WCFG(Potential):\n    \"\"\"\n    A weighted context-free grammar potential.\n\n    This class wraps a `genlm_grammar.CFG` and provides methods for computing the log-weight of a sequence,\n    the prefix log-weight of a sequence, and the log-weights of the next token given a sequence.\n    \"\"\"\n\n    def __init__(self, cfg):\n        \"\"\"\n        Initialize the WCFG potential.\n\n        Args:\n            cfg (genlm_grammar.CFG): The context-free grammar configuration to use.\n                The CFG must in the Float semiring.\n        \"\"\"\n        # TODO: convert to LogSemiring to handle underflow\n        if cfg.R is not Float:\n            raise ValueError(\"cfg semiring must be Float\")\n        self.cfg = cfg  # cfg before prefix transform\n        self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n        self.model = Earley(self.cfg_eos.prefix_grammar)\n        super().__init__(vocabulary=list(cfg.V))\n\n    @classmethod\n    def from_string(cls, grammar, to_bytes=True, **kwargs):\n        \"\"\"Create a WCFG from a string.\n\n        Args:\n            grammar (str): The string grammar specification to create the WCFG from.\n            to_bytes (bool, optional): Whether to convert the WCFG terminals to indivudual bytes.\n                Defaults to True.\n            **kwargs (dict): Additional arguments passed to the WCFG constructor.\n\n        Returns:\n            (WCFG): The created WCFG.\n        \"\"\"\n        cfg = CFG.from_string(grammar, Float)\n        if to_bytes:\n            cfg = cfg.to_bytes()\n        return cls(cfg, **kwargs)\n\n    async def complete(self, context):\n        \"\"\"\n        Compute the log weight of `context` under the WCFG.\n\n        For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\\n\n        - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (float): The log weight of `context` under the WCFG.\n        \"\"\"\n        w = self.model([*context, EOS])\n        return np.log(w) if w &gt; 0 else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Compute the log prefix weight of `context` under the WCFG.\n\n        This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n        For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n        - `prefix(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `prefix(\"d\")` returns $-\\\\infty$ since the WCFG does not accept any sequences with prefix \"d\"\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (float): The log prefix weight of `context` under the WCFG.\n        \"\"\"\n        w = self.model(context)\n        return np.log(w) if w &gt; 0 else float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute the next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WCFG's alphabet.\n\n        Returns:\n            (LazyWeights): The log weights for the next tokens and EOS given `context`.\n        \"\"\"\n        ws = self.model.next_token_weights(self.model.chart(context))\n        ws = ws.trim().normalize()\n\n        ws_array = np.array([ws[x] for x in self.vocab_eos])\n        mask = ws_array &gt; 0\n        log_ws = np.full_like(ws_array, float(\"-inf\"), dtype=np.float64)\n        log_ws[mask] = np.log(ws_array[mask])\n\n        return self.make_lazy_weights(log_ws)\n\n    def clear_cache(self):\n        \"\"\"Clear the internal cache of the parser.\"\"\"\n        self.model.clear_cache()\n\n    def __repr__(self):\n        return f\"WCFG(cfg={self.cfg!r})\"\n\n    def _repr_html_(self):\n        return self.cfg._repr_html_()\n\n    def spawn(self):\n        \"\"\"Spawn a new WCFG.\"\"\"\n        return WCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.WCFG.__init__","title":"<code>__init__(cfg)</code>","text":"<p>Initialize the WCFG potential.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <code>CFG</code> <p>The context-free grammar configuration to use. The CFG must in the Float semiring.</p> required Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def __init__(self, cfg):\n    \"\"\"\n    Initialize the WCFG potential.\n\n    Args:\n        cfg (genlm_grammar.CFG): The context-free grammar configuration to use.\n            The CFG must in the Float semiring.\n    \"\"\"\n    # TODO: convert to LogSemiring to handle underflow\n    if cfg.R is not Float:\n        raise ValueError(\"cfg semiring must be Float\")\n    self.cfg = cfg  # cfg before prefix transform\n    self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n    self.model = Earley(self.cfg_eos.prefix_grammar)\n    super().__init__(vocabulary=list(cfg.V))\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.WCFG.from_string","title":"<code>from_string(grammar, to_bytes=True, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a WCFG from a string.</p> <p>Parameters:</p> Name Type Description Default <code>grammar</code> <code>str</code> <p>The string grammar specification to create the WCFG from.</p> required <code>to_bytes</code> <code>bool</code> <p>Whether to convert the WCFG terminals to indivudual bytes. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>dict</code> <p>Additional arguments passed to the WCFG constructor.</p> <code>{}</code> <p>Returns:</p> Type Description <code>WCFG</code> <p>The created WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>@classmethod\ndef from_string(cls, grammar, to_bytes=True, **kwargs):\n    \"\"\"Create a WCFG from a string.\n\n    Args:\n        grammar (str): The string grammar specification to create the WCFG from.\n        to_bytes (bool, optional): Whether to convert the WCFG terminals to indivudual bytes.\n            Defaults to True.\n        **kwargs (dict): Additional arguments passed to the WCFG constructor.\n\n    Returns:\n        (WCFG): The created WCFG.\n    \"\"\"\n    cfg = CFG.from_string(grammar, Float)\n    if to_bytes:\n        cfg = cfg.to_bytes()\n    return cls(cfg, **kwargs)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.WCFG.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Compute the log weight of <code>context</code> under the WCFG.</p> <p>For example, if the WCFG accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>complete(\"c\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WCFG</p> </li> <li> <p><code>complete(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>complete(\"d\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WCFG</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log weight of <code>context</code> under the WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Compute the log weight of `context` under the WCFG.\n\n    For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\\n\n    - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WCFG\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (float): The log weight of `context` under the WCFG.\n    \"\"\"\n    w = self.model([*context, EOS])\n    return np.log(w) if w &gt; 0 else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.WCFG.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Compute the log prefix weight of <code>context</code> under the WCFG.</p> <p>This corresponds to the log of the sum of the weights of all sequences with prefix <code>context</code>.</p> <p>For example, if the WCFG accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>prefix(\"c\")</code> returns \\(\\log(w_{cat} + w_{car})\\)</p> </li> <li> <p><code>prefix(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>prefix(\"d\")</code> returns \\(-\\infty\\) since the WCFG does not accept any sequences with prefix \"d\"</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>The log prefix weight of <code>context</code> under the WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Compute the log prefix weight of `context` under the WCFG.\n\n    This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n    For example, if the WCFG accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n    - `prefix(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `prefix(\"d\")` returns $-\\\\infty$ since the WCFG does not accept any sequences with prefix \"d\"\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (float): The log prefix weight of `context` under the WCFG.\n    \"\"\"\n    w = self.model(context)\n    return np.log(w) if w &gt; 0 else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.WCFG.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WCFG's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>The log weights for the next tokens and EOS given <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute the next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WCFG's alphabet.\n\n    Returns:\n        (LazyWeights): The log weights for the next tokens and EOS given `context`.\n    \"\"\"\n    ws = self.model.next_token_weights(self.model.chart(context))\n    ws = ws.trim().normalize()\n\n    ws_array = np.array([ws[x] for x in self.vocab_eos])\n    mask = ws_array &gt; 0\n    log_ws = np.full_like(ws_array, float(\"-inf\"), dtype=np.float64)\n    log_ws[mask] = np.log(ws_array[mask])\n\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.WCFG.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the internal cache of the parser.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the internal cache of the parser.\"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.WCFG.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new WCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def spawn(self):\n    \"\"\"Spawn a new WCFG.\"\"\"\n    return WCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.BoolCFG","title":"<code>BoolCFG</code>","text":"<p>               Bases: <code>Potential</code></p> <p>BoolCFG represents a boolean context-free grammar.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>class BoolCFG(Potential):\n    \"\"\"BoolCFG represents a boolean context-free grammar.\"\"\"\n\n    def __init__(self, cfg):\n        if cfg.R != Boolean:\n            cfg = cfg.map_values(lambda x: Boolean(x &gt; 0), Boolean)\n        self.cfg = cfg  # cfg before prefix transform\n        self.cfg_eos = _add_eos(cfg, EOS)  # augmented with eos\n        self.model = Earley(self.cfg_eos.prefix_grammar)\n        super().__init__(vocabulary=list(cfg.V))\n\n    @classmethod\n    def from_lark(cls, lark_string, charset=\"core\"):\n        \"\"\"\n        Create a BoolCFG instance from a Lark grammar string.\n\n        The output grammar will be defined at the byte-level.\n\n        Args:\n            lark_string (str): The Lark grammar string to parse. See Lark documentation for correct syntax.\n            charset (str): The character set to use. Defaults to \"core\".\n                See `genlm-grammar` documentation for more details.\n\n        Returns:\n            (BoolCFG): An instance of BoolCFG created from the provided Lark grammar.\n        \"\"\"\n        byte_cfg = LarkStuff(lark_string).byte_cfg(charset=charset)\n        return cls(byte_cfg)\n\n    async def complete(self, context):\n        \"\"\"\n        Checks whether the context is accepted by the CFG.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (float): Log weight for whether `context` is accepted by the CFG.\n        \"\"\"\n        w = self.model([*context, EOS])\n        return 0 if w.score else float(\"-inf\")\n\n    async def prefix(self, context):\n        \"\"\"\n        Checks whether `context` is accepted as a prefix by the CFG, i.e.,\n        whether there exists a completion to `context` that is accepted by the CFG.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (float): Log weight for whether `context` is accepted as a prefix by the CFG.\n        \"\"\"\n        if not context:  # FIX: this is a hack to handle the empty string because genlm-grammar doesn't support it\n            return 0\n        w = self.model(context)\n        return 0 if w.score else float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Compute the next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the CFG's alphabet.\n\n        Returns:\n            (LazyWeights): The log weights for the next tokens and EOS given `context`.\n        \"\"\"\n        ws = self.model.next_token_weights(self.model.chart(context))\n        log_ws = np.array([0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos])\n        return self.make_lazy_weights(log_ws)\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"\n        Batch version of `logw_next`.\n\n        Args:\n            contexts (list): A list of sequences of tokens in the CFG's alphabet.\n\n        Returns:\n            (list): A list of log-weights for next token, one per context.\n        \"\"\"\n        Ws = []\n        for context in contexts:\n            ws = self.model.next_token_weights(self.model.chart(context))\n            log_ws = np.array(\n                [0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos]\n            )\n            Ws.append(self.make_lazy_weights(log_ws))\n        return Ws\n\n    def spawn(self):\n        \"\"\"Spawn a new BoolCFG.\"\"\"\n        return BoolCFG(self.cfg)\n\n    def clear_cache(self):\n        \"\"\"Clear the internal cache of the parser.\"\"\"\n        self.model.clear_cache()\n\n    def __repr__(self):\n        return f\"BoolCFG(cfg={self.cfg!r})\"\n\n    def _repr_html_(self):\n        return self.cfg._repr_html_()\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.BoolCFG.from_lark","title":"<code>from_lark(lark_string, charset='core')</code>  <code>classmethod</code>","text":"<p>Create a BoolCFG instance from a Lark grammar string.</p> <p>The output grammar will be defined at the byte-level.</p> <p>Parameters:</p> Name Type Description Default <code>lark_string</code> <code>str</code> <p>The Lark grammar string to parse. See Lark documentation for correct syntax.</p> required <code>charset</code> <code>str</code> <p>The character set to use. Defaults to \"core\". See <code>genlm-grammar</code> documentation for more details.</p> <code>'core'</code> <p>Returns:</p> Type Description <code>BoolCFG</code> <p>An instance of BoolCFG created from the provided Lark grammar.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>@classmethod\ndef from_lark(cls, lark_string, charset=\"core\"):\n    \"\"\"\n    Create a BoolCFG instance from a Lark grammar string.\n\n    The output grammar will be defined at the byte-level.\n\n    Args:\n        lark_string (str): The Lark grammar string to parse. See Lark documentation for correct syntax.\n        charset (str): The character set to use. Defaults to \"core\".\n            See `genlm-grammar` documentation for more details.\n\n    Returns:\n        (BoolCFG): An instance of BoolCFG created from the provided Lark grammar.\n    \"\"\"\n    byte_cfg = LarkStuff(lark_string).byte_cfg(charset=charset)\n    return cls(byte_cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.BoolCFG.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Checks whether the context is accepted by the CFG.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight for whether <code>context</code> is accepted by the CFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Checks whether the context is accepted by the CFG.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (float): Log weight for whether `context` is accepted by the CFG.\n    \"\"\"\n    w = self.model([*context, EOS])\n    return 0 if w.score else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.BoolCFG.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Checks whether <code>context</code> is accepted as a prefix by the CFG, i.e., whether there exists a completion to <code>context</code> that is accepted by the CFG.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight for whether <code>context</code> is accepted as a prefix by the CFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Checks whether `context` is accepted as a prefix by the CFG, i.e.,\n    whether there exists a completion to `context` that is accepted by the CFG.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (float): Log weight for whether `context` is accepted as a prefix by the CFG.\n    \"\"\"\n    if not context:  # FIX: this is a hack to handle the empty string because genlm-grammar doesn't support it\n        return 0\n    w = self.model(context)\n    return 0 if w.score else float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.BoolCFG.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Compute the next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>The log weights for the next tokens and EOS given <code>context</code>.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Compute the next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the CFG's alphabet.\n\n    Returns:\n        (LazyWeights): The log weights for the next tokens and EOS given `context`.\n    \"\"\"\n    ws = self.model.next_token_weights(self.model.chart(context))\n    log_ws = np.array([0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos])\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.BoolCFG.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Batch version of <code>logw_next</code>.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>A list of sequences of tokens in the CFG's alphabet.</p> required <p>Returns:</p> Type Description <code>list</code> <p>A list of log-weights for next token, one per context.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"\n    Batch version of `logw_next`.\n\n    Args:\n        contexts (list): A list of sequences of tokens in the CFG's alphabet.\n\n    Returns:\n        (list): A list of log-weights for next token, one per context.\n    \"\"\"\n    Ws = []\n    for context in contexts:\n        ws = self.model.next_token_weights(self.model.chart(context))\n        log_ws = np.array(\n            [0 if ws[x].score else float(\"-inf\") for x in self.vocab_eos]\n        )\n        Ws.append(self.make_lazy_weights(log_ws))\n    return Ws\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.BoolCFG.spawn","title":"<code>spawn()</code>","text":"<p>Spawn a new BoolCFG.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def spawn(self):\n    \"\"\"Spawn a new BoolCFG.\"\"\"\n    return BoolCFG(self.cfg)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wcfg/#genlm.control.potential.built_in.wcfg.BoolCFG.clear_cache","title":"<code>clear_cache()</code>","text":"<p>Clear the internal cache of the parser.</p> Source code in <code>genlm/control/potential/built_in/wcfg.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clear the internal cache of the parser.\"\"\"\n    self.model.clear_cache()\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/","title":"wfsa","text":""},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.WFSA","title":"<code>WFSA</code>","text":"<p>               Bases: <code>Potential</code></p> <p>A weighted finite state automaton (WFSA) potential.</p> <p>This class wraps a <code>genlm_grammar.WFSA</code> and provides methods for computing the log-weight of a context, the prefix log-weight of a context, and the log-weights of the next token given a context.</p> <p>Attributes:</p> Name Type Description <code>wfsa</code> <code>WFSA</code> <p>The weighted finite state automaton used for potential calculations.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>class WFSA(Potential):\n    \"\"\"\n    A weighted finite state automaton (WFSA) potential.\n\n    This class wraps a `genlm_grammar.WFSA` and provides methods for computing the log-weight of a context,\n    the prefix log-weight of a context, and the log-weights of the next token given a context.\n\n    Attributes:\n        wfsa (genlm_grammar.WFSA): The weighted finite state automaton used for potential calculations.\n    \"\"\"\n\n    def __init__(self, wfsa):\n        \"\"\"\n        Initializes the WFSA potential.\n\n        Args:\n            wfsa (genlm_grammar.WFSA): The weighted finite state automaton.\n\n        Raises:\n            ValueError: If the semiring of the provided WFSA is not Float or Log.\n\n        Note:\n            The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.\n        \"\"\"\n        if wfsa.R not in (Float, Log):\n            raise ValueError(f\"Unsupported semiring: {wfsa.R}\")\n\n        if wfsa.R is Float:\n            self.wfsa = self._convert_to_log(wfsa)\n        else:\n            self.wfsa = wfsa\n\n        self.cache = {(): self.wfsa.epsremove.start}\n        super().__init__(vocabulary=list(self.wfsa.alphabet))\n\n    @classmethod\n    def from_regex(cls, pattern, charset=None, to_bytes=True):\n        \"\"\"\n        Create a WFSA from a regex pattern.\n\n        Args:\n            pattern (str): The regex pattern to convert into a WFSA.\n            charset (set): The character set to use for negative character classes.\n                Defaults to characters in string.printable.\n            to_bytes (bool): Whether to convert the WFSA transitions to bytes.\n                Defaults to True. When set to False, the WFSA transitions will be strings.\n\n        Returns:\n            (WFSA): An instance of the WFSA class.\n\n        Note:\n            The transition weights are automatically normalized to form a probability distribution.\n            For each state, the weights of all outgoing transitions (including final state transitions)\n            sum to 1.0. This means if a state has n possible transitions, each transition will have\n            weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use `BoolFSA`.\n        \"\"\"\n        charset = charset or set(string.printable)\n        wfsa = interegular_to_wfsa(pattern, charset=charset)\n        if to_bytes:\n            wfsa = wfsa.to_bytes()\n        return cls(wfsa=wfsa)\n\n    @staticmethod\n    def _convert_to_log(wfsa):\n        \"\"\"Convert a WFSA from the Float semiring to the Log semiring.\"\"\"\n        assert wfsa.R is Float\n        assert isinstance(wfsa, BaseWFSA)\n        new = BaseWFSA(Log)\n\n        for i, w in wfsa.I:\n            new.add_I(i, Log(np.log(w)))\n\n        for i, w in wfsa.F:\n            new.add_F(i, Log(np.log(w)))\n\n        for i, a, j, w in wfsa.arcs():\n            new.add_arc(i, a, j, Log(np.log(w)))\n\n        return new\n\n    def _consume(self, bs):\n        # XXX implement cache eviction\n        bs = tuple(bs)\n\n        try:\n            return self.cache[bs]\n        except KeyError:\n            pass\n\n        wfsa = self.wfsa.epsremove\n        curr = wfsa.R.chart()\n        prev = self._consume(bs[:-1])\n        for i in prev:\n            for j, w in wfsa.arcs(i, bs[-1]):\n                curr[j] += prev[i] * w\n\n        self.cache[bs] = curr\n\n        return curr\n\n    async def complete(self, context):\n        \"\"\"\n        Computes the log weight of the context under the weighted language represented by the WFSA.\n\n        For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\\n\n        - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n        - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): Log weight of context under the WFSA.\n        \"\"\"\n        # TODO: optimize to use _consume cache\n        return self.wfsa(context).score\n\n    def _prefix(self, context):\n        curr = self._consume(context)\n\n        if not curr:\n            return float(\"-inf\"), curr\n\n        bkwd = self.wfsa.epsremove.backward\n        log_ctx_w = logsumexp([(curr[i] * bkwd[i]).score for i in curr])\n\n        if np.isnan(log_ctx_w):\n            return float(\"-inf\"), curr\n\n        return log_ctx_w, curr\n\n    async def prefix(self, context):\n        \"\"\"\n        Computes the prefix log weight of `context` under the WFSA.\n\n        This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n        For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n        - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n        - `prefix(\"ca\")` returns $\\\\log(w_{cat})$\\n\n        - `prefix(\"d\")` returns $-\\\\infty$ since the WFSA does not accept any sequences with prefix \"d\"\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): Log weight of `context` as a prefix under the WFSA.\n        \"\"\"\n        return self._prefix(context)[0]\n\n    async def logw_next(self, context):\n        \"\"\"Returns next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (LazyWeights): Log-weights for next token and EOS.\n        \"\"\"\n        log_ctx_w, curr = self._prefix(context)\n\n        if log_ctx_w == float(\"-inf\"):\n            raise ValueError(f\"Context {context!r} has zero weight.\")\n\n        bkwd = self.wfsa.epsremove.backward\n\n        ws = self.wfsa.R.chart()\n        for i in curr:\n            for b, j, w in self.wfsa.epsremove.arcs(i=i):\n                ws[b] += curr[i] * w * bkwd[j]\n\n        ws[self.eos] = self.wfsa.R.zero\n        for j, w in self.wfsa.epsremove.F:\n            ws[self.eos] += curr[j] * w\n\n        log_ws = np.array([ws[b].score for b in self.vocab_eos]) - log_ctx_w\n\n        return self.make_lazy_weights(log_ws)\n\n    def _repr_svg_(self):\n        return self.wfsa._repr_svg_()\n\n    def __repr__(self):\n        return f\"WFSA(wfsa={self.wfsa!r})\"\n\n    def spawn(self):\n        cls = type(self)\n        return cls(wfsa=self.wfsa)\n\n    def clear_cache(self):\n        self.cache = {(): self.wfsa.epsremove.start}\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.WFSA.__init__","title":"<code>__init__(wfsa)</code>","text":"<p>Initializes the WFSA potential.</p> <p>Parameters:</p> Name Type Description Default <code>wfsa</code> <code>WFSA</code> <p>The weighted finite state automaton.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the semiring of the provided WFSA is not Float or Log.</p> Note <p>The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>def __init__(self, wfsa):\n    \"\"\"\n    Initializes the WFSA potential.\n\n    Args:\n        wfsa (genlm_grammar.WFSA): The weighted finite state automaton.\n\n    Raises:\n        ValueError: If the semiring of the provided WFSA is not Float or Log.\n\n    Note:\n        The WFSA will be converted to the Log semiring to avoid underflow if the semiring is Float.\n    \"\"\"\n    if wfsa.R not in (Float, Log):\n        raise ValueError(f\"Unsupported semiring: {wfsa.R}\")\n\n    if wfsa.R is Float:\n        self.wfsa = self._convert_to_log(wfsa)\n    else:\n        self.wfsa = wfsa\n\n    self.cache = {(): self.wfsa.epsremove.start}\n    super().__init__(vocabulary=list(self.wfsa.alphabet))\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.WFSA.from_regex","title":"<code>from_regex(pattern, charset=None, to_bytes=True)</code>  <code>classmethod</code>","text":"<p>Create a WFSA from a regex pattern.</p> <p>Parameters:</p> Name Type Description Default <code>pattern</code> <code>str</code> <p>The regex pattern to convert into a WFSA.</p> required <code>charset</code> <code>set</code> <p>The character set to use for negative character classes. Defaults to characters in string.printable.</p> <code>None</code> <code>to_bytes</code> <code>bool</code> <p>Whether to convert the WFSA transitions to bytes. Defaults to True. When set to False, the WFSA transitions will be strings.</p> <code>True</code> <p>Returns:</p> Type Description <code>WFSA</code> <p>An instance of the WFSA class.</p> Note <p>The transition weights are automatically normalized to form a probability distribution. For each state, the weights of all outgoing transitions (including final state transitions) sum to 1.0. This means if a state has n possible transitions, each transition will have weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use <code>BoolFSA</code>.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>@classmethod\ndef from_regex(cls, pattern, charset=None, to_bytes=True):\n    \"\"\"\n    Create a WFSA from a regex pattern.\n\n    Args:\n        pattern (str): The regex pattern to convert into a WFSA.\n        charset (set): The character set to use for negative character classes.\n            Defaults to characters in string.printable.\n        to_bytes (bool): Whether to convert the WFSA transitions to bytes.\n            Defaults to True. When set to False, the WFSA transitions will be strings.\n\n    Returns:\n        (WFSA): An instance of the WFSA class.\n\n    Note:\n        The transition weights are automatically normalized to form a probability distribution.\n        For each state, the weights of all outgoing transitions (including final state transitions)\n        sum to 1.0. This means if a state has n possible transitions, each transition will have\n        weight 1/n. To create a WFSA from a regex with non-probabilistic transitions, use `BoolFSA`.\n    \"\"\"\n    charset = charset or set(string.printable)\n    wfsa = interegular_to_wfsa(pattern, charset=charset)\n    if to_bytes:\n        wfsa = wfsa.to_bytes()\n    return cls(wfsa=wfsa)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.WFSA.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Computes the log weight of the context under the weighted language represented by the WFSA.</p> <p>For example, if the WFSA accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>complete(\"c\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WFSA</p> </li> <li> <p><code>complete(\"cat\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>complete(\"d\")</code> returns \\(-\\infty\\) since this sequence is not accepted by the WFSA</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of context under the WFSA.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Computes the log weight of the context under the weighted language represented by the WFSA.\n\n    For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `complete(\"c\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\\n\n    - `complete(\"cat\")` returns $\\\\log(w_{cat})$\\n\n    - `complete(\"d\")` returns $-\\\\infty$ since this sequence is not accepted by the WFSA\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): Log weight of context under the WFSA.\n    \"\"\"\n    # TODO: optimize to use _consume cache\n    return self.wfsa(context).score\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.WFSA.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Computes the prefix log weight of <code>context</code> under the WFSA.</p> <p>This corresponds to the log of the sum of the weights of all sequences with prefix <code>context</code>.</p> <p>For example, if the WFSA accepts \"cat\" and \"car\" with weights \\(w_{cat}\\) and \\(w_{car}\\):</p> <ul> <li> <p><code>prefix(\"c\")</code> returns \\(\\log(w_{cat} + w_{car})\\)</p> </li> <li> <p><code>prefix(\"ca\")</code> returns \\(\\log(w_{cat})\\)</p> </li> <li> <p><code>prefix(\"d\")</code> returns \\(-\\infty\\) since the WFSA does not accept any sequences with prefix \"d\"</p> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p>Log weight of <code>context</code> as a prefix under the WFSA.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Computes the prefix log weight of `context` under the WFSA.\n\n    This corresponds to the log of the sum of the weights of all sequences with prefix `context`.\n\n    For example, if the WFSA accepts \"cat\" and \"car\" with weights $w_{cat}$ and $w_{car}$:\\n\n    - `prefix(\"c\")` returns $\\\\log(w_{cat} + w_{car})$\\n\n    - `prefix(\"ca\")` returns $\\\\log(w_{cat})$\\n\n    - `prefix(\"d\")` returns $-\\\\infty$ since the WFSA does not accept any sequences with prefix \"d\"\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): Log weight of `context` as a prefix under the WFSA.\n    \"\"\"\n    return self._prefix(context)[0]\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.WFSA.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Returns next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Log-weights for next token and EOS.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"Returns next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (LazyWeights): Log-weights for next token and EOS.\n    \"\"\"\n    log_ctx_w, curr = self._prefix(context)\n\n    if log_ctx_w == float(\"-inf\"):\n        raise ValueError(f\"Context {context!r} has zero weight.\")\n\n    bkwd = self.wfsa.epsremove.backward\n\n    ws = self.wfsa.R.chart()\n    for i in curr:\n        for b, j, w in self.wfsa.epsremove.arcs(i=i):\n            ws[b] += curr[i] * w * bkwd[j]\n\n    ws[self.eos] = self.wfsa.R.zero\n    for j, w in self.wfsa.epsremove.F:\n        ws[self.eos] += curr[j] * w\n\n    log_ws = np.array([ws[b].score for b in self.vocab_eos]) - log_ctx_w\n\n    return self.make_lazy_weights(log_ws)\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.BoolFSA","title":"<code>BoolFSA</code>","text":"<p>               Bases: <code>WFSA</code></p> <p>Boolean FSA potential.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>class BoolFSA(WFSA):\n    \"\"\"Boolean FSA potential.\"\"\"\n\n    async def prefix(self, context):\n        \"\"\"\n        Computes whether the context is accepted as a prefix by the FSA.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): `0` if the context is accepted as a prefix, `-inf` otherwise.\n        \"\"\"\n        prefix_w = await super().prefix(context)\n        if prefix_w &gt; float(\"-inf\"):\n            return 0\n        return float(\"-inf\")\n\n    async def complete(self, context):\n        \"\"\"\n        Computes whether the context is accepted by the FSA.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (float): `0` if the context is accepted, `-inf` otherwise.\n        \"\"\"\n        complete_w = await super().complete(context)\n        if complete_w &gt; float(\"-inf\"):\n            return 0\n        return float(\"-inf\")\n\n    async def logw_next(self, context):\n        \"\"\"\n        Returns next token log weights given `context`.\n\n        Args:\n            context (list): A sequence of tokens in the WFSA's alphabet.\n\n        Returns:\n            (LazyWeights): Boolean log-weights for next token.\n        \"\"\"\n        logw_next = await super().logw_next(context)\n        return logw_next.spawn(\n            new_weights=np.where(\n                logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n            )\n        )\n\n    async def batch_logw_next(self, contexts):\n        \"\"\"\n        Returns next token log weights for a batch of contexts.\n\n        Args:\n            contexts (list): The list of contexts.\n\n        Returns:\n            (list): List of log-weights for next token, one per context.\n        \"\"\"\n        logw_nexts = await super().batch_logw_next(contexts)\n        return [\n            logw_next.spawn(\n                new_weights=np.where(\n                    logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n                )\n            )\n            for logw_next in logw_nexts\n        ]\n\n    def __repr__(self):\n        return f\"BoolFSA(wfsa={self.wfsa!r})\"\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.BoolFSA.prefix","title":"<code>prefix(context)</code>  <code>async</code>","text":"<p>Computes whether the context is accepted as a prefix by the FSA.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p><code>0</code> if the context is accepted as a prefix, <code>-inf</code> otherwise.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def prefix(self, context):\n    \"\"\"\n    Computes whether the context is accepted as a prefix by the FSA.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): `0` if the context is accepted as a prefix, `-inf` otherwise.\n    \"\"\"\n    prefix_w = await super().prefix(context)\n    if prefix_w &gt; float(\"-inf\"):\n        return 0\n    return float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.BoolFSA.complete","title":"<code>complete(context)</code>  <code>async</code>","text":"<p>Computes whether the context is accepted by the FSA.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>float</code> <p><code>0</code> if the context is accepted, <code>-inf</code> otherwise.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def complete(self, context):\n    \"\"\"\n    Computes whether the context is accepted by the FSA.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (float): `0` if the context is accepted, `-inf` otherwise.\n    \"\"\"\n    complete_w = await super().complete(context)\n    if complete_w &gt; float(\"-inf\"):\n        return 0\n    return float(\"-inf\")\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.BoolFSA.logw_next","title":"<code>logw_next(context)</code>  <code>async</code>","text":"<p>Returns next token log weights given <code>context</code>.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the WFSA's alphabet.</p> required <p>Returns:</p> Type Description <code>LazyWeights</code> <p>Boolean log-weights for next token.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def logw_next(self, context):\n    \"\"\"\n    Returns next token log weights given `context`.\n\n    Args:\n        context (list): A sequence of tokens in the WFSA's alphabet.\n\n    Returns:\n        (LazyWeights): Boolean log-weights for next token.\n    \"\"\"\n    logw_next = await super().logw_next(context)\n    return logw_next.spawn(\n        new_weights=np.where(\n            logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n        )\n    )\n</code></pre>"},{"location":"reference/genlm/control/potential/built_in/wfsa/#genlm.control.potential.built_in.wfsa.BoolFSA.batch_logw_next","title":"<code>batch_logw_next(contexts)</code>  <code>async</code>","text":"<p>Returns next token log weights for a batch of contexts.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>The list of contexts.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of log-weights for next token, one per context.</p> Source code in <code>genlm/control/potential/built_in/wfsa.py</code> <pre><code>async def batch_logw_next(self, contexts):\n    \"\"\"\n    Returns next token log weights for a batch of contexts.\n\n    Args:\n        contexts (list): The list of contexts.\n\n    Returns:\n        (list): List of log-weights for next token, one per context.\n    \"\"\"\n    logw_nexts = await super().batch_logw_next(contexts)\n    return [\n        logw_next.spawn(\n            new_weights=np.where(\n                logw_next.weights &gt; float(\"-inf\"), 0, logw_next.weights\n            )\n        )\n        for logw_next in logw_nexts\n    ]\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/","title":"sampler","text":""},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.DirectTokenSampler","title":"<code>DirectTokenSampler</code>","text":"<p>               Bases: <code>TokenSampler</code></p> <p>Samples individual tokens directly from the log-normalized <code>logw_next</code> function of a potential.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The potential function to sample from</p> required Warning <p>Only use this sampler if the potential's <code>logw_next</code> method is efficient. This is the case for potentials like <code>PromptedLLM</code>, but for custom potentials with a large vocabulary size, the default implementation of <code>logw_next</code> generally will not be efficient, and thus this sampler will be slow.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>class DirectTokenSampler(TokenSampler):\n    \"\"\"Samples individual tokens directly from the log-normalized `logw_next` function\n    of a potential.\n\n    Args:\n        potential (Potential): The potential function to sample from\n\n    Warning:\n        Only use this sampler if the potential's `logw_next` method is efficient. This is the case\n        for potentials like `PromptedLLM`, but for custom potentials with a large vocabulary size,\n        the default implementation of `logw_next` generally will not be efficient, and thus this\n        sampler will be slow.\n    \"\"\"\n\n    def __init__(self, potential):\n        super().__init__(target=potential)\n        self.potential = potential\n\n    async def sample(self, context, draw=None):\n        \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method.\n\n        Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the target potential's vocabulary,\n        this method samples a token $x_n \\\\in \\\\textsf{target.vocab_eos}$ and weight $w$.\n\n        The sampled token and weight are properly weighted with respect to\n        $$\n        \\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n        $$\n\n        The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n        Returns:\n            (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the sampled token.\n        \"\"\"\n        logws = await self.potential.logw_next(context)\n        logps = logws.normalize()\n        if draw is None:\n            # fast sampling from logps using gumbel-max trick\n            token = fast_sample_lazyweights(logps)\n        else:\n            token = draw(logps.exp().materialize())\n        return token, logws.sum(), logps[token]\n\n    async def cleanup(self):\n        pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.DirectTokenSampler.sample","title":"<code>sample(context, draw=None)</code>  <code>async</code>","text":"<p>Sample a token and weight that are properly weighted with respect to the target potential's <code>logw_next</code> method.</p> <p>Given a context of tokens \\(x_1, \\ldots, x_{n-1}\\) in the target potential's vocabulary, this method samples a token \\(x_n \\in \\textsf{target.vocab_eos}\\) and weight \\(w\\).</p> <p>The sampled token and weight are properly weighted with respect to $$ \\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1}) $$</p> <p>The returned weight corresponds to the log normalizing constant of \\(\\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1})\\).</p> <p>Returns:</p> Type Description <code>(token, weight, logp)</code> <p>A tuple containing the sampled token, weight, and log-probability of the sampled token.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def sample(self, context, draw=None):\n    \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method.\n\n    Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the target potential's vocabulary,\n    this method samples a token $x_n \\\\in \\\\textsf{target.vocab_eos}$ and weight $w$.\n\n    The sampled token and weight are properly weighted with respect to\n    $$\n    \\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n    $$\n\n    The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n    Returns:\n        (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the sampled token.\n    \"\"\"\n    logws = await self.potential.logw_next(context)\n    logps = logws.normalize()\n    if draw is None:\n        # fast sampling from logps using gumbel-max trick\n        token = fast_sample_lazyweights(logps)\n    else:\n        token = draw(logps.exp().materialize())\n    return token, logws.sum(), logps[token]\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.SetTokenSampler","title":"<code>SetTokenSampler</code>","text":"<p>               Bases: <code>TokenSampler</code></p> <p>Samples individual tokens by sampling a weighted set of tokens and then selecting one proportional to its weight.</p> <p>This class wraps a <code>SetSampler</code>.</p> <p>Parameters:</p> Name Type Description Default <code>set_sampler</code> <code>SetSampler</code> <p>The set sampler to sample from</p> required Source code in <code>genlm/control/sampler/token.py</code> <pre><code>class SetTokenSampler(TokenSampler):\n    \"\"\"Samples individual tokens by sampling a weighted set of tokens and then selecting one\n    proportional to its weight.\n\n    This class wraps a `SetSampler`.\n\n    Args:\n        set_sampler (SetSampler): The set sampler to sample from\n    \"\"\"\n\n    def __init__(self, set_sampler):\n        assert isinstance(set_sampler, SetSampler)\n        super().__init__(set_sampler.target)\n        self.set_sampler = set_sampler\n\n    async def sample(self, context, draw=None):\n        \"\"\"Sample a token and weight by sampling a weighted set of tokens from the `set_sampler`\n        and then selecting one proportional to its weight.\n\n        Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the vocabulary of the set sampler's target potential,\n        this method samples a token $x_n \\\\in \\\\textsf{set_sampler.target.vocab_eos}$ and a weight.\n\n        The sampled token and weight are properly weighted with respect to\n        $$\n        \\\\textsf{set_sampler.target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n        $$\n\n        The returned weight corresponds to the sum of the weights of the sampled set.\n\n        Args:\n            context (list[int]): A sequence of tokens in the vocabulary of the set sampler's target potential.\n\n        Returns:\n            (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the random\n                choices made in sampling that token.\n\n        Note:\n            For properly weighted sampling, the `set_sampler` must assign correct weights to each token. See\n            `SetSampler` for more details.\n        \"\"\"\n        logws, logp = await self.set_sampler.sample_set(context, draw=draw)\n        logps = logws.normalize()\n        if draw is None:\n            token = fast_sample_lazyweights(logps)\n        else:\n            token = draw(logps.exp().materialize())\n        return token, logws.sum(), logp + logps[token]\n\n    async def cleanup(self):\n        \"\"\"Clean up the sampler.\n\n        This method should be called when the sampler is no longer needed.\n        \"\"\"\n        await self.set_sampler.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.SetTokenSampler.sample","title":"<code>sample(context, draw=None)</code>  <code>async</code>","text":"<p>Sample a token and weight by sampling a weighted set of tokens from the <code>set_sampler</code> and then selecting one proportional to its weight.</p> <p>Given a context of tokens \\(x_1, \\ldots, x_{n-1}\\) in the vocabulary of the set sampler's target potential, this method samples a token \\(x_n \\in \\textsf{set_sampler.target.vocab_eos}\\) and a weight.</p> <p>The sampled token and weight are properly weighted with respect to $$ \\textsf{set_sampler.target.logw_next}(x_n | x_1, \\ldots, x_{n-1}) $$</p> <p>The returned weight corresponds to the sum of the weights of the sampled set.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[int]</code> <p>A sequence of tokens in the vocabulary of the set sampler's target potential.</p> required <p>Returns:</p> Type Description <code>(token, weight, logp)</code> <p>A tuple containing the sampled token, weight, and log-probability of the random choices made in sampling that token.</p> Note <p>For properly weighted sampling, the <code>set_sampler</code> must assign correct weights to each token. See <code>SetSampler</code> for more details.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def sample(self, context, draw=None):\n    \"\"\"Sample a token and weight by sampling a weighted set of tokens from the `set_sampler`\n    and then selecting one proportional to its weight.\n\n    Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the vocabulary of the set sampler's target potential,\n    this method samples a token $x_n \\\\in \\\\textsf{set_sampler.target.vocab_eos}$ and a weight.\n\n    The sampled token and weight are properly weighted with respect to\n    $$\n    \\\\textsf{set_sampler.target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n    $$\n\n    The returned weight corresponds to the sum of the weights of the sampled set.\n\n    Args:\n        context (list[int]): A sequence of tokens in the vocabulary of the set sampler's target potential.\n\n    Returns:\n        (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the random\n            choices made in sampling that token.\n\n    Note:\n        For properly weighted sampling, the `set_sampler` must assign correct weights to each token. See\n        `SetSampler` for more details.\n    \"\"\"\n    logws, logp = await self.set_sampler.sample_set(context, draw=draw)\n    logps = logws.normalize()\n    if draw is None:\n        token = fast_sample_lazyweights(logps)\n    else:\n        token = draw(logps.exp().materialize())\n    return token, logws.sum(), logp + logps[token]\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.SetTokenSampler.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Clean up the sampler.</p> <p>This method should be called when the sampler is no longer needed.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Clean up the sampler.\n\n    This method should be called when the sampler is no longer needed.\n    \"\"\"\n    await self.set_sampler.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.AWRS","title":"<code>AWRS</code>","text":"<p>               Bases: <code>TokenSampler</code></p> <p>Samples individual tokens through an adaptive weighted rejection sampling algorithm.</p> <p>This sampler is based on the algorithm described in Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</p> <p>It draws properly weighted samples from the product of a non-boolean potential and a boolean condition.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The non-boolean potential.</p> required <code>condition</code> <code>Potential</code> <p>The boolean condition. This potential must only output boolean values (0 or -inf in log-space).</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>42</code> <code>prune_logws</code> <code>bool</code> <p>Whether to prune the logws to only include the tokens in the intersection of the potential and condition vocabularies</p> <code>True</code> <code>proper_weights</code> <code>bool</code> <p>Whether to return properly weighted samples. If False, the sampler will only run one round of adaptive rejection sampling.</p> <code>True</code> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>class AWRS(TokenSampler):\n    \"\"\"Samples individual tokens through an adaptive weighted rejection sampling algorithm.\n\n    This sampler is based on the algorithm described in [Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling](https://arxiv.org/abs/2504.05410)\n\n    It draws properly weighted samples from the product of a non-boolean potential and a boolean condition.\n\n    Args:\n        potential (Potential): The non-boolean potential.\n        condition (Potential): The boolean condition. This potential must only output boolean values (0 or -inf in log-space).\n        seed (int): The seed for the random number generator.\n        prune_logws (bool): Whether to prune the logws to only include the tokens in the intersection of the potential and condition vocabularies\n        proper_weights (bool): Whether to return properly weighted samples.\n            If False, the sampler will only run one round of adaptive rejection sampling.\n    \"\"\"\n\n    def __init__(\n        self, potential, condition, seed=42, prune_logws=True, proper_weights=True\n    ):\n        super().__init__(target=potential * condition)\n        self.potential = potential\n        self.condition = condition\n\n        self.prune_logws = prune_logws\n        self.proper_weights = proper_weights\n        self.valid_idxs = np.array(\n            [self.potential.lookup[t] for t in self.target.vocab_eos]\n        )\n\n        self.vocab_eos_set = set(self.target.vocab_eos)\n        self.V = len(self.potential.vocab_eos)\n        self.rng = np.random.default_rng(seed=seed)\n\n    def _prune_logws(self, logws):\n        # Prune the logws to only include the tokens in the\n        # target vocabulary. (This zeros-out tokens which we know a priori\n        # will be rejected.) Note: We need an additional correction term\n        # to account for the fact that we're throwing away some probability mass.\n        # This should be handled in `sample`.\n        pruned = self.potential.alloc_logws()\n        pruned[self.valid_idxs] = logws.weights[self.valid_idxs]\n        logws.weights = pruned\n        return logws\n\n    async def _accept(self, context, token, verbosity=0):\n        if self.prune_logws or token in self.vocab_eos_set:\n            if token is self.target.eos:\n                logscore = await self.condition.complete(context)\n            else:\n                logscore = await self.condition.prefix(context + [token])\n            assert logscore in {-np.inf, 0}, \"`condition` must be Boolean\"\n        else:\n            logscore = -np.inf\n\n        do_accept = logscore == 0\n\n        if verbosity &gt; 0:\n            if do_accept:\n                print(colors.green % f\". {repr(token)}\")\n            else:\n                print(colors.red % \".\", end=\"\")\n\n        return do_accept\n\n    async def sample(self, context, verbosity=0):\n        \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method via adaptive weighted rejection sampling.\n\n        The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n        Returns:\n            (token, weight, np.nan): A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.\n        \"\"\"\n        logws = await self.potential.logw_next(context)\n        if self.prune_logws:\n            logws = self._prune_logws(logws)\n\n        logZ = logsumexp(logws.weights)\n        logps = logws.weights - logZ\n        toks = logws.decode\n\n        tok, nrej, logp0 = None, 0, []\n        for _ in range(2):\n            keys = logps - np.log(-np.log(self.rng.random((self.V,))))\n            order = np.argsort(-keys)\n            for rank in range(logps.size):\n                item = order[rank]\n                if keys[item] == -np.inf:\n                    break\n                if await self._accept(context, toks[item], verbosity):\n                    if tok is None:\n                        tok = toks[item]\n                    break\n                else:\n                    nrej += 1\n                    if tok is None:\n                        logp0.append(logps[item])\n                    logps[item] = -np.inf\n\n            if not self.proper_weights:\n                if tok is None:\n                    return self.target.eos, float(\"-inf\"), np.nan\n                return tok, 0, np.nan\n\n        if tok is None:  # No token was accepted, return EOS and kill the particle.\n            return self.target.eos, float(\"-inf\"), np.nan\n\n        if not logp0:  # Success on first try.\n            logw = logZ - np.log(nrej + 1)\n        else:\n            logw = logZ + log1mexp(logsumexp(logp0)) - np.log(nrej + 1)\n\n        return tok, logw, np.nan\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.AWRS.sample","title":"<code>sample(context, verbosity=0)</code>  <code>async</code>","text":"<p>Sample a token and weight that are properly weighted with respect to the target potential's <code>logw_next</code> method via adaptive weighted rejection sampling.</p> <p>The returned weight corresponds to the log normalizing constant of \\(\\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1})\\).</p> <p>Returns:</p> Type Description <code>(token, weight, nan)</code> <p>A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def sample(self, context, verbosity=0):\n    \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method via adaptive weighted rejection sampling.\n\n    The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n    Returns:\n        (token, weight, np.nan): A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.\n    \"\"\"\n    logws = await self.potential.logw_next(context)\n    if self.prune_logws:\n        logws = self._prune_logws(logws)\n\n    logZ = logsumexp(logws.weights)\n    logps = logws.weights - logZ\n    toks = logws.decode\n\n    tok, nrej, logp0 = None, 0, []\n    for _ in range(2):\n        keys = logps - np.log(-np.log(self.rng.random((self.V,))))\n        order = np.argsort(-keys)\n        for rank in range(logps.size):\n            item = order[rank]\n            if keys[item] == -np.inf:\n                break\n            if await self._accept(context, toks[item], verbosity):\n                if tok is None:\n                    tok = toks[item]\n                break\n            else:\n                nrej += 1\n                if tok is None:\n                    logp0.append(logps[item])\n                logps[item] = -np.inf\n\n        if not self.proper_weights:\n            if tok is None:\n                return self.target.eos, float(\"-inf\"), np.nan\n            return tok, 0, np.nan\n\n    if tok is None:  # No token was accepted, return EOS and kill the particle.\n        return self.target.eos, float(\"-inf\"), np.nan\n\n    if not logp0:  # Success on first try.\n        logw = logZ - np.log(nrej + 1)\n    else:\n        logw = logZ + log1mexp(logsumexp(logp0)) - np.log(nrej + 1)\n\n    return tok, logw, np.nan\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.EagerSetSampler","title":"<code>EagerSetSampler</code>","text":"<p>               Bases: <code>TrieSetSampler</code></p> <p>A trie-based set sampler that implements an eager sampling strategy for generating a set of tokens.</p> <p>An <code>EagerSetSampler</code> samples tokens by incrementally sampling items from the item-wise product of the <code>iter_potential</code> and <code>item_potential</code>. The sampled set is the set of sequences of items that correspond to valid tokens in <code>iter_potential</code>'s vocabulary.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>class EagerSetSampler(TrieSetSampler):\n    \"\"\"\n    A trie-based set sampler that implements an eager sampling strategy\n    for generating a set of tokens.\n\n    An `EagerSetSampler` samples tokens by incrementally sampling items from the item-wise product of the `iter_potential` and `item_potential`.\n    The sampled set is the set of sequences of items that correspond to valid tokens in `iter_potential`'s vocabulary.\n    \"\"\"\n\n    async def sample_set(self, context, draw=None):\n        \"\"\"\n        Sample a set of tokens given a context.\n\n        Args:\n            context (list): A sequence of tokens in the `iter_potential`'s vocabulary.\n\n        Returns:\n            (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n        \"\"\"\n        if draw is None:\n            draw = sample_dict\n        iter_logws = await self.iter_potential.logw_next(context)\n        item_ws = await self.trie_executor.weight_sum(iter_logws.exp().weights)\n\n        logws = self.target.alloc_logws()\n        curr = self.trie.root\n        coerced_ctx = self.f(context)\n        subtokens = []\n        logp, logw = 0, 0\n\n        while True:\n            children = self.trie.children[curr]\n            item_w_curr = item_ws[curr]\n            item_ws1 = Float.chart(\n                {a: item_ws[c] / item_w_curr for a, c in children.items()}\n            )\n\n            if None in item_ws1:\n                leaf = children[None]\n                token = self.trie.leaf2word[leaf]\n                token_id = self.leaf_to_token_id[leaf]\n                logws[token_id] = iter_logws[token] + logw - logp\n\n            item_logws2 = await self.item_potential.logw_next(coerced_ctx + subtokens)\n            item_ws2 = item_logws2.exp().materialize()\n            w_next = (item_ws1 * item_ws2).trim()\n\n            if not w_next:\n                break\n\n            ps = w_next.normalize()\n            b = draw(ps)\n            logp += np.log(ps[b])\n            logw += item_logws2[b]\n\n            if b == self.target.eos:\n                assert not subtokens, \"subtokens should be empty at EOS.\"\n                logws[-1] = iter_logws[self.target.eos] + logw - logp\n                break\n\n            subtokens.append(b)\n            curr = children[b]\n\n        return self.target.make_lazy_weights(logws), logp\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.EagerSetSampler.sample_set","title":"<code>sample_set(context, draw=None)</code>  <code>async</code>","text":"<p>Sample a set of tokens given a context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the <code>iter_potential</code>'s vocabulary.</p> required <p>Returns:</p> Type Description <code>(LazyWeights, float)</code> <p>A weighted set of tokens and the log-probability of the sampled set.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>async def sample_set(self, context, draw=None):\n    \"\"\"\n    Sample a set of tokens given a context.\n\n    Args:\n        context (list): A sequence of tokens in the `iter_potential`'s vocabulary.\n\n    Returns:\n        (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n    \"\"\"\n    if draw is None:\n        draw = sample_dict\n    iter_logws = await self.iter_potential.logw_next(context)\n    item_ws = await self.trie_executor.weight_sum(iter_logws.exp().weights)\n\n    logws = self.target.alloc_logws()\n    curr = self.trie.root\n    coerced_ctx = self.f(context)\n    subtokens = []\n    logp, logw = 0, 0\n\n    while True:\n        children = self.trie.children[curr]\n        item_w_curr = item_ws[curr]\n        item_ws1 = Float.chart(\n            {a: item_ws[c] / item_w_curr for a, c in children.items()}\n        )\n\n        if None in item_ws1:\n            leaf = children[None]\n            token = self.trie.leaf2word[leaf]\n            token_id = self.leaf_to_token_id[leaf]\n            logws[token_id] = iter_logws[token] + logw - logp\n\n        item_logws2 = await self.item_potential.logw_next(coerced_ctx + subtokens)\n        item_ws2 = item_logws2.exp().materialize()\n        w_next = (item_ws1 * item_ws2).trim()\n\n        if not w_next:\n            break\n\n        ps = w_next.normalize()\n        b = draw(ps)\n        logp += np.log(ps[b])\n        logw += item_logws2[b]\n\n        if b == self.target.eos:\n            assert not subtokens, \"subtokens should be empty at EOS.\"\n            logws[-1] = iter_logws[self.target.eos] + logw - logp\n            break\n\n        subtokens.append(b)\n        curr = children[b]\n\n    return self.target.make_lazy_weights(logws), logp\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.TopKSetSampler","title":"<code>TopKSetSampler</code>","text":"<p>               Bases: <code>TrieSetSampler</code></p> <p>A trie-based set sampler that lazily enumerates the top K tokens by weight in the target, and samples an additional \"wildcard\" token to ensure absolute continuity.</p> Warning <p>This sampler is not guaranteed to be correct if the <code>item_potential</code>'s prefix weights do not monotonically decrease with the length of the context. That is, \\(\\textsf{item_potential.prefix}(x) \\leq \\textsf{item_potential.prefix}(xy)\\) for all sequences of items \\(x, y\\).</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>class TopKSetSampler(TrieSetSampler):\n    \"\"\"\n    A trie-based set sampler that lazily enumerates the top K tokens by weight in the target,\n    and samples an additional \"wildcard\" token to ensure absolute continuity.\n\n    Warning:\n        This sampler is not guaranteed to be correct if the `item_potential`'s\n        prefix weights do not monotonically decrease with the length of the context.\n        That is, $\\\\textsf{item_potential.prefix}(x) \\\\leq \\\\textsf{item_potential.prefix}(xy)$ for all sequences of items $x, y$.\n    \"\"\"\n\n    def __init__(self, iter_potential, item_potential, K):\n        \"\"\"\n        Initialize the TopKSetSampler.\n\n        Args:\n            iter_potential (Potential): The potential defined over a vocabulary of iterables.\n            item_potential (Potential): The potential defined over a vocabulary of items.\n            K (int|None): The number of top tokens to enumerate. If None, all tokens are enumerated.\n        \"\"\"\n        if K is not None and K &lt;= 0:\n            raise ValueError(\"K must be greater than 0 or None\")\n        super().__init__(iter_potential, item_potential)\n        self.K = K\n\n    async def sample_set(self, context, draw=None):\n        \"\"\"\n        Sample a set of tokens given a context.\n\n        Args:\n            context (list): A sequence of tokens in the `iter_potential`'s vocabulary.\n\n        Returns:\n            (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n        \"\"\"\n        if draw is None:\n            draw = sample_dict\n        iter_logws = await self.iter_potential.logw_next(context)\n        max_logws = await self.trie_executor.weight_max(iter_logws.weights)\n\n        k = 0\n        logws = self.target.alloc_logws()\n        sampled = self.target.alloc_logws(default=False)\n\n        async for token_id, logw in self._lazy_enum(context, max_logws):\n            logws[token_id] = logw\n            sampled[token_id] = True\n            k += 1\n            if self.K is not None and k &gt;= self.K:\n                break\n\n        logp_wc = 0\n        if self.K is not None and k == self.K:\n            # Get the distribution over wildcard tokens\n            iter_ws = iter_logws.exp()\n            W_wc = Float.chart(\n                {\n                    token_id: iter_ws[token]\n                    for token_id, token in enumerate(self.target.vocab_eos)\n                    if not sampled[token_id]\n                }\n            )\n\n            # if W_wc is non-empty, sample a wildcard token to ensure absolute continuity\n            if W_wc:\n                P_wc = W_wc.normalize()\n                wc_id = draw(P_wc)\n                logp_wc = np.log(P_wc[wc_id])\n                wc = self.target.vocab_eos[wc_id]\n                item_ctx = self.f(context)\n                prefix_w = await self.item_potential.prefix(item_ctx)\n                if wc == self.target.eos:\n                    w_guide_wc = await self.item_potential.complete(item_ctx) - prefix_w\n                else:\n                    w_guide_wc = (\n                        await self.item_potential.prefix(self.f(context + [wc]))\n                        - prefix_w\n                    )\n                logws[wc_id] = np.log(W_wc[wc_id]) + w_guide_wc - logp_wc\n\n        return self.target.make_lazy_weights(logws), logp_wc\n\n    async def _lazy_enum(self, context, max_logws):\n        agenda = LocatorMaxHeap()\n\n        W = Float.chart()\n\n        # initial conditions\n        (token, node) = ((), self.trie.root)\n        agenda[token, node, False] = max_logws[node]\n        W[node] = 0\n\n        children = self.trie.children\n        coerced_ctx = self.f(context)\n\n        curr_priority = float(\"inf\")\n        prev_best = float(\"inf\")\n        while agenda:\n            (token, node, done), score = agenda.popitem()\n\n            assert score &lt;= curr_priority, (\n                \"Monotonicity assumption violated. \"\n                \"`item_potential` prefix weight must be monotonically decreasing.\"\n            )\n            curr_priority = score\n\n            # terminal state\n            if done:\n                value = W[node] + max_logws[node]\n                assert prev_best &gt;= value\n                prev_best = value\n                yield (self.leaf_to_token_id[node], value)\n                continue\n\n            logws = None\n            for x, y in children[node].items():\n                if x is None:\n                    W_y = W[node]\n                    W[y] = W_y\n                    agenda[token, y, True] = W_y + max_logws[y]\n                else:\n                    if logws is None:\n                        logws = await self.item_potential.logw_next(\n                            coerced_ctx + list(token)\n                        )\n                    W_y = W[node] + logws[x]\n                    if W_y == float(\"-inf\"):\n                        continue\n                    W[y] = W_y\n                    agenda[(*token, x), y, False] = W_y + max_logws[y]\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.TopKSetSampler.__init__","title":"<code>__init__(iter_potential, item_potential, K)</code>","text":"<p>Initialize the TopKSetSampler.</p> <p>Parameters:</p> Name Type Description Default <code>iter_potential</code> <code>Potential</code> <p>The potential defined over a vocabulary of iterables.</p> required <code>item_potential</code> <code>Potential</code> <p>The potential defined over a vocabulary of items.</p> required <code>K</code> <code>int | None</code> <p>The number of top tokens to enumerate. If None, all tokens are enumerated.</p> required Source code in <code>genlm/control/sampler/set.py</code> <pre><code>def __init__(self, iter_potential, item_potential, K):\n    \"\"\"\n    Initialize the TopKSetSampler.\n\n    Args:\n        iter_potential (Potential): The potential defined over a vocabulary of iterables.\n        item_potential (Potential): The potential defined over a vocabulary of items.\n        K (int|None): The number of top tokens to enumerate. If None, all tokens are enumerated.\n    \"\"\"\n    if K is not None and K &lt;= 0:\n        raise ValueError(\"K must be greater than 0 or None\")\n    super().__init__(iter_potential, item_potential)\n    self.K = K\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.TopKSetSampler.sample_set","title":"<code>sample_set(context, draw=None)</code>  <code>async</code>","text":"<p>Sample a set of tokens given a context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the <code>iter_potential</code>'s vocabulary.</p> required <p>Returns:</p> Type Description <code>(LazyWeights, float)</code> <p>A weighted set of tokens and the log-probability of the sampled set.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>async def sample_set(self, context, draw=None):\n    \"\"\"\n    Sample a set of tokens given a context.\n\n    Args:\n        context (list): A sequence of tokens in the `iter_potential`'s vocabulary.\n\n    Returns:\n        (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n    \"\"\"\n    if draw is None:\n        draw = sample_dict\n    iter_logws = await self.iter_potential.logw_next(context)\n    max_logws = await self.trie_executor.weight_max(iter_logws.weights)\n\n    k = 0\n    logws = self.target.alloc_logws()\n    sampled = self.target.alloc_logws(default=False)\n\n    async for token_id, logw in self._lazy_enum(context, max_logws):\n        logws[token_id] = logw\n        sampled[token_id] = True\n        k += 1\n        if self.K is not None and k &gt;= self.K:\n            break\n\n    logp_wc = 0\n    if self.K is not None and k == self.K:\n        # Get the distribution over wildcard tokens\n        iter_ws = iter_logws.exp()\n        W_wc = Float.chart(\n            {\n                token_id: iter_ws[token]\n                for token_id, token in enumerate(self.target.vocab_eos)\n                if not sampled[token_id]\n            }\n        )\n\n        # if W_wc is non-empty, sample a wildcard token to ensure absolute continuity\n        if W_wc:\n            P_wc = W_wc.normalize()\n            wc_id = draw(P_wc)\n            logp_wc = np.log(P_wc[wc_id])\n            wc = self.target.vocab_eos[wc_id]\n            item_ctx = self.f(context)\n            prefix_w = await self.item_potential.prefix(item_ctx)\n            if wc == self.target.eos:\n                w_guide_wc = await self.item_potential.complete(item_ctx) - prefix_w\n            else:\n                w_guide_wc = (\n                    await self.item_potential.prefix(self.f(context + [wc]))\n                    - prefix_w\n                )\n            logws[wc_id] = np.log(W_wc[wc_id]) + w_guide_wc - logp_wc\n\n    return self.target.make_lazy_weights(logws), logp_wc\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.SMC","title":"<code>SMC</code>","text":"<p>This class implements sequential Monte Carlo (SMC) inference for controlled text generation. The generation process works as follows:</p> <ol> <li> <p>Token Sampling: At each step, the <code>unit_sampler</code> is used to extend each particle (candidate sequence)    by sampling a new token. This grows all sequences by one token at a time. The sampler also outputs    an importance weight with each extension to correct for the myopic nature of token-by-token sampling.</p> </li> <li> <p>Critic Evaluation: If a <code>critic</code> is provided, it scores the updated sequences (via it's <code>score</code> method),    reweighting the particles based on how well they satisfy the constraints encoded by the critic.</p> </li> <li> <p>Resampling: When the effective sample size (ESS) falls below the threshold,    particles are resampled according to their weights. This helps focus computation    on more promising sequences.</p> </li> <li> <p>Termination: The process continues until either:</p> <ul> <li> <p>All sequences reach an end-of-sequence (EOS) token</p> </li> <li> <p>The maximum token length is reached</p> </li> </ul> </li> </ol> <p>If a critic is provided, the resulting sequences are properly weighted with respect to the product of the unit sampler's target potential and the critic potential (<code>unit_sampler.target * critic</code>). If a critic is not provided, the resulting sequences are weighted with respect to the unit sampler's target potential.</p> <p>Parameters:</p> Name Type Description Default <code>unit_sampler</code> <code>TokenSampler</code> <p>The sampler that generates tokens.</p> required <code>critic</code> <code>Potential</code> <p>A potential function that guides the generation process by scoring candidate sequences. Must have the same token type as the unit_sampler.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unit_sampler is not a TokenSampler, if critic is not a Potential, or if the token types of unit_sampler and critic don't match.</p> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>class SMC:\n    \"\"\"This class implements sequential Monte Carlo (SMC) inference for controlled text generation.\n    The generation process works as follows:\n\n    1. Token Sampling: At each step, the `unit_sampler` is used to extend each particle (candidate sequence)\n       by sampling a new token. This grows all sequences by one token at a time. The sampler also outputs\n       an importance weight with each extension to correct for the myopic nature of token-by-token sampling.\n\n    2. Critic Evaluation: If a `critic` is provided, it scores the updated sequences (via it's `score` method),\n       reweighting the particles based on how well they satisfy the constraints encoded by the critic.\n\n    3. Resampling: When the effective sample size (ESS) falls below the threshold,\n       particles are resampled according to their weights. This helps focus computation\n       on more promising sequences.\n\n    4. Termination: The process continues until either:\\n\n        - All sequences reach an end-of-sequence (EOS) token\\n\n        - The maximum token length is reached\n\n    If a critic is provided, the resulting sequences are properly weighted with respect to the product of the unit sampler's\n    target potential and the critic potential (`unit_sampler.target * critic`). If a critic is not provided,\n    the resulting sequences are weighted with respect to the unit sampler's target potential.\n\n    Args:\n        unit_sampler (TokenSampler): The sampler that generates tokens.\n        critic (Potential, optional): A potential function that guides the generation process\n            by scoring candidate sequences. Must have the same token type as the unit_sampler.\n\n    Raises:\n        ValueError: If unit_sampler is not a TokenSampler, if critic is not a Potential,\n            or if the token types of unit_sampler and critic don't match.\n    \"\"\"\n\n    def __init__(self, unit_sampler, critic=None):\n        if not isinstance(unit_sampler, TokenSampler):\n            raise ValueError(\"`unit_sampler` must be a TokenSampler\")\n\n        if critic:\n            if not isinstance(critic, Potential):\n                raise ValueError(\"`critic` must be a Potential\")\n            if not unit_sampler.token_type == critic.token_type:\n                raise ValueError(\n                    \"`critic` must have the same token type as the `unit_sampler`. \"\n                    f\"Got {unit_sampler.token_type} and {critic.token_type}.\"\n                    + (\n                        \"\\nMaybe you forgot to coerce the critic to the token type of the unit sampler? See `Coerce`.\"\n                        if unit_sampler.token_type.is_iterable_of(critic.token_type)\n                        else \"\"\n                    )\n                )\n\n        self.unit_sampler = unit_sampler\n        self.critic = critic\n\n    async def __call__(\n        self,\n        n_particles,\n        ess_threshold,\n        max_tokens,\n        verbosity=0,\n        json_path=None,\n        **kwargs,\n    ):\n        \"\"\"Generate sequences using sequential Monte Carlo inference.\n\n        Args:\n            n_particles (int): Number of particles (candidate sequences) to maintain during\n                generation. Higher values provide better exploration but require more\n                computation.\n            ess_threshold (float): Effective sample size threshold for resampling,\n                expressed as a fraction of the number of particles. When ESS falls below\n                this value, particles are resampled according to their weights. Should be between 0 and 1.\n                Higher values lead to more frequent resampling. Note that when ess_threshold = 0,\n                the critic is only applied at the end of the generation (if it is provided).\n            max_tokens (int): Maximum number of tokens to generate per sequence. Generation\n                may terminate earlier if all sequences reach an EOS token.\n            verbosity (int, optional): Verbosity level for the SMC algorithm. 0 is silent, 1 prints the\n                particles at each step. Default is 0.\n            json_path (str, optional): JSON file path for saving a record of the inference run.\n                This can be used in conjunction with the `InferenceVisualizer` to visualize the inference run.\n            **kwargs (dict): Additional keyword arguments to pass to the SMC algorithm.\n                See the `llamppl.inference.smc_standard` documentation for more details.\n\n        Returns:\n            (Sequences): A container holding the generated sequences, their importance weights, and\n                other metadata from the generation process.\n        \"\"\"\n        model = SequenceModel(\n            unit_sampler=self.unit_sampler,\n            critic=self.critic,\n            max_tokens=max_tokens,\n            verbosity=verbosity,\n            twist_with_critic=ess_threshold &gt; 0,\n        )\n\n        particles = await smc_standard(\n            model=model,\n            n_particles=n_particles,\n            ess_threshold=ess_threshold,\n            json_file=json_path,\n            **kwargs,\n        )\n\n        return Sequences(*_unpack_particles(particles))\n\n    async def cleanup(self):\n        \"\"\"Clean up resources used by the inference engine.\n\n        This method should be called when the InferenceEngine is no longer needed.\n\n        Example:\n            ```python\n            sampler = SequenceSampler(unit_sampler, critic)\n            try:\n                sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\n            finally:\n                await sampler.cleanup()\n            ```\n        \"\"\"\n        await self.unit_sampler.cleanup()\n        if self.critic:\n            await self.critic.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.SMC.__call__","title":"<code>__call__(n_particles, ess_threshold, max_tokens, verbosity=0, json_path=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate sequences using sequential Monte Carlo inference.</p> <p>Parameters:</p> Name Type Description Default <code>n_particles</code> <code>int</code> <p>Number of particles (candidate sequences) to maintain during generation. Higher values provide better exploration but require more computation.</p> required <code>ess_threshold</code> <code>float</code> <p>Effective sample size threshold for resampling, expressed as a fraction of the number of particles. When ESS falls below this value, particles are resampled according to their weights. Should be between 0 and 1. Higher values lead to more frequent resampling. Note that when ess_threshold = 0, the critic is only applied at the end of the generation (if it is provided).</p> required <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to generate per sequence. Generation may terminate earlier if all sequences reach an EOS token.</p> required <code>verbosity</code> <code>int</code> <p>Verbosity level for the SMC algorithm. 0 is silent, 1 prints the particles at each step. Default is 0.</p> <code>0</code> <code>json_path</code> <code>str</code> <p>JSON file path for saving a record of the inference run. This can be used in conjunction with the <code>InferenceVisualizer</code> to visualize the inference run.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the SMC algorithm. See the <code>llamppl.inference.smc_standard</code> documentation for more details.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequences</code> <p>A container holding the generated sequences, their importance weights, and other metadata from the generation process.</p> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>async def __call__(\n    self,\n    n_particles,\n    ess_threshold,\n    max_tokens,\n    verbosity=0,\n    json_path=None,\n    **kwargs,\n):\n    \"\"\"Generate sequences using sequential Monte Carlo inference.\n\n    Args:\n        n_particles (int): Number of particles (candidate sequences) to maintain during\n            generation. Higher values provide better exploration but require more\n            computation.\n        ess_threshold (float): Effective sample size threshold for resampling,\n            expressed as a fraction of the number of particles. When ESS falls below\n            this value, particles are resampled according to their weights. Should be between 0 and 1.\n            Higher values lead to more frequent resampling. Note that when ess_threshold = 0,\n            the critic is only applied at the end of the generation (if it is provided).\n        max_tokens (int): Maximum number of tokens to generate per sequence. Generation\n            may terminate earlier if all sequences reach an EOS token.\n        verbosity (int, optional): Verbosity level for the SMC algorithm. 0 is silent, 1 prints the\n            particles at each step. Default is 0.\n        json_path (str, optional): JSON file path for saving a record of the inference run.\n            This can be used in conjunction with the `InferenceVisualizer` to visualize the inference run.\n        **kwargs (dict): Additional keyword arguments to pass to the SMC algorithm.\n            See the `llamppl.inference.smc_standard` documentation for more details.\n\n    Returns:\n        (Sequences): A container holding the generated sequences, their importance weights, and\n            other metadata from the generation process.\n    \"\"\"\n    model = SequenceModel(\n        unit_sampler=self.unit_sampler,\n        critic=self.critic,\n        max_tokens=max_tokens,\n        verbosity=verbosity,\n        twist_with_critic=ess_threshold &gt; 0,\n    )\n\n    particles = await smc_standard(\n        model=model,\n        n_particles=n_particles,\n        ess_threshold=ess_threshold,\n        json_file=json_path,\n        **kwargs,\n    )\n\n    return Sequences(*_unpack_particles(particles))\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.SMC.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Clean up resources used by the inference engine.</p> <p>This method should be called when the InferenceEngine is no longer needed.</p> Example <pre><code>sampler = SequenceSampler(unit_sampler, critic)\ntry:\n    sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\nfinally:\n    await sampler.cleanup()\n</code></pre> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Clean up resources used by the inference engine.\n\n    This method should be called when the InferenceEngine is no longer needed.\n\n    Example:\n        ```python\n        sampler = SequenceSampler(unit_sampler, critic)\n        try:\n            sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\n        finally:\n            await sampler.cleanup()\n        ```\n    \"\"\"\n    await self.unit_sampler.cleanup()\n    if self.critic:\n        await self.critic.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.direct_token_sampler","title":"<code>direct_token_sampler(potential)</code>","text":"<p>Create a <code>DirectTokenSampler</code> that samples directly from a potential's vocabulary.</p> <p>See <code>DirectTokenSampler</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The potential function to sample from. Should have an efficient logw_next method.</p> required <p>Returns:</p> Type Description <code>DirectTokenSampler</code> <p>A sampler that directly samples tokens from the potential's vocabulary.</p> Source code in <code>genlm/control/sampler/__init__.py</code> <pre><code>def direct_token_sampler(potential):\n    \"\"\"Create a `DirectTokenSampler` that samples directly from a potential's vocabulary.\n\n    See `DirectTokenSampler` for more details.\n\n    Args:\n        potential (Potential): The potential function to sample from. Should have an efficient logw_next method.\n\n    Returns:\n        (DirectTokenSampler): A sampler that directly samples tokens from the potential's vocabulary.\n    \"\"\"\n    assert isinstance(potential, Potential)\n    return DirectTokenSampler(potential)\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.eager_token_sampler","title":"<code>eager_token_sampler(iter_potential, item_potential)</code>","text":"<p>Create a <code>SetTokenSampler</code> that uses the <code>EagerSetSampler</code> to sample a set of tokens.</p> <p>See <code>EagerSetSampler</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>iter_potential</code> <code>Potential</code> <p>A potential function defined over a vocabulary of iterables.</p> required <code>item_potential</code> <code>Potential</code> <p>A potential function defined over a vocabulary of items which are elements of the iterables.</p> required <p>Returns:</p> Type Description <code>SetTokenSampler</code> <p>A sampler that wraps an <code>EagerSetSampler</code>.</p> Note <p>This is the fastest sampler in most cases.</p> Source code in <code>genlm/control/sampler/__init__.py</code> <pre><code>def eager_token_sampler(iter_potential, item_potential):\n    \"\"\"Create a `SetTokenSampler` that uses the `EagerSetSampler` to sample a set of tokens.\n\n    See `EagerSetSampler` for more details.\n\n    Args:\n        iter_potential (Potential): A potential function defined over a vocabulary of iterables.\n        item_potential (Potential): A potential function defined over a vocabulary of items which are elements of the iterables.\n\n    Returns:\n        (SetTokenSampler): A sampler that wraps an `EagerSetSampler`.\n\n    Note:\n        This is the fastest sampler in most cases.\n    \"\"\"\n    return SetTokenSampler(EagerSetSampler(iter_potential, item_potential))\n</code></pre>"},{"location":"reference/genlm/control/sampler/__init__/#genlm.control.sampler.topk_token_sampler","title":"<code>topk_token_sampler(iter_potential, item_potential, K)</code>","text":"<p>Create a <code>SetTokenSampler</code> that uses the <code>TopKSetSampler</code> to sample a set of tokens.</p> <p>See <code>TopKSetSampler</code> for more details.</p> <p>Parameters:</p> Name Type Description Default <code>iter_potential</code> <code>Potential</code> <p>A potential function defined over a vocabulary of iterables.</p> required <code>item_potential</code> <code>Potential</code> <p>A potential function defined over a vocabulary of items which are elements of the iterables.</p> required <code>K</code> <code>int | None</code> <p>The <code>K</code> parameter for the <code>TopKSetSampler</code>.</p> required <p>Returns:</p> Type Description <code>SetTokenSampler</code> <p>A sampler that wraps an <code>TopKSetSampler</code>.</p> Source code in <code>genlm/control/sampler/__init__.py</code> <pre><code>def topk_token_sampler(iter_potential, item_potential, K):\n    \"\"\"Create a `SetTokenSampler` that uses the `TopKSetSampler` to sample a set of tokens.\n\n    See `TopKSetSampler` for more details.\n\n    Args:\n        iter_potential (Potential): A potential function defined over a vocabulary of iterables.\n        item_potential (Potential): A potential function defined over a vocabulary of items which are elements of the iterables.\n        K (int|None): The `K` parameter for the `TopKSetSampler`.\n\n    Returns:\n        (SetTokenSampler): A sampler that wraps an `TopKSetSampler`.\n    \"\"\"\n    return SetTokenSampler(TopKSetSampler(iter_potential, item_potential, K))\n</code></pre>"},{"location":"reference/genlm/control/sampler/sequence/","title":"sequence","text":""},{"location":"reference/genlm/control/sampler/sequence/#genlm.control.sampler.sequence.SMC","title":"<code>SMC</code>","text":"<p>This class implements sequential Monte Carlo (SMC) inference for controlled text generation. The generation process works as follows:</p> <ol> <li> <p>Token Sampling: At each step, the <code>unit_sampler</code> is used to extend each particle (candidate sequence)    by sampling a new token. This grows all sequences by one token at a time. The sampler also outputs    an importance weight with each extension to correct for the myopic nature of token-by-token sampling.</p> </li> <li> <p>Critic Evaluation: If a <code>critic</code> is provided, it scores the updated sequences (via it's <code>score</code> method),    reweighting the particles based on how well they satisfy the constraints encoded by the critic.</p> </li> <li> <p>Resampling: When the effective sample size (ESS) falls below the threshold,    particles are resampled according to their weights. This helps focus computation    on more promising sequences.</p> </li> <li> <p>Termination: The process continues until either:</p> <ul> <li> <p>All sequences reach an end-of-sequence (EOS) token</p> </li> <li> <p>The maximum token length is reached</p> </li> </ul> </li> </ol> <p>If a critic is provided, the resulting sequences are properly weighted with respect to the product of the unit sampler's target potential and the critic potential (<code>unit_sampler.target * critic</code>). If a critic is not provided, the resulting sequences are weighted with respect to the unit sampler's target potential.</p> <p>Parameters:</p> Name Type Description Default <code>unit_sampler</code> <code>TokenSampler</code> <p>The sampler that generates tokens.</p> required <code>critic</code> <code>Potential</code> <p>A potential function that guides the generation process by scoring candidate sequences. Must have the same token type as the unit_sampler.</p> <code>None</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If unit_sampler is not a TokenSampler, if critic is not a Potential, or if the token types of unit_sampler and critic don't match.</p> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>class SMC:\n    \"\"\"This class implements sequential Monte Carlo (SMC) inference for controlled text generation.\n    The generation process works as follows:\n\n    1. Token Sampling: At each step, the `unit_sampler` is used to extend each particle (candidate sequence)\n       by sampling a new token. This grows all sequences by one token at a time. The sampler also outputs\n       an importance weight with each extension to correct for the myopic nature of token-by-token sampling.\n\n    2. Critic Evaluation: If a `critic` is provided, it scores the updated sequences (via it's `score` method),\n       reweighting the particles based on how well they satisfy the constraints encoded by the critic.\n\n    3. Resampling: When the effective sample size (ESS) falls below the threshold,\n       particles are resampled according to their weights. This helps focus computation\n       on more promising sequences.\n\n    4. Termination: The process continues until either:\\n\n        - All sequences reach an end-of-sequence (EOS) token\\n\n        - The maximum token length is reached\n\n    If a critic is provided, the resulting sequences are properly weighted with respect to the product of the unit sampler's\n    target potential and the critic potential (`unit_sampler.target * critic`). If a critic is not provided,\n    the resulting sequences are weighted with respect to the unit sampler's target potential.\n\n    Args:\n        unit_sampler (TokenSampler): The sampler that generates tokens.\n        critic (Potential, optional): A potential function that guides the generation process\n            by scoring candidate sequences. Must have the same token type as the unit_sampler.\n\n    Raises:\n        ValueError: If unit_sampler is not a TokenSampler, if critic is not a Potential,\n            or if the token types of unit_sampler and critic don't match.\n    \"\"\"\n\n    def __init__(self, unit_sampler, critic=None):\n        if not isinstance(unit_sampler, TokenSampler):\n            raise ValueError(\"`unit_sampler` must be a TokenSampler\")\n\n        if critic:\n            if not isinstance(critic, Potential):\n                raise ValueError(\"`critic` must be a Potential\")\n            if not unit_sampler.token_type == critic.token_type:\n                raise ValueError(\n                    \"`critic` must have the same token type as the `unit_sampler`. \"\n                    f\"Got {unit_sampler.token_type} and {critic.token_type}.\"\n                    + (\n                        \"\\nMaybe you forgot to coerce the critic to the token type of the unit sampler? See `Coerce`.\"\n                        if unit_sampler.token_type.is_iterable_of(critic.token_type)\n                        else \"\"\n                    )\n                )\n\n        self.unit_sampler = unit_sampler\n        self.critic = critic\n\n    async def __call__(\n        self,\n        n_particles,\n        ess_threshold,\n        max_tokens,\n        verbosity=0,\n        json_path=None,\n        **kwargs,\n    ):\n        \"\"\"Generate sequences using sequential Monte Carlo inference.\n\n        Args:\n            n_particles (int): Number of particles (candidate sequences) to maintain during\n                generation. Higher values provide better exploration but require more\n                computation.\n            ess_threshold (float): Effective sample size threshold for resampling,\n                expressed as a fraction of the number of particles. When ESS falls below\n                this value, particles are resampled according to their weights. Should be between 0 and 1.\n                Higher values lead to more frequent resampling. Note that when ess_threshold = 0,\n                the critic is only applied at the end of the generation (if it is provided).\n            max_tokens (int): Maximum number of tokens to generate per sequence. Generation\n                may terminate earlier if all sequences reach an EOS token.\n            verbosity (int, optional): Verbosity level for the SMC algorithm. 0 is silent, 1 prints the\n                particles at each step. Default is 0.\n            json_path (str, optional): JSON file path for saving a record of the inference run.\n                This can be used in conjunction with the `InferenceVisualizer` to visualize the inference run.\n            **kwargs (dict): Additional keyword arguments to pass to the SMC algorithm.\n                See the `llamppl.inference.smc_standard` documentation for more details.\n\n        Returns:\n            (Sequences): A container holding the generated sequences, their importance weights, and\n                other metadata from the generation process.\n        \"\"\"\n        model = SequenceModel(\n            unit_sampler=self.unit_sampler,\n            critic=self.critic,\n            max_tokens=max_tokens,\n            verbosity=verbosity,\n            twist_with_critic=ess_threshold &gt; 0,\n        )\n\n        particles = await smc_standard(\n            model=model,\n            n_particles=n_particles,\n            ess_threshold=ess_threshold,\n            json_file=json_path,\n            **kwargs,\n        )\n\n        return Sequences(*_unpack_particles(particles))\n\n    async def cleanup(self):\n        \"\"\"Clean up resources used by the inference engine.\n\n        This method should be called when the InferenceEngine is no longer needed.\n\n        Example:\n            ```python\n            sampler = SequenceSampler(unit_sampler, critic)\n            try:\n                sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\n            finally:\n                await sampler.cleanup()\n            ```\n        \"\"\"\n        await self.unit_sampler.cleanup()\n        if self.critic:\n            await self.critic.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/sequence/#genlm.control.sampler.sequence.SMC.__call__","title":"<code>__call__(n_particles, ess_threshold, max_tokens, verbosity=0, json_path=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate sequences using sequential Monte Carlo inference.</p> <p>Parameters:</p> Name Type Description Default <code>n_particles</code> <code>int</code> <p>Number of particles (candidate sequences) to maintain during generation. Higher values provide better exploration but require more computation.</p> required <code>ess_threshold</code> <code>float</code> <p>Effective sample size threshold for resampling, expressed as a fraction of the number of particles. When ESS falls below this value, particles are resampled according to their weights. Should be between 0 and 1. Higher values lead to more frequent resampling. Note that when ess_threshold = 0, the critic is only applied at the end of the generation (if it is provided).</p> required <code>max_tokens</code> <code>int</code> <p>Maximum number of tokens to generate per sequence. Generation may terminate earlier if all sequences reach an EOS token.</p> required <code>verbosity</code> <code>int</code> <p>Verbosity level for the SMC algorithm. 0 is silent, 1 prints the particles at each step. Default is 0.</p> <code>0</code> <code>json_path</code> <code>str</code> <p>JSON file path for saving a record of the inference run. This can be used in conjunction with the <code>InferenceVisualizer</code> to visualize the inference run.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to the SMC algorithm. See the <code>llamppl.inference.smc_standard</code> documentation for more details.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequences</code> <p>A container holding the generated sequences, their importance weights, and other metadata from the generation process.</p> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>async def __call__(\n    self,\n    n_particles,\n    ess_threshold,\n    max_tokens,\n    verbosity=0,\n    json_path=None,\n    **kwargs,\n):\n    \"\"\"Generate sequences using sequential Monte Carlo inference.\n\n    Args:\n        n_particles (int): Number of particles (candidate sequences) to maintain during\n            generation. Higher values provide better exploration but require more\n            computation.\n        ess_threshold (float): Effective sample size threshold for resampling,\n            expressed as a fraction of the number of particles. When ESS falls below\n            this value, particles are resampled according to their weights. Should be between 0 and 1.\n            Higher values lead to more frequent resampling. Note that when ess_threshold = 0,\n            the critic is only applied at the end of the generation (if it is provided).\n        max_tokens (int): Maximum number of tokens to generate per sequence. Generation\n            may terminate earlier if all sequences reach an EOS token.\n        verbosity (int, optional): Verbosity level for the SMC algorithm. 0 is silent, 1 prints the\n            particles at each step. Default is 0.\n        json_path (str, optional): JSON file path for saving a record of the inference run.\n            This can be used in conjunction with the `InferenceVisualizer` to visualize the inference run.\n        **kwargs (dict): Additional keyword arguments to pass to the SMC algorithm.\n            See the `llamppl.inference.smc_standard` documentation for more details.\n\n    Returns:\n        (Sequences): A container holding the generated sequences, their importance weights, and\n            other metadata from the generation process.\n    \"\"\"\n    model = SequenceModel(\n        unit_sampler=self.unit_sampler,\n        critic=self.critic,\n        max_tokens=max_tokens,\n        verbosity=verbosity,\n        twist_with_critic=ess_threshold &gt; 0,\n    )\n\n    particles = await smc_standard(\n        model=model,\n        n_particles=n_particles,\n        ess_threshold=ess_threshold,\n        json_file=json_path,\n        **kwargs,\n    )\n\n    return Sequences(*_unpack_particles(particles))\n</code></pre>"},{"location":"reference/genlm/control/sampler/sequence/#genlm.control.sampler.sequence.SMC.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Clean up resources used by the inference engine.</p> <p>This method should be called when the InferenceEngine is no longer needed.</p> Example <pre><code>sampler = SequenceSampler(unit_sampler, critic)\ntry:\n    sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\nfinally:\n    await sampler.cleanup()\n</code></pre> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Clean up resources used by the inference engine.\n\n    This method should be called when the InferenceEngine is no longer needed.\n\n    Example:\n        ```python\n        sampler = SequenceSampler(unit_sampler, critic)\n        try:\n            sequences = await sampler(n_particles=10, ess_threshold=0.5, max_tokens=20)\n        finally:\n            await sampler.cleanup()\n        ```\n    \"\"\"\n    await self.unit_sampler.cleanup()\n    if self.critic:\n        await self.critic.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/sequence/#genlm.control.sampler.sequence.Sequences","title":"<code>Sequences</code>  <code>dataclass</code>","text":"<p>Container for sequence samples with their weights and probabilities.</p> <p>Parameters:</p> Name Type Description Default <code>contexts</code> <code>list</code> <p>List of token sequences generated by the sampler.</p> required <code>log_weights</code> <code>list</code> <p>Log importance weights for each sequence.</p> required <p>Attributes:</p> Name Type Description <code>size</code> <code>int</code> <p>Number of sequences in the container.</p> <code>logp</code> <code>float</code> <p>Sum of log probabilities across all sequences.</p> <code>log_total</code> <code>float</code> <p>Log of the sum of importance weights.</p> <code>log_ml</code> <code>float</code> <p>Log marginal likelihood estimate.</p> <code>log_normalized_weights</code> <code>list</code> <p>Log weights normalized to sum to 1.</p> <code>log_ess</code> <code>float</code> <p>Log of the effective sample size.</p> <code>ess</code> <code>float</code> <p>Effective sample size of the particle population.</p> Source code in <code>genlm/control/sampler/sequence.py</code> <pre><code>@dataclass\nclass Sequences:\n    \"\"\"Container for sequence samples with their weights and probabilities.\n\n    Args:\n        contexts (list): List of token sequences generated by the sampler.\n        log_weights (list): Log importance weights for each sequence.\n\n    Attributes:\n        size (int): Number of sequences in the container.\n        logp (float): Sum of log probabilities across all sequences.\n        log_total (float): Log of the sum of importance weights.\n        log_ml (float): Log marginal likelihood estimate.\n        log_normalized_weights (list): Log weights normalized to sum to 1.\n        log_ess (float): Log of the effective sample size.\n        ess (float): Effective sample size of the particle population.\n    \"\"\"\n\n    contexts: list\n    log_weights: list\n\n    def __post_init__(self):\n        assert len(self.contexts) == len(self.log_weights)\n\n        if not isinstance(self.log_weights, np.ndarray):\n            self.log_weights = np.array(self.log_weights)\n\n        self.size = len(self.contexts)\n\n        # Handle case where all weights are -inf\n        if np.all(np.isneginf(self.log_weights)):\n            self.log_total = float(\"-inf\")\n            self.log_ml = float(\"-inf\")\n            self.log_normalized_weights = np.full_like(self.log_weights, float(\"-inf\"))\n            self.log_ess = float(\"-inf\")\n            self.ess = 0.0\n            return\n\n        self.log_total = logsumexp(self.log_weights)\n        max_weight = max(self.log_weights)\n        self.log_ml = (\n            np.log(np.mean(np.exp(self.log_weights - max_weight))) + max_weight\n        )\n        self.log_normalized_weights = self.log_weights - self.log_total\n        self.log_ess = -logsumexp(2 * self.log_normalized_weights)\n        self.ess = np.exp(self.log_ess)\n\n    @cached_property\n    def posterior(self):\n        \"\"\"Compute the estimated posterior distribution over sequences.\n\n        The probability of a sequence corresponds to its normalized weight. The probabilities\n        of duplicate sequences are summed.\n\n        Returns:\n            (Float.chart): A normalized chart mapping sequences to their posterior probabilities,\n                sorted in descending order by probability.\n        \"\"\"\n        posterior = Float.chart()\n        for sequence, prob in zip(self.contexts, self.normalized_weights):\n            posterior[tuple(sequence)] += prob\n        return posterior.normalize().sort_descending()\n\n    @cached_property\n    def decoded_posterior(self):\n        \"\"\"Compute posterior distribution over completed UTF-8 decodable sequences.\n\n        Filters for sequences that:\\n\n        1. End with an EndOfSequence token\\n\n        2. Can be decoded as UTF-8 strings\n\n        The probability of each sequence corresponds to its normalized weight among completed and decodable sequences.\n        Probabilities of duplicate sequences (after decoding) are summed.\n\n        To obtain the posterior distribution over all byte sequences, use `self.posterior`.\n\n        Returns:\n            (Float.chart): A normalized chart mapping decoded string sequences to their\n                posterior probabilities, sorted in descending order by probability.\n                Only includes sequences that meet both filtering criteria.\n        \"\"\"\n        posterior = Float.chart()\n        for sequence, w in zip(self.contexts, np.exp(self.log_weights)):\n            if sequence and isinstance(sequence[-1], EndOfSequence):\n                try:\n                    string_sequence = b\"\".join(sequence[:-1]).decode(\"utf-8\")\n                    posterior[string_sequence] += w\n                except UnicodeDecodeError:\n                    pass\n        return posterior.normalize().sort_descending()\n\n    @property\n    def normalized_weights(self):\n        \"\"\"Return exponential of normalized log weights.\"\"\"\n        if np.all(np.isneginf(self.log_weights)):\n            return np.full_like(self.log_weights, 0.0)\n        return np.exp(self.log_normalized_weights)\n\n    def __len__(self):\n        return self.size\n\n    def __iter__(self):\n        return iter(zip(self.contexts, self.log_weights))\n\n    def __getitem__(self, i):\n        return self.contexts[i], self.log_weights[i]\n\n    def __str__(self):\n        return str(self.decoded_posterior)\n\n    def _repr_html_(self):\n        return self.decoded_posterior._repr_html_()\n\n    def __repr__(self):\n        return str(self.decoded_posterior)\n\n    def show(self):\n        for p in sorted(self, reverse=True):\n            print(p)\n</code></pre>"},{"location":"reference/genlm/control/sampler/sequence/#genlm.control.sampler.sequence.Sequences.posterior","title":"<code>posterior</code>  <code>cached</code> <code>property</code>","text":"<p>Compute the estimated posterior distribution over sequences.</p> <p>The probability of a sequence corresponds to its normalized weight. The probabilities of duplicate sequences are summed.</p> <p>Returns:</p> Type Description <code>chart</code> <p>A normalized chart mapping sequences to their posterior probabilities, sorted in descending order by probability.</p>"},{"location":"reference/genlm/control/sampler/sequence/#genlm.control.sampler.sequence.Sequences.decoded_posterior","title":"<code>decoded_posterior</code>  <code>cached</code> <code>property</code>","text":"<p>Compute posterior distribution over completed UTF-8 decodable sequences.</p> <p>Filters for sequences that:</p> <ol> <li> <p>End with an EndOfSequence token</p> </li> <li> <p>Can be decoded as UTF-8 strings</p> </li> </ol> <p>The probability of each sequence corresponds to its normalized weight among completed and decodable sequences. Probabilities of duplicate sequences (after decoding) are summed.</p> <p>To obtain the posterior distribution over all byte sequences, use <code>self.posterior</code>.</p> <p>Returns:</p> Type Description <code>chart</code> <p>A normalized chart mapping decoded string sequences to their posterior probabilities, sorted in descending order by probability. Only includes sequences that meet both filtering criteria.</p>"},{"location":"reference/genlm/control/sampler/sequence/#genlm.control.sampler.sequence.Sequences.normalized_weights","title":"<code>normalized_weights</code>  <code>property</code>","text":"<p>Return exponential of normalized log weights.</p>"},{"location":"reference/genlm/control/sampler/set/","title":"set","text":""},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.SetSampler","title":"<code>SetSampler</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Base class for set samplers.</p> <p>A set sampler samples a weighted set of tokens from a the vocabulary of a <code>target</code> potential.</p> <p>Given a context of tokens \\(x_1, \\ldots, x_{n-1}\\) in the target potential's vocabulary and a sampled set of tokens \\(S \\subseteq \\textsf{target.vocab_eos}\\), the log-weight associated with each token \\(x_n\\) must correspond to:</p> \\[     \\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1}) - \\log \\Pr(x_n \\in S) \\] <p>where \\(\\Pr(x_n \\in S)\\) is the probability the token was included in a sampled set.</p> <p>Attributes:</p> Name Type Description <code>target</code> <code>Potential</code> <p>The target potential with respect to which the set's weights are computed.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>class SetSampler(ABC):\n    \"\"\"Base class for set samplers.\n\n    A set sampler samples a weighted set of tokens from a the vocabulary of a `target` potential.\n\n    Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the target potential's vocabulary and a sampled set of tokens $S \\\\subseteq \\\\textsf{target.vocab_eos}$,\n    the log-weight associated with each token $x_n$ must correspond to:\n\n    $$\n        \\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1}) - \\\\log \\\\Pr(x_n \\\\in S)\n    $$\n\n    where $\\\\Pr(x_n \\\\in S)$ is the probability the token was included in a sampled set.\n\n    Attributes:\n        target (Potential): The target potential with respect to which the set's weights are computed.\n    \"\"\"\n\n    def __init__(self, target):\n        self.target = target\n\n    @abstractmethod\n    async def sample_set(self, context):\n        \"\"\"Sample a weighted set of tokens from the target potential's vocabulary.\"\"\"\n        pass  # pragma: no cover\n\n    async def cleanup(self):\n        pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.SetSampler.sample_set","title":"<code>sample_set(context)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Sample a weighted set of tokens from the target potential's vocabulary.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>@abstractmethod\nasync def sample_set(self, context):\n    \"\"\"Sample a weighted set of tokens from the target potential's vocabulary.\"\"\"\n    pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.TrieSetSampler","title":"<code>TrieSetSampler</code>","text":"<p>               Bases: <code>SetSampler</code></p> <p>TrieSetSampler is a specialized set sampler that utilizes a trie data structure to efficiently sample a weighted set of tokens.</p> <p>This sampler is designed to work with two potentials:</p> <ul> <li> <p>a potential over a vocabulary of iterables (<code>iter_potential</code>) and</p> </li> <li> <p>a potential over a vocabulary of items which are the elements of the iterables (<code>item_potential</code>).</p> </li> </ul> <p>For example, if <code>iter_potential</code> is a potential over byte sequences, then <code>item_potential</code> is a potential over bytes.</p> <p>The target potential is the product of <code>iter_potential</code> and the <code>item_potential</code> coerced to operate on the token type of <code>iter_potential</code>. Thus, <code>TrieSetSampler</code>s sample tokens from the <code>iter_potential</code>'s vocabulary.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>class TrieSetSampler(SetSampler):\n    \"\"\"\n    TrieSetSampler is a specialized set sampler that utilizes a trie data structure to efficiently sample a weighted set of tokens.\n\n    This sampler is designed to work with two potentials:\\n\n    - a potential over a vocabulary of iterables (`iter_potential`) and\\n\n    - a potential over a vocabulary of items which are the elements of the iterables (`item_potential`).\n\n    For example, if `iter_potential` is a potential over byte sequences, then `item_potential` is a potential over bytes.\n\n    The target potential is the product of `iter_potential` and the `item_potential` coerced to operate on the token type of `iter_potential`. Thus,\n    `TrieSetSampler`s sample tokens from the `iter_potential`'s vocabulary.\n    \"\"\"\n\n    def __init__(self, iter_potential, item_potential):\n        \"\"\"\n        Initialize the `TrieSetSampler`.\n\n        Args:\n            iter_potential (Potential): The potential defined over a vocabulary of iterables.\n            item_potential (Potential): The potential defined over a vocabulary of items.\n\n        Raises:\n            ValueError: If the token type of `iter_potential` is not an iterable of the token type of `item_potential`.\n        \"\"\"\n        if not iter_potential.token_type.is_iterable_of(item_potential.token_type):\n            raise ValueError(\n                \"Token type of `iter_potential` must be an iterable of token type of `item_potential`. \"\n                f\"Got {iter_potential.token_type} and {item_potential.token_type}.\"\n            )\n        self.iter_potential = iter_potential\n        self.item_potential = item_potential\n        self.f = lambda context: [item for items in context for item in items]\n\n        super().__init__(\n            iter_potential * item_potential.coerce(iter_potential, f=self.f)\n        )\n\n        self.trie_executor = load_async_trie(\n            self.iter_potential.vocab_eos, backend=\"parallel\"\n        )\n        self.trie = self.trie_executor.trie\n\n        vocab_eos = self.target.vocab_eos\n        word2leaf = self.trie.word2leaf\n        lookup = self.target.lookup\n\n        common_tokens = set(vocab_eos) &amp; set(word2leaf)\n\n        self.leaf_to_token_id = dict(\n            (word2leaf[token], lookup[token]) for token in common_tokens\n        )\n\n    async def sample_set(self, context):\n        \"\"\"\n        Sample a weighted set of tokens given a context.\n\n        Args:\n            context (list): The sequence to condition on.\n\n        Returns:\n            (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n\n        Raises:\n            NotImplementedError: If the method is not implemented in subclasses.\n        \"\"\"\n        raise NotImplementedError(\n            \"Subclasses must implement sample_set\"\n        )  # pragma: no cover\n\n    async def cleanup(self):\n        \"\"\"\n        Cleanup the TrieSetSampler. It is recommended to call this method at the end of usage.\n        \"\"\"\n        await self.trie_executor.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.TrieSetSampler.__init__","title":"<code>__init__(iter_potential, item_potential)</code>","text":"<p>Initialize the <code>TrieSetSampler</code>.</p> <p>Parameters:</p> Name Type Description Default <code>iter_potential</code> <code>Potential</code> <p>The potential defined over a vocabulary of iterables.</p> required <code>item_potential</code> <code>Potential</code> <p>The potential defined over a vocabulary of items.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the token type of <code>iter_potential</code> is not an iterable of the token type of <code>item_potential</code>.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>def __init__(self, iter_potential, item_potential):\n    \"\"\"\n    Initialize the `TrieSetSampler`.\n\n    Args:\n        iter_potential (Potential): The potential defined over a vocabulary of iterables.\n        item_potential (Potential): The potential defined over a vocabulary of items.\n\n    Raises:\n        ValueError: If the token type of `iter_potential` is not an iterable of the token type of `item_potential`.\n    \"\"\"\n    if not iter_potential.token_type.is_iterable_of(item_potential.token_type):\n        raise ValueError(\n            \"Token type of `iter_potential` must be an iterable of token type of `item_potential`. \"\n            f\"Got {iter_potential.token_type} and {item_potential.token_type}.\"\n        )\n    self.iter_potential = iter_potential\n    self.item_potential = item_potential\n    self.f = lambda context: [item for items in context for item in items]\n\n    super().__init__(\n        iter_potential * item_potential.coerce(iter_potential, f=self.f)\n    )\n\n    self.trie_executor = load_async_trie(\n        self.iter_potential.vocab_eos, backend=\"parallel\"\n    )\n    self.trie = self.trie_executor.trie\n\n    vocab_eos = self.target.vocab_eos\n    word2leaf = self.trie.word2leaf\n    lookup = self.target.lookup\n\n    common_tokens = set(vocab_eos) &amp; set(word2leaf)\n\n    self.leaf_to_token_id = dict(\n        (word2leaf[token], lookup[token]) for token in common_tokens\n    )\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.TrieSetSampler.sample_set","title":"<code>sample_set(context)</code>  <code>async</code>","text":"<p>Sample a weighted set of tokens given a context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>The sequence to condition on.</p> required <p>Returns:</p> Type Description <code>(LazyWeights, float)</code> <p>A weighted set of tokens and the log-probability of the sampled set.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the method is not implemented in subclasses.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>async def sample_set(self, context):\n    \"\"\"\n    Sample a weighted set of tokens given a context.\n\n    Args:\n        context (list): The sequence to condition on.\n\n    Returns:\n        (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n\n    Raises:\n        NotImplementedError: If the method is not implemented in subclasses.\n    \"\"\"\n    raise NotImplementedError(\n        \"Subclasses must implement sample_set\"\n    )  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.TrieSetSampler.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Cleanup the TrieSetSampler. It is recommended to call this method at the end of usage.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>async def cleanup(self):\n    \"\"\"\n    Cleanup the TrieSetSampler. It is recommended to call this method at the end of usage.\n    \"\"\"\n    await self.trie_executor.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.EagerSetSampler","title":"<code>EagerSetSampler</code>","text":"<p>               Bases: <code>TrieSetSampler</code></p> <p>A trie-based set sampler that implements an eager sampling strategy for generating a set of tokens.</p> <p>An <code>EagerSetSampler</code> samples tokens by incrementally sampling items from the item-wise product of the <code>iter_potential</code> and <code>item_potential</code>. The sampled set is the set of sequences of items that correspond to valid tokens in <code>iter_potential</code>'s vocabulary.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>class EagerSetSampler(TrieSetSampler):\n    \"\"\"\n    A trie-based set sampler that implements an eager sampling strategy\n    for generating a set of tokens.\n\n    An `EagerSetSampler` samples tokens by incrementally sampling items from the item-wise product of the `iter_potential` and `item_potential`.\n    The sampled set is the set of sequences of items that correspond to valid tokens in `iter_potential`'s vocabulary.\n    \"\"\"\n\n    async def sample_set(self, context, draw=None):\n        \"\"\"\n        Sample a set of tokens given a context.\n\n        Args:\n            context (list): A sequence of tokens in the `iter_potential`'s vocabulary.\n\n        Returns:\n            (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n        \"\"\"\n        if draw is None:\n            draw = sample_dict\n        iter_logws = await self.iter_potential.logw_next(context)\n        item_ws = await self.trie_executor.weight_sum(iter_logws.exp().weights)\n\n        logws = self.target.alloc_logws()\n        curr = self.trie.root\n        coerced_ctx = self.f(context)\n        subtokens = []\n        logp, logw = 0, 0\n\n        while True:\n            children = self.trie.children[curr]\n            item_w_curr = item_ws[curr]\n            item_ws1 = Float.chart(\n                {a: item_ws[c] / item_w_curr for a, c in children.items()}\n            )\n\n            if None in item_ws1:\n                leaf = children[None]\n                token = self.trie.leaf2word[leaf]\n                token_id = self.leaf_to_token_id[leaf]\n                logws[token_id] = iter_logws[token] + logw - logp\n\n            item_logws2 = await self.item_potential.logw_next(coerced_ctx + subtokens)\n            item_ws2 = item_logws2.exp().materialize()\n            w_next = (item_ws1 * item_ws2).trim()\n\n            if not w_next:\n                break\n\n            ps = w_next.normalize()\n            b = draw(ps)\n            logp += np.log(ps[b])\n            logw += item_logws2[b]\n\n            if b == self.target.eos:\n                assert not subtokens, \"subtokens should be empty at EOS.\"\n                logws[-1] = iter_logws[self.target.eos] + logw - logp\n                break\n\n            subtokens.append(b)\n            curr = children[b]\n\n        return self.target.make_lazy_weights(logws), logp\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.EagerSetSampler.sample_set","title":"<code>sample_set(context, draw=None)</code>  <code>async</code>","text":"<p>Sample a set of tokens given a context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the <code>iter_potential</code>'s vocabulary.</p> required <p>Returns:</p> Type Description <code>(LazyWeights, float)</code> <p>A weighted set of tokens and the log-probability of the sampled set.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>async def sample_set(self, context, draw=None):\n    \"\"\"\n    Sample a set of tokens given a context.\n\n    Args:\n        context (list): A sequence of tokens in the `iter_potential`'s vocabulary.\n\n    Returns:\n        (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n    \"\"\"\n    if draw is None:\n        draw = sample_dict\n    iter_logws = await self.iter_potential.logw_next(context)\n    item_ws = await self.trie_executor.weight_sum(iter_logws.exp().weights)\n\n    logws = self.target.alloc_logws()\n    curr = self.trie.root\n    coerced_ctx = self.f(context)\n    subtokens = []\n    logp, logw = 0, 0\n\n    while True:\n        children = self.trie.children[curr]\n        item_w_curr = item_ws[curr]\n        item_ws1 = Float.chart(\n            {a: item_ws[c] / item_w_curr for a, c in children.items()}\n        )\n\n        if None in item_ws1:\n            leaf = children[None]\n            token = self.trie.leaf2word[leaf]\n            token_id = self.leaf_to_token_id[leaf]\n            logws[token_id] = iter_logws[token] + logw - logp\n\n        item_logws2 = await self.item_potential.logw_next(coerced_ctx + subtokens)\n        item_ws2 = item_logws2.exp().materialize()\n        w_next = (item_ws1 * item_ws2).trim()\n\n        if not w_next:\n            break\n\n        ps = w_next.normalize()\n        b = draw(ps)\n        logp += np.log(ps[b])\n        logw += item_logws2[b]\n\n        if b == self.target.eos:\n            assert not subtokens, \"subtokens should be empty at EOS.\"\n            logws[-1] = iter_logws[self.target.eos] + logw - logp\n            break\n\n        subtokens.append(b)\n        curr = children[b]\n\n    return self.target.make_lazy_weights(logws), logp\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.TopKSetSampler","title":"<code>TopKSetSampler</code>","text":"<p>               Bases: <code>TrieSetSampler</code></p> <p>A trie-based set sampler that lazily enumerates the top K tokens by weight in the target, and samples an additional \"wildcard\" token to ensure absolute continuity.</p> Warning <p>This sampler is not guaranteed to be correct if the <code>item_potential</code>'s prefix weights do not monotonically decrease with the length of the context. That is, \\(\\textsf{item_potential.prefix}(x) \\leq \\textsf{item_potential.prefix}(xy)\\) for all sequences of items \\(x, y\\).</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>class TopKSetSampler(TrieSetSampler):\n    \"\"\"\n    A trie-based set sampler that lazily enumerates the top K tokens by weight in the target,\n    and samples an additional \"wildcard\" token to ensure absolute continuity.\n\n    Warning:\n        This sampler is not guaranteed to be correct if the `item_potential`'s\n        prefix weights do not monotonically decrease with the length of the context.\n        That is, $\\\\textsf{item_potential.prefix}(x) \\\\leq \\\\textsf{item_potential.prefix}(xy)$ for all sequences of items $x, y$.\n    \"\"\"\n\n    def __init__(self, iter_potential, item_potential, K):\n        \"\"\"\n        Initialize the TopKSetSampler.\n\n        Args:\n            iter_potential (Potential): The potential defined over a vocabulary of iterables.\n            item_potential (Potential): The potential defined over a vocabulary of items.\n            K (int|None): The number of top tokens to enumerate. If None, all tokens are enumerated.\n        \"\"\"\n        if K is not None and K &lt;= 0:\n            raise ValueError(\"K must be greater than 0 or None\")\n        super().__init__(iter_potential, item_potential)\n        self.K = K\n\n    async def sample_set(self, context, draw=None):\n        \"\"\"\n        Sample a set of tokens given a context.\n\n        Args:\n            context (list): A sequence of tokens in the `iter_potential`'s vocabulary.\n\n        Returns:\n            (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n        \"\"\"\n        if draw is None:\n            draw = sample_dict\n        iter_logws = await self.iter_potential.logw_next(context)\n        max_logws = await self.trie_executor.weight_max(iter_logws.weights)\n\n        k = 0\n        logws = self.target.alloc_logws()\n        sampled = self.target.alloc_logws(default=False)\n\n        async for token_id, logw in self._lazy_enum(context, max_logws):\n            logws[token_id] = logw\n            sampled[token_id] = True\n            k += 1\n            if self.K is not None and k &gt;= self.K:\n                break\n\n        logp_wc = 0\n        if self.K is not None and k == self.K:\n            # Get the distribution over wildcard tokens\n            iter_ws = iter_logws.exp()\n            W_wc = Float.chart(\n                {\n                    token_id: iter_ws[token]\n                    for token_id, token in enumerate(self.target.vocab_eos)\n                    if not sampled[token_id]\n                }\n            )\n\n            # if W_wc is non-empty, sample a wildcard token to ensure absolute continuity\n            if W_wc:\n                P_wc = W_wc.normalize()\n                wc_id = draw(P_wc)\n                logp_wc = np.log(P_wc[wc_id])\n                wc = self.target.vocab_eos[wc_id]\n                item_ctx = self.f(context)\n                prefix_w = await self.item_potential.prefix(item_ctx)\n                if wc == self.target.eos:\n                    w_guide_wc = await self.item_potential.complete(item_ctx) - prefix_w\n                else:\n                    w_guide_wc = (\n                        await self.item_potential.prefix(self.f(context + [wc]))\n                        - prefix_w\n                    )\n                logws[wc_id] = np.log(W_wc[wc_id]) + w_guide_wc - logp_wc\n\n        return self.target.make_lazy_weights(logws), logp_wc\n\n    async def _lazy_enum(self, context, max_logws):\n        agenda = LocatorMaxHeap()\n\n        W = Float.chart()\n\n        # initial conditions\n        (token, node) = ((), self.trie.root)\n        agenda[token, node, False] = max_logws[node]\n        W[node] = 0\n\n        children = self.trie.children\n        coerced_ctx = self.f(context)\n\n        curr_priority = float(\"inf\")\n        prev_best = float(\"inf\")\n        while agenda:\n            (token, node, done), score = agenda.popitem()\n\n            assert score &lt;= curr_priority, (\n                \"Monotonicity assumption violated. \"\n                \"`item_potential` prefix weight must be monotonically decreasing.\"\n            )\n            curr_priority = score\n\n            # terminal state\n            if done:\n                value = W[node] + max_logws[node]\n                assert prev_best &gt;= value\n                prev_best = value\n                yield (self.leaf_to_token_id[node], value)\n                continue\n\n            logws = None\n            for x, y in children[node].items():\n                if x is None:\n                    W_y = W[node]\n                    W[y] = W_y\n                    agenda[token, y, True] = W_y + max_logws[y]\n                else:\n                    if logws is None:\n                        logws = await self.item_potential.logw_next(\n                            coerced_ctx + list(token)\n                        )\n                    W_y = W[node] + logws[x]\n                    if W_y == float(\"-inf\"):\n                        continue\n                    W[y] = W_y\n                    agenda[(*token, x), y, False] = W_y + max_logws[y]\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.TopKSetSampler.__init__","title":"<code>__init__(iter_potential, item_potential, K)</code>","text":"<p>Initialize the TopKSetSampler.</p> <p>Parameters:</p> Name Type Description Default <code>iter_potential</code> <code>Potential</code> <p>The potential defined over a vocabulary of iterables.</p> required <code>item_potential</code> <code>Potential</code> <p>The potential defined over a vocabulary of items.</p> required <code>K</code> <code>int | None</code> <p>The number of top tokens to enumerate. If None, all tokens are enumerated.</p> required Source code in <code>genlm/control/sampler/set.py</code> <pre><code>def __init__(self, iter_potential, item_potential, K):\n    \"\"\"\n    Initialize the TopKSetSampler.\n\n    Args:\n        iter_potential (Potential): The potential defined over a vocabulary of iterables.\n        item_potential (Potential): The potential defined over a vocabulary of items.\n        K (int|None): The number of top tokens to enumerate. If None, all tokens are enumerated.\n    \"\"\"\n    if K is not None and K &lt;= 0:\n        raise ValueError(\"K must be greater than 0 or None\")\n    super().__init__(iter_potential, item_potential)\n    self.K = K\n</code></pre>"},{"location":"reference/genlm/control/sampler/set/#genlm.control.sampler.set.TopKSetSampler.sample_set","title":"<code>sample_set(context, draw=None)</code>  <code>async</code>","text":"<p>Sample a set of tokens given a context.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list</code> <p>A sequence of tokens in the <code>iter_potential</code>'s vocabulary.</p> required <p>Returns:</p> Type Description <code>(LazyWeights, float)</code> <p>A weighted set of tokens and the log-probability of the sampled set.</p> Source code in <code>genlm/control/sampler/set.py</code> <pre><code>async def sample_set(self, context, draw=None):\n    \"\"\"\n    Sample a set of tokens given a context.\n\n    Args:\n        context (list): A sequence of tokens in the `iter_potential`'s vocabulary.\n\n    Returns:\n        (LazyWeights, float): A weighted set of tokens and the log-probability of the sampled set.\n    \"\"\"\n    if draw is None:\n        draw = sample_dict\n    iter_logws = await self.iter_potential.logw_next(context)\n    max_logws = await self.trie_executor.weight_max(iter_logws.weights)\n\n    k = 0\n    logws = self.target.alloc_logws()\n    sampled = self.target.alloc_logws(default=False)\n\n    async for token_id, logw in self._lazy_enum(context, max_logws):\n        logws[token_id] = logw\n        sampled[token_id] = True\n        k += 1\n        if self.K is not None and k &gt;= self.K:\n            break\n\n    logp_wc = 0\n    if self.K is not None and k == self.K:\n        # Get the distribution over wildcard tokens\n        iter_ws = iter_logws.exp()\n        W_wc = Float.chart(\n            {\n                token_id: iter_ws[token]\n                for token_id, token in enumerate(self.target.vocab_eos)\n                if not sampled[token_id]\n            }\n        )\n\n        # if W_wc is non-empty, sample a wildcard token to ensure absolute continuity\n        if W_wc:\n            P_wc = W_wc.normalize()\n            wc_id = draw(P_wc)\n            logp_wc = np.log(P_wc[wc_id])\n            wc = self.target.vocab_eos[wc_id]\n            item_ctx = self.f(context)\n            prefix_w = await self.item_potential.prefix(item_ctx)\n            if wc == self.target.eos:\n                w_guide_wc = await self.item_potential.complete(item_ctx) - prefix_w\n            else:\n                w_guide_wc = (\n                    await self.item_potential.prefix(self.f(context + [wc]))\n                    - prefix_w\n                )\n            logws[wc_id] = np.log(W_wc[wc_id]) + w_guide_wc - logp_wc\n\n    return self.target.make_lazy_weights(logws), logp_wc\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/","title":"token","text":""},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.TokenSampler","title":"<code>TokenSampler</code>","text":"<p>               Bases: <code>SubModel</code></p> <p>Base class for sampling a token from a potential's vocabulary.</p> <p><code>TokenSampler</code>s generate properly weighted samples with respect to a <code>target</code> potential.</p> <p>Given a context of tokens \\(x_1, \\ldots, x_{n-1}\\) in the target potential's vocabulary, a <code>TokenSampler</code> samples a token \\(x_n \\in \\textsf{target.vocab_eos}\\) and weight \\(w\\).</p> <p>The sampled token and weight are properly weighted with respect to $$ \\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1}) $$</p> <p>Parameters:</p> Name Type Description Default <code>target</code> <code>Potential</code> <p>The potential that samples are properly weighted with respect to.</p> required Source code in <code>genlm/control/sampler/token.py</code> <pre><code>class TokenSampler(SubModel):\n    \"\"\"Base class for sampling a token from a potential's vocabulary.\n\n    `TokenSampler`s generate properly weighted samples with respect to a `target` potential.\n\n    Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the target potential's vocabulary,\n    a `TokenSampler` samples a token $x_n \\\\in \\\\textsf{target.vocab_eos}$ and weight $w$.\n\n    The sampled token and weight are properly weighted with respect to\n    $$\n    \\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n    $$\n\n    Args:\n        target (Potential): The potential that samples are properly weighted with respect to.\n    \"\"\"\n\n    def __init__(self, target):\n        super().__init__()\n        self.target = target\n        self.token_type = self.target.token_type\n\n    async def start_weight(self):\n        \"\"\"Compute the weight of the empty sequence under the target potential.\"\"\"\n        return await self.target.prefix([])\n\n    async def forward(self):\n        parent = self.parent  # For some reason, need to hold onto this reference.\n        token, logw, logp = await self.sample(parent.token_ctx)\n        parent.score(logw)\n        parent.logp += logp\n        return token\n\n    async def sample(self, context, draw):\n        \"\"\"Sample a token and weight from the `target`potential's vocabulary.\n\n        Args:\n            context (list[int]): A sequence of tokens in the `target` potential's vocabulary.\n            draw (callable): A callable that draws a sample from a distribution.\n\n        Returns:\n            (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the sampled token.\n        \"\"\"\n        raise NotImplementedError(\n            \"Subclasses must implement sample method\"\n        )  # pragma: no cover\n\n    async def cleanup(self):\n        pass  # pragma: no cover\n\n    async def smc(self, n_particles, ess_threshold, max_tokens, critic=None, **kwargs):\n        \"\"\"Generate sequences using sequential Monte Carlo (SMC) inference with this token sampler and an optional critic.\n\n        This method is a convenience wrapper around [`SMC`][genlm.control.sampler.sequence.SMC].\n        See [`SMC`][genlm.control.sampler.sequence.SMC] for more details on the generation process.\n\n        Args:\n            n_particles (int): The number of particles to use in the SMC algorithm.\n            ess_threshold (float): The threshold for the effective sample size (ESS).\n            max_tokens (int): The maximum number of tokens to generate.\n            critic (Potential, optional): A potential function that guides the generation process\n                by scoring candidate sequences. Must have the same token type as the token sampler.\n            **kwargs (dict): Additional keyword arguments to pass to `SMC`'s `__call__` method.\n        \"\"\"\n        from genlm.control.sampler.sequence import SMC\n\n        return await SMC(self, critic)(\n            n_particles=n_particles,\n            ess_threshold=ess_threshold,\n            max_tokens=max_tokens,\n            **kwargs,\n        )\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.TokenSampler.start_weight","title":"<code>start_weight()</code>  <code>async</code>","text":"<p>Compute the weight of the empty sequence under the target potential.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def start_weight(self):\n    \"\"\"Compute the weight of the empty sequence under the target potential.\"\"\"\n    return await self.target.prefix([])\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.TokenSampler.sample","title":"<code>sample(context, draw)</code>  <code>async</code>","text":"<p>Sample a token and weight from the <code>target</code>potential's vocabulary.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[int]</code> <p>A sequence of tokens in the <code>target</code> potential's vocabulary.</p> required <code>draw</code> <code>callable</code> <p>A callable that draws a sample from a distribution.</p> required <p>Returns:</p> Type Description <code>(token, weight, logp)</code> <p>A tuple containing the sampled token, weight, and log-probability of the sampled token.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def sample(self, context, draw):\n    \"\"\"Sample a token and weight from the `target`potential's vocabulary.\n\n    Args:\n        context (list[int]): A sequence of tokens in the `target` potential's vocabulary.\n        draw (callable): A callable that draws a sample from a distribution.\n\n    Returns:\n        (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the sampled token.\n    \"\"\"\n    raise NotImplementedError(\n        \"Subclasses must implement sample method\"\n    )  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.TokenSampler.smc","title":"<code>smc(n_particles, ess_threshold, max_tokens, critic=None, **kwargs)</code>  <code>async</code>","text":"<p>Generate sequences using sequential Monte Carlo (SMC) inference with this token sampler and an optional critic.</p> <p>This method is a convenience wrapper around <code>SMC</code>. See <code>SMC</code> for more details on the generation process.</p> <p>Parameters:</p> Name Type Description Default <code>n_particles</code> <code>int</code> <p>The number of particles to use in the SMC algorithm.</p> required <code>ess_threshold</code> <code>float</code> <p>The threshold for the effective sample size (ESS).</p> required <code>max_tokens</code> <code>int</code> <p>The maximum number of tokens to generate.</p> required <code>critic</code> <code>Potential</code> <p>A potential function that guides the generation process by scoring candidate sequences. Must have the same token type as the token sampler.</p> <code>None</code> <code>**kwargs</code> <code>dict</code> <p>Additional keyword arguments to pass to <code>SMC</code>'s <code>__call__</code> method.</p> <code>{}</code> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def smc(self, n_particles, ess_threshold, max_tokens, critic=None, **kwargs):\n    \"\"\"Generate sequences using sequential Monte Carlo (SMC) inference with this token sampler and an optional critic.\n\n    This method is a convenience wrapper around [`SMC`][genlm.control.sampler.sequence.SMC].\n    See [`SMC`][genlm.control.sampler.sequence.SMC] for more details on the generation process.\n\n    Args:\n        n_particles (int): The number of particles to use in the SMC algorithm.\n        ess_threshold (float): The threshold for the effective sample size (ESS).\n        max_tokens (int): The maximum number of tokens to generate.\n        critic (Potential, optional): A potential function that guides the generation process\n            by scoring candidate sequences. Must have the same token type as the token sampler.\n        **kwargs (dict): Additional keyword arguments to pass to `SMC`'s `__call__` method.\n    \"\"\"\n    from genlm.control.sampler.sequence import SMC\n\n    return await SMC(self, critic)(\n        n_particles=n_particles,\n        ess_threshold=ess_threshold,\n        max_tokens=max_tokens,\n        **kwargs,\n    )\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.DirectTokenSampler","title":"<code>DirectTokenSampler</code>","text":"<p>               Bases: <code>TokenSampler</code></p> <p>Samples individual tokens directly from the log-normalized <code>logw_next</code> function of a potential.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The potential function to sample from</p> required Warning <p>Only use this sampler if the potential's <code>logw_next</code> method is efficient. This is the case for potentials like <code>PromptedLLM</code>, but for custom potentials with a large vocabulary size, the default implementation of <code>logw_next</code> generally will not be efficient, and thus this sampler will be slow.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>class DirectTokenSampler(TokenSampler):\n    \"\"\"Samples individual tokens directly from the log-normalized `logw_next` function\n    of a potential.\n\n    Args:\n        potential (Potential): The potential function to sample from\n\n    Warning:\n        Only use this sampler if the potential's `logw_next` method is efficient. This is the case\n        for potentials like `PromptedLLM`, but for custom potentials with a large vocabulary size,\n        the default implementation of `logw_next` generally will not be efficient, and thus this\n        sampler will be slow.\n    \"\"\"\n\n    def __init__(self, potential):\n        super().__init__(target=potential)\n        self.potential = potential\n\n    async def sample(self, context, draw=None):\n        \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method.\n\n        Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the target potential's vocabulary,\n        this method samples a token $x_n \\\\in \\\\textsf{target.vocab_eos}$ and weight $w$.\n\n        The sampled token and weight are properly weighted with respect to\n        $$\n        \\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n        $$\n\n        The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n        Returns:\n            (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the sampled token.\n        \"\"\"\n        logws = await self.potential.logw_next(context)\n        logps = logws.normalize()\n        if draw is None:\n            # fast sampling from logps using gumbel-max trick\n            token = fast_sample_lazyweights(logps)\n        else:\n            token = draw(logps.exp().materialize())\n        return token, logws.sum(), logps[token]\n\n    async def cleanup(self):\n        pass  # pragma: no cover\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.DirectTokenSampler.sample","title":"<code>sample(context, draw=None)</code>  <code>async</code>","text":"<p>Sample a token and weight that are properly weighted with respect to the target potential's <code>logw_next</code> method.</p> <p>Given a context of tokens \\(x_1, \\ldots, x_{n-1}\\) in the target potential's vocabulary, this method samples a token \\(x_n \\in \\textsf{target.vocab_eos}\\) and weight \\(w\\).</p> <p>The sampled token and weight are properly weighted with respect to $$ \\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1}) $$</p> <p>The returned weight corresponds to the log normalizing constant of \\(\\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1})\\).</p> <p>Returns:</p> Type Description <code>(token, weight, logp)</code> <p>A tuple containing the sampled token, weight, and log-probability of the sampled token.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def sample(self, context, draw=None):\n    \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method.\n\n    Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the target potential's vocabulary,\n    this method samples a token $x_n \\\\in \\\\textsf{target.vocab_eos}$ and weight $w$.\n\n    The sampled token and weight are properly weighted with respect to\n    $$\n    \\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n    $$\n\n    The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n    Returns:\n        (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the sampled token.\n    \"\"\"\n    logws = await self.potential.logw_next(context)\n    logps = logws.normalize()\n    if draw is None:\n        # fast sampling from logps using gumbel-max trick\n        token = fast_sample_lazyweights(logps)\n    else:\n        token = draw(logps.exp().materialize())\n    return token, logws.sum(), logps[token]\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.SetTokenSampler","title":"<code>SetTokenSampler</code>","text":"<p>               Bases: <code>TokenSampler</code></p> <p>Samples individual tokens by sampling a weighted set of tokens and then selecting one proportional to its weight.</p> <p>This class wraps a <code>SetSampler</code>.</p> <p>Parameters:</p> Name Type Description Default <code>set_sampler</code> <code>SetSampler</code> <p>The set sampler to sample from</p> required Source code in <code>genlm/control/sampler/token.py</code> <pre><code>class SetTokenSampler(TokenSampler):\n    \"\"\"Samples individual tokens by sampling a weighted set of tokens and then selecting one\n    proportional to its weight.\n\n    This class wraps a `SetSampler`.\n\n    Args:\n        set_sampler (SetSampler): The set sampler to sample from\n    \"\"\"\n\n    def __init__(self, set_sampler):\n        assert isinstance(set_sampler, SetSampler)\n        super().__init__(set_sampler.target)\n        self.set_sampler = set_sampler\n\n    async def sample(self, context, draw=None):\n        \"\"\"Sample a token and weight by sampling a weighted set of tokens from the `set_sampler`\n        and then selecting one proportional to its weight.\n\n        Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the vocabulary of the set sampler's target potential,\n        this method samples a token $x_n \\\\in \\\\textsf{set_sampler.target.vocab_eos}$ and a weight.\n\n        The sampled token and weight are properly weighted with respect to\n        $$\n        \\\\textsf{set_sampler.target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n        $$\n\n        The returned weight corresponds to the sum of the weights of the sampled set.\n\n        Args:\n            context (list[int]): A sequence of tokens in the vocabulary of the set sampler's target potential.\n\n        Returns:\n            (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the random\n                choices made in sampling that token.\n\n        Note:\n            For properly weighted sampling, the `set_sampler` must assign correct weights to each token. See\n            `SetSampler` for more details.\n        \"\"\"\n        logws, logp = await self.set_sampler.sample_set(context, draw=draw)\n        logps = logws.normalize()\n        if draw is None:\n            token = fast_sample_lazyweights(logps)\n        else:\n            token = draw(logps.exp().materialize())\n        return token, logws.sum(), logp + logps[token]\n\n    async def cleanup(self):\n        \"\"\"Clean up the sampler.\n\n        This method should be called when the sampler is no longer needed.\n        \"\"\"\n        await self.set_sampler.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.SetTokenSampler.sample","title":"<code>sample(context, draw=None)</code>  <code>async</code>","text":"<p>Sample a token and weight by sampling a weighted set of tokens from the <code>set_sampler</code> and then selecting one proportional to its weight.</p> <p>Given a context of tokens \\(x_1, \\ldots, x_{n-1}\\) in the vocabulary of the set sampler's target potential, this method samples a token \\(x_n \\in \\textsf{set_sampler.target.vocab_eos}\\) and a weight.</p> <p>The sampled token and weight are properly weighted with respect to $$ \\textsf{set_sampler.target.logw_next}(x_n | x_1, \\ldots, x_{n-1}) $$</p> <p>The returned weight corresponds to the sum of the weights of the sampled set.</p> <p>Parameters:</p> Name Type Description Default <code>context</code> <code>list[int]</code> <p>A sequence of tokens in the vocabulary of the set sampler's target potential.</p> required <p>Returns:</p> Type Description <code>(token, weight, logp)</code> <p>A tuple containing the sampled token, weight, and log-probability of the random choices made in sampling that token.</p> Note <p>For properly weighted sampling, the <code>set_sampler</code> must assign correct weights to each token. See <code>SetSampler</code> for more details.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def sample(self, context, draw=None):\n    \"\"\"Sample a token and weight by sampling a weighted set of tokens from the `set_sampler`\n    and then selecting one proportional to its weight.\n\n    Given a context of tokens $x_1, \\\\ldots, x_{n-1}$ in the vocabulary of the set sampler's target potential,\n    this method samples a token $x_n \\\\in \\\\textsf{set_sampler.target.vocab_eos}$ and a weight.\n\n    The sampled token and weight are properly weighted with respect to\n    $$\n    \\\\textsf{set_sampler.target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})\n    $$\n\n    The returned weight corresponds to the sum of the weights of the sampled set.\n\n    Args:\n        context (list[int]): A sequence of tokens in the vocabulary of the set sampler's target potential.\n\n    Returns:\n        (token, weight, logp): A tuple containing the sampled token, weight, and log-probability of the random\n            choices made in sampling that token.\n\n    Note:\n        For properly weighted sampling, the `set_sampler` must assign correct weights to each token. See\n        `SetSampler` for more details.\n    \"\"\"\n    logws, logp = await self.set_sampler.sample_set(context, draw=draw)\n    logps = logws.normalize()\n    if draw is None:\n        token = fast_sample_lazyweights(logps)\n    else:\n        token = draw(logps.exp().materialize())\n    return token, logws.sum(), logp + logps[token]\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.SetTokenSampler.cleanup","title":"<code>cleanup()</code>  <code>async</code>","text":"<p>Clean up the sampler.</p> <p>This method should be called when the sampler is no longer needed.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def cleanup(self):\n    \"\"\"Clean up the sampler.\n\n    This method should be called when the sampler is no longer needed.\n    \"\"\"\n    await self.set_sampler.cleanup()\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.AWRS","title":"<code>AWRS</code>","text":"<p>               Bases: <code>TokenSampler</code></p> <p>Samples individual tokens through an adaptive weighted rejection sampling algorithm.</p> <p>This sampler is based on the algorithm described in Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling</p> <p>It draws properly weighted samples from the product of a non-boolean potential and a boolean condition.</p> <p>Parameters:</p> Name Type Description Default <code>potential</code> <code>Potential</code> <p>The non-boolean potential.</p> required <code>condition</code> <code>Potential</code> <p>The boolean condition. This potential must only output boolean values (0 or -inf in log-space).</p> required <code>seed</code> <code>int</code> <p>The seed for the random number generator.</p> <code>42</code> <code>prune_logws</code> <code>bool</code> <p>Whether to prune the logws to only include the tokens in the intersection of the potential and condition vocabularies</p> <code>True</code> <code>proper_weights</code> <code>bool</code> <p>Whether to return properly weighted samples. If False, the sampler will only run one round of adaptive rejection sampling.</p> <code>True</code> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>class AWRS(TokenSampler):\n    \"\"\"Samples individual tokens through an adaptive weighted rejection sampling algorithm.\n\n    This sampler is based on the algorithm described in [Fast Controlled Generation from Language Models with Adaptive Weighted Rejection Sampling](https://arxiv.org/abs/2504.05410)\n\n    It draws properly weighted samples from the product of a non-boolean potential and a boolean condition.\n\n    Args:\n        potential (Potential): The non-boolean potential.\n        condition (Potential): The boolean condition. This potential must only output boolean values (0 or -inf in log-space).\n        seed (int): The seed for the random number generator.\n        prune_logws (bool): Whether to prune the logws to only include the tokens in the intersection of the potential and condition vocabularies\n        proper_weights (bool): Whether to return properly weighted samples.\n            If False, the sampler will only run one round of adaptive rejection sampling.\n    \"\"\"\n\n    def __init__(\n        self, potential, condition, seed=42, prune_logws=True, proper_weights=True\n    ):\n        super().__init__(target=potential * condition)\n        self.potential = potential\n        self.condition = condition\n\n        self.prune_logws = prune_logws\n        self.proper_weights = proper_weights\n        self.valid_idxs = np.array(\n            [self.potential.lookup[t] for t in self.target.vocab_eos]\n        )\n\n        self.vocab_eos_set = set(self.target.vocab_eos)\n        self.V = len(self.potential.vocab_eos)\n        self.rng = np.random.default_rng(seed=seed)\n\n    def _prune_logws(self, logws):\n        # Prune the logws to only include the tokens in the\n        # target vocabulary. (This zeros-out tokens which we know a priori\n        # will be rejected.) Note: We need an additional correction term\n        # to account for the fact that we're throwing away some probability mass.\n        # This should be handled in `sample`.\n        pruned = self.potential.alloc_logws()\n        pruned[self.valid_idxs] = logws.weights[self.valid_idxs]\n        logws.weights = pruned\n        return logws\n\n    async def _accept(self, context, token, verbosity=0):\n        if self.prune_logws or token in self.vocab_eos_set:\n            if token is self.target.eos:\n                logscore = await self.condition.complete(context)\n            else:\n                logscore = await self.condition.prefix(context + [token])\n            assert logscore in {-np.inf, 0}, \"`condition` must be Boolean\"\n        else:\n            logscore = -np.inf\n\n        do_accept = logscore == 0\n\n        if verbosity &gt; 0:\n            if do_accept:\n                print(colors.green % f\". {repr(token)}\")\n            else:\n                print(colors.red % \".\", end=\"\")\n\n        return do_accept\n\n    async def sample(self, context, verbosity=0):\n        \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method via adaptive weighted rejection sampling.\n\n        The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n        Returns:\n            (token, weight, np.nan): A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.\n        \"\"\"\n        logws = await self.potential.logw_next(context)\n        if self.prune_logws:\n            logws = self._prune_logws(logws)\n\n        logZ = logsumexp(logws.weights)\n        logps = logws.weights - logZ\n        toks = logws.decode\n\n        tok, nrej, logp0 = None, 0, []\n        for _ in range(2):\n            keys = logps - np.log(-np.log(self.rng.random((self.V,))))\n            order = np.argsort(-keys)\n            for rank in range(logps.size):\n                item = order[rank]\n                if keys[item] == -np.inf:\n                    break\n                if await self._accept(context, toks[item], verbosity):\n                    if tok is None:\n                        tok = toks[item]\n                    break\n                else:\n                    nrej += 1\n                    if tok is None:\n                        logp0.append(logps[item])\n                    logps[item] = -np.inf\n\n            if not self.proper_weights:\n                if tok is None:\n                    return self.target.eos, float(\"-inf\"), np.nan\n                return tok, 0, np.nan\n\n        if tok is None:  # No token was accepted, return EOS and kill the particle.\n            return self.target.eos, float(\"-inf\"), np.nan\n\n        if not logp0:  # Success on first try.\n            logw = logZ - np.log(nrej + 1)\n        else:\n            logw = logZ + log1mexp(logsumexp(logp0)) - np.log(nrej + 1)\n\n        return tok, logw, np.nan\n</code></pre>"},{"location":"reference/genlm/control/sampler/token/#genlm.control.sampler.token.AWRS.sample","title":"<code>sample(context, verbosity=0)</code>  <code>async</code>","text":"<p>Sample a token and weight that are properly weighted with respect to the target potential's <code>logw_next</code> method via adaptive weighted rejection sampling.</p> <p>The returned weight corresponds to the log normalizing constant of \\(\\textsf{target.logw_next}(x_n | x_1, \\ldots, x_{n-1})\\).</p> <p>Returns:</p> Type Description <code>(token, weight, nan)</code> <p>A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.</p> Source code in <code>genlm/control/sampler/token.py</code> <pre><code>async def sample(self, context, verbosity=0):\n    \"\"\"Sample a token and weight that are properly weighted with respect to the target potential's `logw_next` method via adaptive weighted rejection sampling.\n\n    The returned weight corresponds to the log normalizing constant of $\\\\textsf{target.logw_next}(x_n | x_1, \\\\ldots, x_{n-1})$.\n\n    Returns:\n        (token, weight, np.nan): A tuple containing the sampled token, weight, and a dummy value for the log-probability of the sampled token.\n    \"\"\"\n    logws = await self.potential.logw_next(context)\n    if self.prune_logws:\n        logws = self._prune_logws(logws)\n\n    logZ = logsumexp(logws.weights)\n    logps = logws.weights - logZ\n    toks = logws.decode\n\n    tok, nrej, logp0 = None, 0, []\n    for _ in range(2):\n        keys = logps - np.log(-np.log(self.rng.random((self.V,))))\n        order = np.argsort(-keys)\n        for rank in range(logps.size):\n            item = order[rank]\n            if keys[item] == -np.inf:\n                break\n            if await self._accept(context, toks[item], verbosity):\n                if tok is None:\n                    tok = toks[item]\n                break\n            else:\n                nrej += 1\n                if tok is None:\n                    logp0.append(logps[item])\n                logps[item] = -np.inf\n\n        if not self.proper_weights:\n            if tok is None:\n                return self.target.eos, float(\"-inf\"), np.nan\n            return tok, 0, np.nan\n\n    if tok is None:  # No token was accepted, return EOS and kill the particle.\n        return self.target.eos, float(\"-inf\"), np.nan\n\n    if not logp0:  # Success on first try.\n        logw = logZ - np.log(nrej + 1)\n    else:\n        logw = logZ + log1mexp(logsumexp(logp0)) - np.log(nrej + 1)\n\n    return tok, logw, np.nan\n</code></pre>"}]}